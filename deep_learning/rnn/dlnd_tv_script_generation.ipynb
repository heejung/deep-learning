{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TV Script Generation\n",
    "\n",
    "In this project, you'll generate your own [Seinfeld](https://en.wikipedia.org/wiki/Seinfeld) TV scripts using RNNs.  You'll be using part of the [Seinfeld dataset](https://www.kaggle.com/thec03u5/seinfeld-chronicles#scripts.csv) of scripts from 9 seasons.  The Neural Network you'll build will generate a new ,\"fake\" TV script, based on patterns it recognizes in this training data.\n",
    "\n",
    "## Get the Data\n",
    "\n",
    "The data is already provided for you in `./data/Seinfeld_Scripts.txt` and you're encouraged to open that file and look at the text. \n",
    ">* As a first step, we'll load in this data and look at some samples. \n",
    "* Then, you'll be tasked with defining and training an RNN to generate a new script!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# load in data\n",
    "import helper\n",
    "data_dir = './data/Seinfeld_Scripts.txt'\n",
    "text = helper.load_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "Play around with `view_line_range` to view different parts of the data. This will give you a sense of the data you'll be working with. You can see, for example, that it is all lowercase text, and each new line of dialogue is separated by a newline character `\\n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 46367\n",
      "Number of lines: 109233\n",
      "Average number of words in each line: 5.544240293684143\n",
      "\n",
      "The lines 0 to 10:\n",
      "jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people trying to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he didnt tell me where he was going. he must have gone out. you wanna go out you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...then youre standing around, what do you do? you go we gotta be getting back. once youre out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, its my feeling, youve gotta go. \n",
      "\n",
      "jerry: (pointing at georges shirt) see, to me, that button is in the worst possible spot. the second button literally makes or breaks the shirt, look at it. its too high! its in no-mans-land. you look like you live with your mother. \n",
      "\n",
      "george: are you through? \n",
      "\n",
      "jerry: you do of course try on, when you buy? \n",
      "\n",
      "george: yes, it was purple, i liked it, i dont actually recall considering the buttons. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "view_line_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "\n",
    "lines = text.split('\\n')\n",
    "print('Number of lines: {}'.format(len(lines)))\n",
    "word_count_line = [len(line.split()) for line in lines]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
    "\n",
    "print()\n",
    "print('The lines {} to {}:'.format(*view_line_range))\n",
    "print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implement Pre-processing Functions\n",
    "The first thing to do to any dataset is pre-processing.  Implement the following pre-processing functions below:\n",
    "- Lookup Table\n",
    "- Tokenize Punctuation\n",
    "\n",
    "### Lookup Table\n",
    "To create a word embedding, you first need to transform the words to ids.  In this function, create two dictionaries:\n",
    "- Dictionary to go from the words to an id, we'll call `vocab_to_int`\n",
    "- Dictionary to go from the id to word, we'll call `int_to_vocab`\n",
    "\n",
    "Return these dictionaries in the following **tuple** `(vocab_to_int, int_to_vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import problem_unittests as tests\n",
    "from string import punctuation\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    vocab = set(text)\n",
    "    vocab_to_int = {}\n",
    "    int_to_vocab = {}\n",
    "    vocab_to_int['<PAD>'] = 0\n",
    "    int_to_vocab[0] = '<PAD>'\n",
    "    for ii, word in enumerate(vocab, 1):\n",
    "        word = word.lower()\n",
    "        vocab_to_int[word] = ii\n",
    "        int_to_vocab[ii] = word\n",
    "    \n",
    "    # return tuple\n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Punctuation\n",
    "We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks can create multiple ids for the same word. For example, \"bye\" and \"bye!\" would generate two different word ids.\n",
    "\n",
    "Implement the function `token_lookup` to return a dict that will be used to tokenize symbols like \"!\" into \"||Exclamation_Mark||\".  Create a dictionary for the following symbols where the symbol is the key and value is the token:\n",
    "- Period ( **.** )\n",
    "- Comma ( **,** )\n",
    "- Quotation Mark ( **\"** )\n",
    "- Semicolon ( **;** )\n",
    "- Exclamation mark ( **!** )\n",
    "- Question mark ( **?** )\n",
    "- Left Parentheses ( **(** )\n",
    "- Right Parentheses ( **)** )\n",
    "- Dash ( **-** )\n",
    "- Return ( **\\n** )\n",
    "\n",
    "This dictionary will be used to tokenize the symbols and add the delimiter (space) around it.  This separates each symbols as its own word, making it easier for the neural network to predict the next word. Make sure you don't use a value that could be confused as a word; for example, instead of using the value \"dash\", try using something like \"||dash||\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenized dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    lookup = {}\n",
    "    lookup['.'] = '||period||'\n",
    "    lookup[','] = '||comma||'\n",
    "    lookup['\"'] = '||quotation_mark||'\n",
    "    lookup[';'] = '||semicolon||'\n",
    "    lookup['!'] = '||exclamation_mark||'\n",
    "    lookup['?'] = '||question_mark||'\n",
    "    lookup['('] = '||left_paraentheses||'\n",
    "    lookup[')'] = '||right_parentheses||'\n",
    "    lookup['-'] = '||dash||'\n",
    "    lookup['\\n'] = '||return||'\n",
    "        \n",
    "    return lookup\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process all the data and save it\n",
    "\n",
    "Running the code cell below will pre-process all the data and save it to file. You're encouraged to lok at the code for `preprocess_and_save_data` in the `helpers.py` file to see what it's doing in detail, but you do not need to change this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# pre-process training data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "In this section, you'll build the components necessary to build an RNN by implementing the RNN Module and forward and backpropagation functions.\n",
    "\n",
    "### Check Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input\n",
    "Let's start with the preprocessed input data. We'll use [TensorDataset](http://pytorch.org/docs/master/data.html#torch.utils.data.TensorDataset) to provide a known format to our dataset; in combination with [DataLoader](http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader), it will handle batching, shuffling, and other dataset iteration functions.\n",
    "\n",
    "You can create data with TensorDataset by passing in feature and target tensors. Then create a DataLoader as usual.\n",
    "```\n",
    "data = TensorDataset(feature_tensors, target_tensors)\n",
    "data_loader = torch.utils.data.DataLoader(data, \n",
    "                                          batch_size=batch_size)\n",
    "```\n",
    "\n",
    "### Batching\n",
    "Implement the `batch_data` function to batch `words` data into chunks of size `batch_size` using the `TensorDataset` and `DataLoader` classes.\n",
    "\n",
    ">You can batch words using the DataLoader, but it will be up to you to create `feature_tensors` and `target_tensors` of the correct size and content for a given `sequence_length`.\n",
    "\n",
    "For example, say we have these as input:\n",
    "```\n",
    "words = [1, 2, 3, 4, 5, 6, 7]\n",
    "sequence_length = 4\n",
    "```\n",
    "\n",
    "Your first `feature_tensor` should contain the values:\n",
    "```\n",
    "[1, 2, 3, 4]\n",
    "```\n",
    "And the corresponding `target_tensor` should just be the next \"word\"/tokenized word value:\n",
    "```\n",
    "5\n",
    "```\n",
    "This should continue with the second `feature_tensor`, `target_tensor` being:\n",
    "```\n",
    "[2, 3, 4, 5]  # features\n",
    "6             # target\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f2a0760ec88>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def batch_data(words, sequence_length, batch_size):\n",
    "    \"\"\"\n",
    "    Batch the neural network data using DataLoader\n",
    "    :param words: The word ids of the TV scripts\n",
    "    :param sequence_length: The sequence length of each batch\n",
    "    :param batch_size: The size of each batch; the number of sequences in a batch\n",
    "    :return: DataLoader with batched data\n",
    "    \"\"\"\n",
    "    # TODO: Implement function\n",
    "    features = []\n",
    "    labels = []\n",
    "    size = len(words)\n",
    "    max_i = size - sequence_length\n",
    "    for i in range(max_i):\n",
    "        row = []\n",
    "        for ii in range(sequence_length):\n",
    "            row.append(words[i+ii])\n",
    "        features.append(row)\n",
    "        labels.append(words[i+sequence_length])\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    data = TensorDataset(torch.from_numpy(features), torch.from_numpy(labels))\n",
    "    dataloader = DataLoader(data, shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "# there is no test for this function, but you are encouraged to create\n",
    "# print statements and tests of your own\n",
    "\n",
    "words = [1, 2, 3, 4, 5, 6, 7]\n",
    "batch_data(words, 4, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your dataloader \n",
    "\n",
    "You'll have to modify this code to test a batching function, but it should look fairly similar.\n",
    "\n",
    "Below, we're generating some test text data and defining a dataloader using the function you defined, above. Then, we are getting some sample batch of inputs `sample_x` and targets `sample_y` from our dataloader.\n",
    "\n",
    "Your code should return something like the following (likely in a different order, if you shuffled your data):\n",
    "\n",
    "```\n",
    "torch.Size([10, 5])\n",
    "tensor([[ 28,  29,  30,  31,  32],\n",
    "        [ 21,  22,  23,  24,  25],\n",
    "        [ 17,  18,  19,  20,  21],\n",
    "        [ 34,  35,  36,  37,  38],\n",
    "        [ 11,  12,  13,  14,  15],\n",
    "        [ 23,  24,  25,  26,  27],\n",
    "        [  6,   7,   8,   9,  10],\n",
    "        [ 38,  39,  40,  41,  42],\n",
    "        [ 25,  26,  27,  28,  29],\n",
    "        [  7,   8,   9,  10,  11]])\n",
    "\n",
    "torch.Size([10])\n",
    "tensor([ 33,  26,  22,  39,  16,  28,  11,  43,  30,  12])\n",
    "```\n",
    "\n",
    "### Sizes\n",
    "Your sample_x should be of size `(batch_size, sequence_length)` or (10, 5) in this case and sample_y should just have one dimension: batch_size (10). \n",
    "\n",
    "### Values\n",
    "\n",
    "You should also notice that the targets, sample_y, are the *next* value in the ordered test_text data. So, for an input sequence `[ 28,  29,  30,  31,  32]` that ends with the value `32`, the corresponding output should be `33`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n",
      "tensor([[  8,   9,  10,  11,  12],\n",
      "        [ 32,  33,  34,  35,  36],\n",
      "        [ 30,  31,  32,  33,  34],\n",
      "        [ 44,  45,  46,  47,  48],\n",
      "        [ 11,  12,  13,  14,  15],\n",
      "        [ 29,  30,  31,  32,  33],\n",
      "        [ 22,  23,  24,  25,  26],\n",
      "        [ 28,  29,  30,  31,  32],\n",
      "        [ 14,  15,  16,  17,  18],\n",
      "        [ 36,  37,  38,  39,  40]])\n",
      "\n",
      "torch.Size([10])\n",
      "tensor([ 13,  37,  35,  49,  16,  34,  27,  33,  19,  41])\n"
     ]
    }
   ],
   "source": [
    "# test dataloader\n",
    "\n",
    "test_text = range(50)\n",
    "t_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
    "\n",
    "data_iter = iter(t_loader)\n",
    "sample_x, sample_y = data_iter.next()\n",
    "\n",
    "print(sample_x.shape)\n",
    "print(sample_x)\n",
    "print()\n",
    "print(sample_y.shape)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Build the Neural Network\n",
    "Implement an RNN using PyTorch's [Module class](http://pytorch.org/docs/master/nn.html#torch.nn.Module). You may choose to use a GRU or an LSTM. To complete the RNN, you'll have to implement the following functions for the class:\n",
    " - `__init__` - The initialize function. \n",
    " - `init_hidden` - The initialization function for an LSTM/GRU hidden state\n",
    " - `forward` - Forward propagation function.\n",
    " \n",
    "The initialize function should create the layers of the neural network and save them to the class. The forward propagation function will use these layers to run forward propagation and generate an output and a hidden state.\n",
    "\n",
    "**The output of this model should be the *last* batch of word scores** after a complete sequence has been processed. That is, for each input sequence of words, we only want to output the word scores for a single, most likely, next word.\n",
    "\n",
    "### Hints\n",
    "\n",
    "1. Make sure to stack the outputs of the lstm to pass to your fully-connected layer, you can do this with `lstm_output = lstm_output.contiguous().view(-1, self.hidden_dim)`\n",
    "2. You can get the last batch of word scores by shaping the output of the final, fully-connected layer like so:\n",
    "\n",
    "```\n",
    "# reshape into (batch_size, seq_length, output_size)\n",
    "output = output.view(batch_size, -1, self.output_size)\n",
    "# get last batch\n",
    "out = output[:, -1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch RNN Module\n",
    "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
    "        :param output_size: The number of output dimensions of the neural network\n",
    "        :param embedding_dim: The size of embeddings, should you choose to use them        \n",
    "        :param hidden_dim: The size of the hidden layer outputs\n",
    "        :param dropout: dropout to add in between LSTM/GRU layers\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        # TODO: Implement function\n",
    "        \n",
    "        # set class variables\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # define model layers\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param nn_input: The input to the neural network\n",
    "        :param hidden: The hidden state        \n",
    "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
    "        \"\"\"\n",
    "        batch_size = nn_input.size(0)\n",
    "        # TODO: Implement function   \n",
    "        x = self.embed(nn_input)\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.dropout(x)\n",
    "        x = x.contiguous().view(-1,self.hidden_dim)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(batch_size, -1, self.output_size)[:,-1,:]\n",
    "\n",
    "        # return one batch of output word scores and the hidden state\n",
    "        return x, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM/GRU\n",
    "        :param batch_size: The batch_size of the hidden state\n",
    "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
    "        '''\n",
    "        # Implement function\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        # initialize hidden state with zero weights, and move to GPU if available\n",
    "        if train_on_gpu:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_rnn(RNN, train_on_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define forward and backpropagation\n",
    "\n",
    "Use the RNN class you implemented to apply forward and back propagation. This function will be called, iteratively, in the training loop as follows:\n",
    "```\n",
    "loss = forward_back_prop(decoder, decoder_optimizer, criterion, inp, target)\n",
    "```\n",
    "\n",
    "And it should return the average loss over a batch and the hidden state returned by a call to `RNN(inp, hidden)`. Recall that you can get this loss by computing it, as usual, and calling `loss.item()`.\n",
    "\n",
    "**If a GPU is available, you should move your data to that GPU device, here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    :param decoder: The PyTorch Module that holds the neural network\n",
    "    :param decoder_optimizer: The PyTorch optimizer for the neural network\n",
    "    :param criterion: The PyTorch loss function\n",
    "    :param inp: A batch of input to the neural network\n",
    "    :param target: The target output for the batch of input\n",
    "    :return: The loss and the latest hidden state Tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # move data to GPU, if available\n",
    "    if train_on_gpu:\n",
    "        rnn.cuda()\n",
    "        inp = inp.cuda()\n",
    "        target = target.cuda()\n",
    "    \n",
    "    clip = 5\n",
    "\n",
    "    rnn.train()\n",
    "    \n",
    "    # perform backpropagation and optimization\n",
    "    rnn.zero_grad()\n",
    "    hidden = tuple([each.data for each in hidden])\n",
    "    sftmx, hidden = rnn(inp, hidden)\n",
    "    loss = criterion(sftmx.squeeze(), target.long())\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), clip)\n",
    "    optimizer.step()\n",
    "\n",
    "    # return the loss over a batch and the hidden state produced by our model\n",
    "    return loss.item(), hidden\n",
    "\n",
    "# Note that these tests aren't completely extensive.\n",
    "# they are here to act as general checks on the expected outputs of your functions\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_forward_back_prop(RNN, forward_back_prop, train_on_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "\n",
    "With the structure of the network complete and data ready to be fed in the neural network, it's time to train it.\n",
    "\n",
    "### Train Loop\n",
    "\n",
    "The training loop is implemented for you in the `train_decoder` function. This function will train the network over all the batches for the number of epochs given. The model progress will be shown every number of batches. This number is set with the `show_every_n_batches` parameter. You'll set this parameter along with other parameters in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "def save_hidden(filename, decoder):\n",
    "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '_hidden.pt'\n",
    "    torch.save(decoder, save_filename)\n",
    "\n",
    "\n",
    "def load_hidden(filename):\n",
    "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '_hidden.pt'\n",
    "    return torch.load(save_filename)\n",
    "\n",
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100, path='./save/trained_rnn_512_02'):\n",
    "    batch_losses = []\n",
    "    \n",
    "    rnn.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        # initialize hidden state\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            # make sure you iterate over completely full batches, only\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            # forward, back prop\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            # record loss\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            # printing loss stats\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
    "                batch_losses = []\n",
    "        helper.save_model(path, rnn)\n",
    "\n",
    "    # returns a trained rnn\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Set and train the neural network with the following parameters:\n",
    "- Set `sequence_length` to the length of a sequence.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `num_epochs` to the number of epochs to train for.\n",
    "- Set `learning_rate` to the learning rate for an Adam optimizer.\n",
    "- Set `vocab_size` to the number of uniqe tokens in our vocabulary.\n",
    "- Set `output_size` to the desired size of the output.\n",
    "- Set `embedding_dim` to the embedding dimension; smaller than the vocab_size.\n",
    "- Set `hidden_dim` to the hidden dimension of your RNN.\n",
    "- Set `n_layers` to the number of layers/cells in your RNN.\n",
    "- Set `show_every_n_batches` to the number of batches at which the neural network should print progress.\n",
    "\n",
    "If the network isn't getting the desired results, tweak these parameters and/or the layers in the `RNN` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data params\n",
    "# Sequence Length\n",
    "sequence_length = 200  # of words in a sequence\n",
    "# Batch Size\n",
    "batch_size = 32\n",
    "\n",
    "# data loader - do not change\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# Number of Epochs\n",
    "num_epochs = 5\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model parameters\n",
    "# Vocab size\n",
    "vocab_size = len(vocab_to_int)\n",
    "# Output size\n",
    "output_size = len(vocab_to_int)\n",
    "# Embedding Dimension\n",
    "embedding_dim = 200\n",
    "# Hidden Dimension\n",
    "hidden_dim = 256\n",
    "# Number of RNN Layers\n",
    "n_layers = 2\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "In the next cell, you'll train the neural network on the pre-processed data.  If you have a hard time getting a good loss, you may consider changing your hyperparameters. In general, you may get better results with larger hidden and n_layer dimensions, but larger models take a longer time to train. \n",
    "> **You should aim for a loss less than 3.5.** \n",
    "\n",
    "You should also experiment with different sequence lengths, which determine the size of the long range dependencies that a model can learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5 epoch(s)...\n",
      "Epoch:    1/5     Loss: 5.814536405563355\n",
      "\n",
      "Epoch:    1/5     Loss: 5.294642675876617\n",
      "\n",
      "Epoch:    1/5     Loss: 5.215630855083465\n",
      "\n",
      "Epoch:    1/5     Loss: 5.05716952085495\n",
      "\n",
      "Epoch:    1/5     Loss: 5.065799199104309\n",
      "\n",
      "Epoch:    1/5     Loss: 4.929776544570923\n",
      "\n",
      "Epoch:    1/5     Loss: 4.91144019985199\n",
      "\n",
      "Epoch:    1/5     Loss: 4.871308234214783\n",
      "\n",
      "Epoch:    1/5     Loss: 4.795930293560028\n",
      "\n",
      "Epoch:    1/5     Loss: 4.8035445337295535\n",
      "\n",
      "Epoch:    1/5     Loss: 4.744415888786316\n",
      "\n",
      "Epoch:    1/5     Loss: 4.728337649822235\n",
      "\n",
      "Epoch:    1/5     Loss: 4.714304267883301\n",
      "\n",
      "Epoch:    1/5     Loss: 4.7128020849227905\n",
      "\n",
      "Epoch:    1/5     Loss: 4.638632520675659\n",
      "\n",
      "Epoch:    1/5     Loss: 4.645726988792419\n",
      "\n",
      "Epoch:    1/5     Loss: 4.662727818965912\n",
      "\n",
      "Epoch:    1/5     Loss: 4.625527530193329\n",
      "\n",
      "Epoch:    1/5     Loss: 4.616027403831482\n",
      "\n",
      "Epoch:    1/5     Loss: 4.683815835475921\n",
      "\n",
      "Epoch:    1/5     Loss: 4.589832183837891\n",
      "\n",
      "Epoch:    1/5     Loss: 4.63441016292572\n",
      "\n",
      "Epoch:    1/5     Loss: 4.591241439819336\n",
      "\n",
      "Epoch:    1/5     Loss: 4.60772371339798\n",
      "\n",
      "Epoch:    1/5     Loss: 4.5759191536903385\n",
      "\n",
      "Epoch:    1/5     Loss: 4.569259301662445\n",
      "\n",
      "Epoch:    1/5     Loss: 4.572786736011505\n",
      "\n",
      "Epoch:    1/5     Loss: 4.50164525938034\n",
      "\n",
      "Epoch:    1/5     Loss: 4.541713942527771\n",
      "\n",
      "Epoch:    1/5     Loss: 4.52987335395813\n",
      "\n",
      "Epoch:    1/5     Loss: 4.518937622070313\n",
      "\n",
      "Epoch:    1/5     Loss: 4.549552967071533\n",
      "\n",
      "Epoch:    1/5     Loss: 4.579280997753143\n",
      "\n",
      "Epoch:    1/5     Loss: 4.571717021942138\n",
      "\n",
      "Epoch:    1/5     Loss: 4.474426788806915\n",
      "\n",
      "Epoch:    1/5     Loss: 4.519259392261505\n",
      "\n",
      "Epoch:    1/5     Loss: 4.493209090232849\n",
      "\n",
      "Epoch:    1/5     Loss: 4.519373109817505\n",
      "\n",
      "Epoch:    1/5     Loss: 4.497708770751953\n",
      "\n",
      "Epoch:    1/5     Loss: 4.551624008178711\n",
      "\n",
      "Epoch:    1/5     Loss: 4.50586887216568\n",
      "\n",
      "Epoch:    1/5     Loss: 4.517372351169586\n",
      "\n",
      "Epoch:    1/5     Loss: 4.503399394989014\n",
      "\n",
      "Epoch:    1/5     Loss: 4.536330268859864\n",
      "\n",
      "Epoch:    1/5     Loss: 4.4872063860893245\n",
      "\n",
      "Epoch:    1/5     Loss: 4.4743593883514405\n",
      "\n",
      "Epoch:    1/5     Loss: 4.510736158370972\n",
      "\n",
      "Epoch:    1/5     Loss: 4.472101217746735\n",
      "\n",
      "Epoch:    1/5     Loss: 4.410363340377808\n",
      "\n",
      "Epoch:    1/5     Loss: 4.39698825931549\n",
      "\n",
      "Epoch:    1/5     Loss: 4.458993353366852\n",
      "\n",
      "Epoch:    1/5     Loss: 4.503716428279876\n",
      "\n",
      "Epoch:    1/5     Loss: 4.485940454483032\n",
      "\n",
      "Epoch:    1/5     Loss: 4.442064736366272\n",
      "\n",
      "Epoch:    1/5     Loss: 4.44131012582779\n",
      "\n",
      "Epoch:    2/5     Loss: 4.436734755925082\n",
      "\n",
      "Epoch:    2/5     Loss: 4.417560906410217\n",
      "\n",
      "Epoch:    2/5     Loss: 4.370200464725494\n",
      "\n",
      "Epoch:    2/5     Loss: 4.378452419281006\n",
      "\n",
      "Epoch:    2/5     Loss: 4.362026927947998\n",
      "\n",
      "Epoch:    2/5     Loss: 4.366460356235504\n",
      "\n",
      "Epoch:    2/5     Loss: 4.353839126586914\n",
      "\n",
      "Epoch:    2/5     Loss: 4.400313698768616\n",
      "\n",
      "Epoch:    2/5     Loss: 4.3600624394416805\n",
      "\n",
      "Epoch:    2/5     Loss: 4.363661772251129\n",
      "\n",
      "Epoch:    2/5     Loss: 4.353125068664551\n",
      "\n",
      "Epoch:    2/5     Loss: 4.354106616973877\n",
      "\n",
      "Epoch:    2/5     Loss: 4.395263878822327\n",
      "\n",
      "Epoch:    2/5     Loss: 4.325454029560089\n",
      "\n",
      "Epoch:    2/5     Loss: 4.351720818042756\n",
      "\n",
      "Epoch:    2/5     Loss: 4.370812026977539\n",
      "\n",
      "Epoch:    2/5     Loss: 4.379920693397522\n",
      "\n",
      "Epoch:    2/5     Loss: 4.385114273548126\n",
      "\n",
      "Epoch:    2/5     Loss: 4.373790499687195\n",
      "\n",
      "Epoch:    2/5     Loss: 4.354629680633545\n",
      "\n",
      "Epoch:    2/5     Loss: 4.392915211200714\n",
      "\n",
      "Epoch:    2/5     Loss: 4.417196600914002\n",
      "\n",
      "Epoch:    2/5     Loss: 4.313290849685669\n",
      "\n",
      "Epoch:    2/5     Loss: 4.374248741626739\n",
      "\n",
      "Epoch:    2/5     Loss: 4.3975505814552305\n",
      "\n",
      "Epoch:    2/5     Loss: 4.37189598941803\n",
      "\n",
      "Epoch:    2/5     Loss: 4.389361053943634\n",
      "\n",
      "Epoch:    2/5     Loss: 4.3629054317474365\n",
      "\n",
      "Epoch:    2/5     Loss: 4.387731311321258\n",
      "\n",
      "Epoch:    2/5     Loss: 4.371873223781586\n",
      "\n",
      "Epoch:    2/5     Loss: 4.352988237380981\n",
      "\n",
      "Epoch:    2/5     Loss: 4.382421110630036\n",
      "\n",
      "Epoch:    2/5     Loss: 4.4150300226211545\n",
      "\n",
      "Epoch:    2/5     Loss: 4.401484179973602\n",
      "\n",
      "Epoch:    2/5     Loss: 4.29018062877655\n",
      "\n",
      "Epoch:    2/5     Loss: 4.333398000717163\n",
      "\n",
      "Epoch:    2/5     Loss: 4.368011328220367\n",
      "\n",
      "Epoch:    2/5     Loss: 4.349314429759979\n",
      "\n",
      "Epoch:    2/5     Loss: 4.410820715904236\n",
      "\n",
      "Epoch:    2/5     Loss: 4.315499948501587\n",
      "\n",
      "Epoch:    2/5     Loss: 4.393279444217682\n",
      "\n",
      "Epoch:    2/5     Loss: 4.402535596370697\n",
      "\n",
      "Epoch:    2/5     Loss: 4.35371554517746\n",
      "\n",
      "Epoch:    2/5     Loss: 4.343973242759705\n",
      "\n",
      "Epoch:    2/5     Loss: 4.318058572769165\n",
      "\n",
      "Epoch:    2/5     Loss: 4.39408775472641\n",
      "\n",
      "Epoch:    2/5     Loss: 4.32995825624466\n",
      "\n",
      "Epoch:    2/5     Loss: 4.393920383453369\n",
      "\n",
      "Epoch:    2/5     Loss: 4.412745111465454\n",
      "\n",
      "Epoch:    2/5     Loss: 4.377167280197144\n",
      "\n",
      "Epoch:    2/5     Loss: 4.34530922460556\n",
      "\n",
      "Epoch:    2/5     Loss: 4.37680310344696\n",
      "\n",
      "Epoch:    2/5     Loss: 4.354883913516998\n",
      "\n",
      "Epoch:    2/5     Loss: 4.397716065883636\n",
      "\n",
      "Epoch:    2/5     Loss: 4.383840825557709\n",
      "\n",
      "Epoch:    3/5     Loss: 4.3114678053134075\n",
      "\n",
      "Epoch:    3/5     Loss: 4.24996200799942\n",
      "\n",
      "Epoch:    3/5     Loss: 4.306850504875183\n",
      "\n",
      "Epoch:    3/5     Loss: 4.289680249214173\n",
      "\n",
      "Epoch:    3/5     Loss: 4.277653028488159\n",
      "\n",
      "Epoch:    3/5     Loss: 4.33966392993927\n",
      "\n",
      "Epoch:    3/5     Loss: 4.284390163898468\n",
      "\n",
      "Epoch:    3/5     Loss: 4.329356000423432\n",
      "\n",
      "Epoch:    3/5     Loss: 4.333802765846253\n",
      "\n",
      "Epoch:    3/5     Loss: 4.290928920269012\n",
      "\n",
      "Epoch:    3/5     Loss: 4.264094372272491\n",
      "\n",
      "Epoch:    3/5     Loss: 4.322128223896026\n",
      "\n",
      "Epoch:    3/5     Loss: 4.2978744773864745\n",
      "\n",
      "Epoch:    3/5     Loss: 4.2732779116630555\n",
      "\n",
      "Epoch:    3/5     Loss: 4.294999272346496\n",
      "\n",
      "Epoch:    3/5     Loss: 4.287670799970627\n",
      "\n",
      "Epoch:    3/5     Loss: 4.274223787784576\n",
      "\n",
      "Epoch:    3/5     Loss: 4.262778072357178\n",
      "\n",
      "Epoch:    3/5     Loss: 4.283401708602906\n",
      "\n",
      "Epoch:    3/5     Loss: 4.277910152435303\n",
      "\n",
      "Epoch:    3/5     Loss: 4.321307597637176\n",
      "\n",
      "Epoch:    3/5     Loss: 4.282060063362121\n",
      "\n",
      "Epoch:    3/5     Loss: 4.326119116783142\n",
      "\n",
      "Epoch:    3/5     Loss: 4.322975734233856\n",
      "\n",
      "Epoch:    3/5     Loss: 4.361100654125214\n",
      "\n",
      "Epoch:    3/5     Loss: 4.272227937698364\n",
      "\n",
      "Epoch:    3/5     Loss: 4.294287593364715\n",
      "\n",
      "Epoch:    3/5     Loss: 4.253145534038544\n",
      "\n",
      "Epoch:    3/5     Loss: 4.374750430107117\n",
      "\n",
      "Epoch:    3/5     Loss: 4.346342374324799\n",
      "\n",
      "Epoch:    3/5     Loss: 4.373277500629425\n",
      "\n",
      "Epoch:    3/5     Loss: 4.333756207466125\n",
      "\n",
      "Epoch:    3/5     Loss: 4.322094758510589\n",
      "\n",
      "Epoch:    3/5     Loss: 4.32598024225235\n",
      "\n",
      "Epoch:    3/5     Loss: 4.306200777053833\n",
      "\n",
      "Epoch:    3/5     Loss: 4.387970173358918\n",
      "\n",
      "Epoch:    3/5     Loss: 4.369224238395691\n",
      "\n",
      "Epoch:    3/5     Loss: 4.359998817443848\n",
      "\n",
      "Epoch:    3/5     Loss: 4.296407831668854\n",
      "\n",
      "Epoch:    3/5     Loss: 4.3463978796005245\n",
      "\n",
      "Epoch:    3/5     Loss: 4.305065712928772\n",
      "\n",
      "Epoch:    3/5     Loss: 4.420124529838562\n",
      "\n",
      "Epoch:    3/5     Loss: 4.292814611911774\n",
      "\n",
      "Epoch:    3/5     Loss: 4.319289827823639\n",
      "\n",
      "Epoch:    3/5     Loss: 4.3475373916625975\n",
      "\n",
      "Epoch:    3/5     Loss: 4.314881556034088\n",
      "\n",
      "Epoch:    3/5     Loss: 4.370634526252746\n",
      "\n",
      "Epoch:    3/5     Loss: 4.316488303661346\n",
      "\n",
      "Epoch:    3/5     Loss: 4.325541396617889\n",
      "\n",
      "Epoch:    3/5     Loss: 4.3558143739700315\n",
      "\n",
      "Epoch:    3/5     Loss: 4.292404132366181\n",
      "\n",
      "Epoch:    3/5     Loss: 4.3303565726280215\n",
      "\n",
      "Epoch:    3/5     Loss: 4.254721306324005\n",
      "\n",
      "Epoch:    3/5     Loss: 4.34211433506012\n",
      "\n",
      "Epoch:    3/5     Loss: 4.311913259983063\n",
      "\n",
      "Epoch:    4/5     Loss: 4.297258165724781\n",
      "\n",
      "Epoch:    4/5     Loss: 4.249032436370849\n",
      "\n",
      "Epoch:    4/5     Loss: 4.213305417537689\n",
      "\n",
      "Epoch:    4/5     Loss: 4.232932032585144\n",
      "\n",
      "Epoch:    4/5     Loss: 4.268379186153412\n",
      "\n",
      "Epoch:    4/5     Loss: 4.257885073661805\n",
      "\n",
      "Epoch:    4/5     Loss: 4.3163082566261295\n",
      "\n",
      "Epoch:    4/5     Loss: 4.2904212236404415\n",
      "\n",
      "Epoch:    4/5     Loss: 4.301093700885772\n",
      "\n",
      "Epoch:    4/5     Loss: 4.217848712444305\n",
      "\n",
      "Epoch:    4/5     Loss: 4.2117259321212765\n",
      "\n",
      "Epoch:    4/5     Loss: 4.208044074058533\n",
      "\n",
      "Epoch:    4/5     Loss: 4.261417052268982\n",
      "\n",
      "Epoch:    4/5     Loss: 4.279077393531799\n",
      "\n",
      "Epoch:    4/5     Loss: 4.298964917182922\n",
      "\n",
      "Epoch:    4/5     Loss: 4.2157950582504276\n",
      "\n",
      "Epoch:    4/5     Loss: 4.2977444024086\n",
      "\n",
      "Epoch:    4/5     Loss: 4.269743401527405\n",
      "\n",
      "Epoch:    4/5     Loss: 4.210197645664215\n",
      "\n",
      "Epoch:    4/5     Loss: 4.274595117568969\n",
      "\n",
      "Epoch:    4/5     Loss: 4.294284312725067\n",
      "\n",
      "Epoch:    4/5     Loss: 4.291557998657226\n",
      "\n",
      "Epoch:    4/5     Loss: 4.244408650875092\n",
      "\n",
      "Epoch:    4/5     Loss: 4.28458522605896\n",
      "\n",
      "Epoch:    4/5     Loss: 4.274147415161133\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    4/5     Loss: 4.2759792160987855\n",
      "\n",
      "Epoch:    4/5     Loss: 4.237377481937409\n",
      "\n",
      "Epoch:    4/5     Loss: 4.309260502338409\n",
      "\n",
      "Epoch:    4/5     Loss: 4.240190428256988\n",
      "\n",
      "Epoch:    4/5     Loss: 4.260425121307373\n",
      "\n",
      "Epoch:    4/5     Loss: 4.2841018733978276\n",
      "\n",
      "Epoch:    4/5     Loss: 4.311780556678772\n",
      "\n",
      "Epoch:    4/5     Loss: 4.298802248001099\n",
      "\n",
      "Epoch:    4/5     Loss: 4.318086175918579\n",
      "\n",
      "Epoch:    4/5     Loss: 4.307427879810334\n",
      "\n",
      "Epoch:    4/5     Loss: 4.278925110816956\n",
      "\n",
      "Epoch:    4/5     Loss: 4.284010627269745\n",
      "\n",
      "Epoch:    4/5     Loss: 4.280887735843659\n",
      "\n",
      "Epoch:    4/5     Loss: 4.298950122833252\n",
      "\n",
      "Epoch:    4/5     Loss: 4.323087350845337\n",
      "\n",
      "Epoch:    4/5     Loss: 4.27782289981842\n",
      "\n",
      "Epoch:    4/5     Loss: 4.288273348331451\n",
      "\n",
      "Epoch:    4/5     Loss: 4.300994512557984\n",
      "\n",
      "Epoch:    4/5     Loss: 4.2853131637573245\n",
      "\n",
      "Epoch:    4/5     Loss: 4.305103519439697\n",
      "\n",
      "Epoch:    4/5     Loss: 4.318372834205627\n",
      "\n",
      "Epoch:    4/5     Loss: 4.333410906791687\n",
      "\n",
      "Epoch:    4/5     Loss: 4.272612528800964\n",
      "\n",
      "Epoch:    4/5     Loss: 4.323982586860657\n",
      "\n",
      "Epoch:    4/5     Loss: 4.279165284156799\n",
      "\n",
      "Epoch:    4/5     Loss: 4.38596960401535\n",
      "\n",
      "Epoch:    4/5     Loss: 4.278731982707978\n",
      "\n",
      "Epoch:    4/5     Loss: 4.338526822090149\n",
      "\n",
      "Epoch:    4/5     Loss: 4.3012764735221864\n",
      "\n",
      "Epoch:    4/5     Loss: 4.311482988834381\n",
      "\n",
      "Epoch:    5/5     Loss: 4.30925162122884\n",
      "\n",
      "Epoch:    5/5     Loss: 4.242174993991852\n",
      "\n",
      "Epoch:    5/5     Loss: 4.259645723342896\n",
      "\n",
      "Epoch:    5/5     Loss: 4.245050137519836\n",
      "\n",
      "Epoch:    5/5     Loss: 4.197731098175049\n",
      "\n",
      "Epoch:    5/5     Loss: 4.205617435455323\n",
      "\n",
      "Epoch:    5/5     Loss: 4.2245950632095335\n",
      "\n",
      "Epoch:    5/5     Loss: 4.253448969364166\n",
      "\n",
      "Epoch:    5/5     Loss: 4.268926404953003\n",
      "\n",
      "Epoch:    5/5     Loss: 4.2009023308753966\n",
      "\n",
      "Epoch:    5/5     Loss: 4.251063664913177\n",
      "\n",
      "Epoch:    5/5     Loss: 4.241796107292175\n",
      "\n",
      "Epoch:    5/5     Loss: 4.23146158695221\n",
      "\n",
      "Epoch:    5/5     Loss: 4.2623475074768065\n",
      "\n",
      "Epoch:    5/5     Loss: 4.264617736339569\n",
      "\n",
      "Epoch:    5/5     Loss: 4.2045357837677\n",
      "\n",
      "Epoch:    5/5     Loss: 4.2229589438438415\n",
      "\n",
      "Epoch:    5/5     Loss: 4.263518837451935\n",
      "\n",
      "Epoch:    5/5     Loss: 4.240155561447144\n",
      "\n",
      "Epoch:    5/5     Loss: 4.273262493133545\n",
      "\n",
      "Epoch:    5/5     Loss: 4.274212830066681\n",
      "\n",
      "Epoch:    5/5     Loss: 4.189021192073822\n",
      "\n",
      "Epoch:    5/5     Loss: 4.287757121562958\n",
      "\n",
      "Epoch:    5/5     Loss: 4.276266750335694\n",
      "\n",
      "Epoch:    5/5     Loss: 4.236813409328461\n",
      "\n",
      "Epoch:    5/5     Loss: 4.251303292274475\n",
      "\n",
      "Epoch:    5/5     Loss: 4.255988213062286\n",
      "\n",
      "Epoch:    5/5     Loss: 4.236939625740051\n",
      "\n",
      "Epoch:    5/5     Loss: 4.266084270477295\n",
      "\n",
      "Epoch:    5/5     Loss: 4.209412664890289\n",
      "\n",
      "Epoch:    5/5     Loss: 4.253606575012207\n",
      "\n",
      "Epoch:    5/5     Loss: 4.3446528444290164\n",
      "\n",
      "Epoch:    5/5     Loss: 4.2463562135696415\n",
      "\n",
      "Epoch:    5/5     Loss: 4.28322264289856\n",
      "\n",
      "Epoch:    5/5     Loss: 4.261962521076202\n",
      "\n",
      "Epoch:    5/5     Loss: 4.287533810138703\n",
      "\n",
      "Epoch:    5/5     Loss: 4.324227420806885\n",
      "\n",
      "Epoch:    5/5     Loss: 4.2512546210289\n",
      "\n",
      "Epoch:    5/5     Loss: 4.300012996196747\n",
      "\n",
      "Epoch:    5/5     Loss: 4.2675820846557615\n",
      "\n",
      "Epoch:    5/5     Loss: 4.290779872894287\n",
      "\n",
      "Epoch:    5/5     Loss: 4.300566472053528\n",
      "\n",
      "Epoch:    5/5     Loss: 4.304964000225067\n",
      "\n",
      "Epoch:    5/5     Loss: 4.254397308349609\n",
      "\n",
      "Epoch:    5/5     Loss: 4.290576299190521\n",
      "\n",
      "Epoch:    5/5     Loss: 4.2427856044769285\n",
      "\n",
      "Epoch:    5/5     Loss: 4.276902160644531\n",
      "\n",
      "Epoch:    5/5     Loss: 4.285637125492096\n",
      "\n",
      "Epoch:    5/5     Loss: 4.274611342906952\n",
      "\n",
      "Epoch:    5/5     Loss: 4.305828317642212\n",
      "\n",
      "Epoch:    5/5     Loss: 4.3425978322029115\n",
      "\n",
      "Epoch:    5/5     Loss: 4.255847392082214\n",
      "\n",
      "Epoch:    5/5     Loss: 4.268331612110138\n",
      "\n",
      "Epoch:    5/5     Loss: 4.274453416824341\n",
      "\n",
      "Epoch:    5/5     Loss: 4.280846730232239\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "\n",
    "# create model and move to gpu if available\n",
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "# defining loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training the model\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
    "\n",
    "# saving the trained model\n",
    "helper.save_model('./save/trained_rnn', trained_rnn)\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 6 epoch(s)...\n",
      "Epoch:    1/6     Loss: 5.701030227661133\n",
      "\n",
      "Epoch:    1/6     Loss: 5.260918291568756\n",
      "\n",
      "Epoch:    1/6     Loss: 5.180226408481598\n",
      "\n",
      "Epoch:    1/6     Loss: 5.059348489761352\n",
      "\n",
      "Epoch:    1/6     Loss: 4.99691535282135\n",
      "\n",
      "Epoch:    1/6     Loss: 4.89206879234314\n",
      "\n",
      "Epoch:    1/6     Loss: 4.860235902786255\n",
      "\n",
      "Epoch:    1/6     Loss: 4.863872250080108\n",
      "\n",
      "Epoch:    1/6     Loss: 4.776177392482758\n",
      "\n",
      "Epoch:    1/6     Loss: 4.740982954502106\n",
      "\n",
      "Epoch:    1/6     Loss: 4.72337942314148\n",
      "\n",
      "Epoch:    1/6     Loss: 4.725228506088257\n",
      "\n",
      "Epoch:    1/6     Loss: 4.656517887115479\n",
      "\n",
      "Epoch:    1/6     Loss: 4.680493153572082\n",
      "\n",
      "Epoch:    1/6     Loss: 4.627993297100067\n",
      "\n",
      "Epoch:    1/6     Loss: 4.632838337898255\n",
      "\n",
      "Epoch:    1/6     Loss: 4.586052298069\n",
      "\n",
      "Epoch:    1/6     Loss: 4.632166959285736\n",
      "\n",
      "Epoch:    1/6     Loss: 4.578626882553101\n",
      "\n",
      "Epoch:    1/6     Loss: 4.591191054821015\n",
      "\n",
      "Epoch:    1/6     Loss: 4.639580221176147\n",
      "\n",
      "Epoch:    1/6     Loss: 4.5601844339370725\n",
      "\n",
      "Epoch:    1/6     Loss: 4.56793138551712\n",
      "\n",
      "Epoch:    1/6     Loss: 4.500314930915833\n",
      "\n",
      "Epoch:    1/6     Loss: 4.553431995868683\n",
      "\n",
      "Epoch:    1/6     Loss: 4.515501795768738\n",
      "\n",
      "Epoch:    1/6     Loss: 4.48710808467865\n",
      "\n",
      "Epoch:    1/6     Loss: 4.443469840049744\n",
      "\n",
      "Epoch:    1/6     Loss: 4.514402007102967\n",
      "\n",
      "Epoch:    1/6     Loss: 4.520723526954651\n",
      "\n",
      "Epoch:    1/6     Loss: 4.562497427463532\n",
      "\n",
      "Epoch:    1/6     Loss: 4.484114751338959\n",
      "\n",
      "Epoch:    1/6     Loss: 4.493692784786225\n",
      "\n",
      "Epoch:    1/6     Loss: 4.481010324001312\n",
      "\n",
      "Epoch:    1/6     Loss: 4.509758762359619\n",
      "\n",
      "Epoch:    1/6     Loss: 4.474657634735108\n",
      "\n",
      "Epoch:    1/6     Loss: 4.501046468257904\n",
      "\n",
      "Epoch:    1/6     Loss: 4.386840504646301\n",
      "\n",
      "Epoch:    1/6     Loss: 4.452244378566742\n",
      "\n",
      "Epoch:    1/6     Loss: 4.488375137329101\n",
      "\n",
      "Epoch:    1/6     Loss: 4.447592710971832\n",
      "\n",
      "Epoch:    1/6     Loss: 4.391558739185333\n",
      "\n",
      "Epoch:    1/6     Loss: 4.489153428554535\n",
      "\n",
      "Epoch:    1/6     Loss: 4.463572186946869\n",
      "\n",
      "Epoch:    1/6     Loss: 4.481047057151795\n",
      "\n",
      "Epoch:    1/6     Loss: 4.43047857093811\n",
      "\n",
      "Epoch:    1/6     Loss: 4.436689263820648\n",
      "\n",
      "Epoch:    1/6     Loss: 4.4355114817619326\n",
      "\n",
      "Epoch:    1/6     Loss: 4.460312595844269\n",
      "\n",
      "Epoch:    1/6     Loss: 4.468953829288482\n",
      "\n",
      "Epoch:    1/6     Loss: 4.465102061271668\n",
      "\n",
      "Epoch:    1/6     Loss: 4.484792927265167\n",
      "\n",
      "Epoch:    1/6     Loss: 4.442335616111755\n",
      "\n",
      "Epoch:    1/6     Loss: 4.427594386100769\n",
      "\n",
      "Epoch:    1/6     Loss: 4.402979548454285\n",
      "\n",
      "Epoch:    2/6     Loss: 4.38388700632874\n",
      "\n",
      "Epoch:    2/6     Loss: 4.358009289741516\n",
      "\n",
      "Epoch:    2/6     Loss: 4.263394702911377\n",
      "\n",
      "Epoch:    2/6     Loss: 4.316195414543152\n",
      "\n",
      "Epoch:    2/6     Loss: 4.356477009296417\n",
      "\n",
      "Epoch:    2/6     Loss: 4.317631012439728\n",
      "\n",
      "Epoch:    2/6     Loss: 4.329648234367371\n",
      "\n",
      "Epoch:    2/6     Loss: 4.370244465351105\n",
      "\n",
      "Epoch:    2/6     Loss: 4.3133094129562375\n",
      "\n",
      "Epoch:    2/6     Loss: 4.294911477088928\n",
      "\n",
      "Epoch:    2/6     Loss: 4.322437298297882\n",
      "\n",
      "Epoch:    2/6     Loss: 4.3229911665916445\n",
      "\n",
      "Epoch:    2/6     Loss: 4.35503913640976\n",
      "\n",
      "Epoch:    2/6     Loss: 4.357208342552185\n",
      "\n",
      "Epoch:    2/6     Loss: 4.335872775554657\n",
      "\n",
      "Epoch:    2/6     Loss: 4.320801797389984\n",
      "\n",
      "Epoch:    2/6     Loss: 4.294976740837098\n",
      "\n",
      "Epoch:    2/6     Loss: 4.313738481521606\n",
      "\n",
      "Epoch:    2/6     Loss: 4.326353988170624\n",
      "\n",
      "Epoch:    2/6     Loss: 4.346925703525543\n",
      "\n",
      "Epoch:    2/6     Loss: 4.3203676891326905\n",
      "\n",
      "Epoch:    2/6     Loss: 4.220286897659301\n",
      "\n",
      "Epoch:    2/6     Loss: 4.30948206949234\n",
      "\n",
      "Epoch:    2/6     Loss: 4.375392034530639\n",
      "\n",
      "Epoch:    2/6     Loss: 4.355157259464264\n",
      "\n",
      "Epoch:    2/6     Loss: 4.330185353755951\n",
      "\n",
      "Epoch:    2/6     Loss: 4.358835931777954\n",
      "\n",
      "Epoch:    2/6     Loss: 4.358094181060791\n",
      "\n",
      "Epoch:    2/6     Loss: 4.340825248718262\n",
      "\n",
      "Epoch:    2/6     Loss: 4.329459800243377\n",
      "\n",
      "Epoch:    2/6     Loss: 4.3431539039611815\n",
      "\n",
      "Epoch:    2/6     Loss: 4.331312223434448\n",
      "\n",
      "Epoch:    2/6     Loss: 4.304586505889892\n",
      "\n",
      "Epoch:    2/6     Loss: 4.278025516033173\n",
      "\n",
      "Epoch:    2/6     Loss: 4.360925302982331\n",
      "\n",
      "Epoch:    2/6     Loss: 4.337044331550598\n",
      "\n",
      "Epoch:    2/6     Loss: 4.3635128335952755\n",
      "\n",
      "Epoch:    2/6     Loss: 4.40333637714386\n",
      "\n",
      "Epoch:    2/6     Loss: 4.386283615112305\n",
      "\n",
      "Epoch:    2/6     Loss: 4.387636853694916\n",
      "\n",
      "Epoch:    2/6     Loss: 4.316885260105133\n",
      "\n",
      "Epoch:    2/6     Loss: 4.358449684619903\n",
      "\n",
      "Epoch:    2/6     Loss: 4.325638465881347\n",
      "\n",
      "Epoch:    2/6     Loss: 4.361772323131562\n",
      "\n",
      "Epoch:    2/6     Loss: 4.369630922794342\n",
      "\n",
      "Epoch:    2/6     Loss: 4.3009457712173464\n",
      "\n",
      "Epoch:    2/6     Loss: 4.292300718307495\n",
      "\n",
      "Epoch:    2/6     Loss: 4.335692317008972\n",
      "\n",
      "Epoch:    2/6     Loss: 4.362612679958343\n",
      "\n",
      "Epoch:    2/6     Loss: 4.34149726819992\n",
      "\n",
      "Epoch:    2/6     Loss: 4.351125703334809\n",
      "\n",
      "Epoch:    2/6     Loss: 4.298160389900207\n",
      "\n",
      "Epoch:    2/6     Loss: 4.313467939853668\n",
      "\n",
      "Epoch:    2/6     Loss: 4.38067727804184\n",
      "\n",
      "Epoch:    2/6     Loss: 4.341505874633789\n",
      "\n",
      "Epoch:    3/6     Loss: 4.297112948577339\n",
      "\n",
      "Epoch:    3/6     Loss: 4.251842072486878\n",
      "\n",
      "Epoch:    3/6     Loss: 4.242411389350891\n",
      "\n",
      "Epoch:    3/6     Loss: 4.263036450386047\n",
      "\n",
      "Epoch:    3/6     Loss: 4.215744866371155\n",
      "\n",
      "Epoch:    3/6     Loss: 4.2810477814674375\n",
      "\n",
      "Epoch:    3/6     Loss: 4.232729960918427\n",
      "\n",
      "Epoch:    3/6     Loss: 4.26682990026474\n",
      "\n",
      "Epoch:    3/6     Loss: 4.268178519248963\n",
      "\n",
      "Epoch:    3/6     Loss: 4.2524546780586245\n",
      "\n",
      "Epoch:    3/6     Loss: 4.284310310840607\n",
      "\n",
      "Epoch:    3/6     Loss: 4.2357911195755005\n",
      "\n",
      "Epoch:    3/6     Loss: 4.2537705864906314\n",
      "\n",
      "Epoch:    3/6     Loss: 4.2468522610664365\n",
      "\n",
      "Epoch:    3/6     Loss: 4.291675462245941\n",
      "\n",
      "Epoch:    3/6     Loss: 4.291636218547821\n",
      "\n",
      "Epoch:    3/6     Loss: 4.319695935726166\n",
      "\n",
      "Epoch:    3/6     Loss: 4.2587246370315555\n",
      "\n",
      "Epoch:    3/6     Loss: 4.317487360954285\n",
      "\n",
      "Epoch:    3/6     Loss: 4.284208433151245\n",
      "\n",
      "Epoch:    3/6     Loss: 4.25315761756897\n",
      "\n",
      "Epoch:    3/6     Loss: 4.179068434238434\n",
      "\n",
      "Epoch:    3/6     Loss: 4.27910374879837\n",
      "\n",
      "Epoch:    3/6     Loss: 4.309640462875366\n",
      "\n",
      "Epoch:    3/6     Loss: 4.295538711071014\n",
      "\n",
      "Epoch:    3/6     Loss: 4.269431962966919\n",
      "\n",
      "Epoch:    3/6     Loss: 4.253035593986511\n",
      "\n",
      "Epoch:    3/6     Loss: 4.221095680236816\n",
      "\n",
      "Epoch:    3/6     Loss: 4.324777279376984\n",
      "\n",
      "Epoch:    3/6     Loss: 4.256996763706208\n",
      "\n",
      "Epoch:    3/6     Loss: 4.305589493274689\n",
      "\n",
      "Epoch:    3/6     Loss: 4.264681200981141\n",
      "\n",
      "Epoch:    3/6     Loss: 4.2106413407325745\n",
      "\n",
      "Epoch:    3/6     Loss: 4.247989062309265\n",
      "\n",
      "Epoch:    3/6     Loss: 4.254265688896179\n",
      "\n",
      "Epoch:    3/6     Loss: 4.253300138473511\n",
      "\n",
      "Epoch:    3/6     Loss: 4.263430927753449\n",
      "\n",
      "Epoch:    3/6     Loss: 4.313189587116241\n",
      "\n",
      "Epoch:    3/6     Loss: 4.2961508355140685\n",
      "\n",
      "Epoch:    3/6     Loss: 4.307748294353485\n",
      "\n",
      "Epoch:    3/6     Loss: 4.279314478397369\n",
      "\n",
      "Epoch:    3/6     Loss: 4.306174078464508\n",
      "\n",
      "Epoch:    3/6     Loss: 4.235829972267151\n",
      "\n",
      "Epoch:    3/6     Loss: 4.275776389122009\n",
      "\n",
      "Epoch:    3/6     Loss: 4.244743484020233\n",
      "\n",
      "Epoch:    3/6     Loss: 4.261920366764069\n",
      "\n",
      "Epoch:    3/6     Loss: 4.319067988395691\n",
      "\n",
      "Epoch:    3/6     Loss: 4.286671767711639\n",
      "\n",
      "Epoch:    3/6     Loss: 4.257329419612884\n",
      "\n",
      "Epoch:    3/6     Loss: 4.219883731365204\n",
      "\n",
      "Epoch:    3/6     Loss: 4.301542709350586\n",
      "\n",
      "Epoch:    3/6     Loss: 4.243861968994141\n",
      "\n",
      "Epoch:    3/6     Loss: 4.256505951881409\n",
      "\n",
      "Epoch:    3/6     Loss: 4.278557065963745\n",
      "\n",
      "Epoch:    3/6     Loss: 4.305141835689545\n",
      "\n",
      "Epoch:    4/6     Loss: 4.230327012342051\n",
      "\n",
      "Epoch:    4/6     Loss: 4.249119858264923\n",
      "\n",
      "Epoch:    4/6     Loss: 4.1780069289207455\n",
      "\n",
      "Epoch:    4/6     Loss: 4.207733062744141\n",
      "\n",
      "Epoch:    4/6     Loss: 4.180700419425964\n",
      "\n",
      "Epoch:    4/6     Loss: 4.169172397613526\n",
      "\n",
      "Epoch:    4/6     Loss: 4.184490277290344\n",
      "\n",
      "Epoch:    4/6     Loss: 4.214334466934204\n",
      "\n",
      "Epoch:    4/6     Loss: 4.227661234855652\n",
      "\n",
      "Epoch:    4/6     Loss: 4.229136524677276\n",
      "\n",
      "Epoch:    4/6     Loss: 4.210945460796356\n",
      "\n",
      "Epoch:    4/6     Loss: 4.218902178764344\n",
      "\n",
      "Epoch:    4/6     Loss: 4.18569588136673\n",
      "\n",
      "Epoch:    4/6     Loss: 4.2557807841300965\n",
      "\n",
      "Epoch:    4/6     Loss: 4.2300594625473025\n",
      "\n",
      "Epoch:    4/6     Loss: 4.2510426864624025\n",
      "\n",
      "Epoch:    4/6     Loss: 4.238143540382385\n",
      "\n",
      "Epoch:    4/6     Loss: 4.181602203845978\n",
      "\n",
      "Epoch:    4/6     Loss: 4.23384601020813\n",
      "\n",
      "Epoch:    4/6     Loss: 4.243624746322632\n",
      "\n",
      "Epoch:    4/6     Loss: 4.235107341766358\n",
      "\n",
      "Epoch:    4/6     Loss: 4.289003617286682\n",
      "\n",
      "Epoch:    4/6     Loss: 4.2454900598526\n",
      "\n",
      "Epoch:    4/6     Loss: 4.256924128532409\n",
      "\n",
      "Epoch:    4/6     Loss: 4.222599952697754\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    4/6     Loss: 4.189552246570587\n",
      "\n",
      "Epoch:    4/6     Loss: 4.201244663238525\n",
      "\n",
      "Epoch:    4/6     Loss: 4.177928785324097\n",
      "\n",
      "Epoch:    4/6     Loss: 4.222064717769623\n",
      "\n",
      "Epoch:    4/6     Loss: 4.279292543888092\n",
      "\n",
      "Epoch:    4/6     Loss: 4.246867390632629\n",
      "\n",
      "Epoch:    4/6     Loss: 4.244134308815003\n",
      "\n",
      "Epoch:    4/6     Loss: 4.250814400672913\n",
      "\n",
      "Epoch:    4/6     Loss: 4.237067190170288\n",
      "\n",
      "Epoch:    4/6     Loss: 4.297513787746429\n",
      "\n",
      "Epoch:    4/6     Loss: 4.226821291923523\n",
      "\n",
      "Epoch:    4/6     Loss: 4.197367931842804\n",
      "\n",
      "Epoch:    4/6     Loss: 4.281052847862243\n",
      "\n",
      "Epoch:    4/6     Loss: 4.202565054416657\n",
      "\n",
      "Epoch:    4/6     Loss: 4.212696791172028\n",
      "\n",
      "Epoch:    4/6     Loss: 4.2089838395118715\n",
      "\n",
      "Epoch:    4/6     Loss: 4.242517303466797\n",
      "\n",
      "Epoch:    4/6     Loss: 4.3000672011375425\n",
      "\n",
      "Epoch:    4/6     Loss: 4.258489910602569\n",
      "\n",
      "Epoch:    4/6     Loss: 4.232053030014038\n",
      "\n",
      "Epoch:    4/6     Loss: 4.266660823345184\n",
      "\n",
      "Epoch:    4/6     Loss: 4.282714551448822\n",
      "\n",
      "Epoch:    4/6     Loss: 4.2842117805480955\n",
      "\n",
      "Epoch:    4/6     Loss: 4.257649147510529\n",
      "\n",
      "Epoch:    4/6     Loss: 4.250472623825074\n",
      "\n",
      "Epoch:    4/6     Loss: 4.237328123569489\n",
      "\n",
      "Epoch:    4/6     Loss: 4.2082679324150085\n",
      "\n",
      "Epoch:    4/6     Loss: 4.2341140995025635\n",
      "\n",
      "Epoch:    4/6     Loss: 4.241720463275909\n",
      "\n",
      "Epoch:    4/6     Loss: 4.24253375339508\n",
      "\n",
      "Epoch:    5/6     Loss: 4.19440710818002\n",
      "\n",
      "Epoch:    5/6     Loss: 4.176937668323517\n",
      "\n",
      "Epoch:    5/6     Loss: 4.130272393703461\n",
      "\n",
      "Epoch:    5/6     Loss: 4.200358955383301\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data params\n",
    "# Sequence Length\n",
    "sequence_length = 200  # of words in a sequence\n",
    "# Batch Size\n",
    "batch_size = 32\n",
    "\n",
    "# data loader - do not change\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)\n",
    "\n",
    "# Training parameters\n",
    "# Number of Epochs\n",
    "num_epochs = 6\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model parameters\n",
    "# Vocab size\n",
    "vocab_size = len(vocab_to_int)\n",
    "# Output size\n",
    "output_size = len(vocab_to_int)\n",
    "# Embedding Dimension\n",
    "embedding_dim = 200\n",
    "# Hidden Dimension\n",
    "hidden_dim = 512\n",
    "# Number of RNN Layers\n",
    "n_layers = 2\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 500\n",
    "\n",
    "# create model and move to gpu if available\n",
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "# defining loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "path = './save/trained_rnn_512'\n",
    "\n",
    "# training the model\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches, path)\n",
    "\n",
    "# saving the trained model\n",
    "helper.save_model(train_rnn, trained_rnn)\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3 - Change dropout from 0.8 to 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 6 epoch(s)...\n",
      "Epoch:    1/6     Loss: 5.734573011398315\n",
      "\n",
      "Epoch:    1/6     Loss: 5.161716278076172\n",
      "\n",
      "Epoch:    1/6     Loss: 4.998364925861359\n",
      "\n",
      "Epoch:    1/6     Loss: 4.864034643173218\n",
      "\n",
      "Epoch:    1/6     Loss: 4.795320579051971\n",
      "\n",
      "Epoch:    1/6     Loss: 4.740472389698029\n",
      "\n",
      "Epoch:    1/6     Loss: 4.699350751876831\n",
      "\n",
      "Epoch:    1/6     Loss: 4.663703182220459\n",
      "\n",
      "Epoch:    1/6     Loss: 4.634458442687988\n",
      "\n",
      "Epoch:    1/6     Loss: 4.6119224681854245\n",
      "\n",
      "Epoch:    1/6     Loss: 4.52804301404953\n",
      "\n",
      "Epoch:    1/6     Loss: 4.599263755321503\n",
      "\n",
      "Epoch:    1/6     Loss: 4.472692346096038\n",
      "\n",
      "Epoch:    1/6     Loss: 4.467523460865021\n",
      "\n",
      "Epoch:    1/6     Loss: 4.500551551342011\n",
      "\n",
      "Epoch:    1/6     Loss: 4.45882213640213\n",
      "\n",
      "Epoch:    1/6     Loss: 4.456091603279114\n",
      "\n",
      "Epoch:    1/6     Loss: 4.440711449623108\n",
      "\n",
      "Epoch:    1/6     Loss: 4.478302415370941\n",
      "\n",
      "Epoch:    1/6     Loss: 4.483567180156708\n",
      "\n",
      "Epoch:    1/6     Loss: 4.406284973621369\n",
      "\n",
      "Epoch:    1/6     Loss: 4.442367895126343\n",
      "\n",
      "Epoch:    1/6     Loss: 4.399805123329163\n",
      "\n",
      "Epoch:    1/6     Loss: 4.380669004440308\n",
      "\n",
      "Epoch:    1/6     Loss: 4.330969979763031\n",
      "\n",
      "Epoch:    1/6     Loss: 4.346683025360107\n",
      "\n",
      "Epoch:    1/6     Loss: 4.33855081319809\n",
      "\n",
      "Epoch:    1/6     Loss: 4.31176300907135\n",
      "\n",
      "Epoch:    1/6     Loss: 4.329718015193939\n",
      "\n",
      "Epoch:    1/6     Loss: 4.268311316490173\n",
      "\n",
      "Epoch:    1/6     Loss: 4.315736757755279\n",
      "\n",
      "Epoch:    1/6     Loss: 4.315679818153382\n",
      "\n",
      "Epoch:    1/6     Loss: 4.394713943481445\n",
      "\n",
      "Epoch:    1/6     Loss: 4.281537644386291\n",
      "\n",
      "Epoch:    1/6     Loss: 4.2849920134544375\n",
      "\n",
      "Epoch:    1/6     Loss: 4.358104929447174\n",
      "\n",
      "Epoch:    1/6     Loss: 4.3066707601547245\n",
      "\n",
      "Epoch:    1/6     Loss: 4.258146995067596\n",
      "\n",
      "Epoch:    1/6     Loss: 4.27384174489975\n",
      "\n",
      "Epoch:    1/6     Loss: 4.325539772987366\n",
      "\n",
      "Epoch:    1/6     Loss: 4.274967180252075\n",
      "\n",
      "Epoch:    1/6     Loss: 4.294312523841858\n",
      "\n",
      "Epoch:    1/6     Loss: 4.213638094425201\n",
      "\n",
      "Epoch:    1/6     Loss: 4.290584919929504\n",
      "\n",
      "Epoch:    1/6     Loss: 4.2361456422805785\n",
      "\n",
      "Epoch:    1/6     Loss: 4.269218295574189\n",
      "\n",
      "Epoch:    1/6     Loss: 4.253614478588104\n",
      "\n",
      "Epoch:    1/6     Loss: 4.277387089252472\n",
      "\n",
      "Epoch:    1/6     Loss: 4.227234152793884\n",
      "\n",
      "Epoch:    1/6     Loss: 4.23446344089508\n",
      "\n",
      "Epoch:    1/6     Loss: 4.225768690109253\n",
      "\n",
      "Epoch:    1/6     Loss: 4.220718732833863\n",
      "\n",
      "Epoch:    1/6     Loss: 4.259363542556763\n",
      "\n",
      "Epoch:    1/6     Loss: 4.255987502098083\n",
      "\n",
      "Epoch:    1/6     Loss: 4.276127859115601\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    2/6     Loss: 4.1630324029594385\n",
      "\n",
      "Epoch:    2/6     Loss: 4.097840004920959\n",
      "\n",
      "Epoch:    2/6     Loss: 4.101337086677551\n",
      "\n",
      "Epoch:    2/6     Loss: 4.083431650161743\n",
      "\n",
      "Epoch:    2/6     Loss: 4.07443700838089\n",
      "\n",
      "Epoch:    2/6     Loss: 4.1117211842536925\n",
      "\n",
      "Epoch:    2/6     Loss: 4.056476910591125\n",
      "\n",
      "Epoch:    2/6     Loss: 4.141862450122833\n",
      "\n",
      "Epoch:    2/6     Loss: 4.1032451667785645\n",
      "\n",
      "Epoch:    2/6     Loss: 4.1334022841453555\n",
      "\n",
      "Epoch:    2/6     Loss: 4.095456813812256\n",
      "\n",
      "Epoch:    2/6     Loss: 4.162261750221252\n",
      "\n",
      "Epoch:    2/6     Loss: 4.061561032772064\n",
      "\n",
      "Epoch:    2/6     Loss: 4.104068965435028\n",
      "\n",
      "Epoch:    2/6     Loss: 4.095857221841812\n",
      "\n",
      "Epoch:    2/6     Loss: 4.135977490901947\n",
      "\n",
      "Epoch:    2/6     Loss: 4.138124445438385\n",
      "\n",
      "Epoch:    2/6     Loss: 4.114831743717193\n",
      "\n",
      "Epoch:    2/6     Loss: 4.140637580871582\n",
      "\n",
      "Epoch:    2/6     Loss: 4.120273303985596\n",
      "\n",
      "Epoch:    2/6     Loss: 4.165741947650909\n",
      "\n",
      "Epoch:    2/6     Loss: 4.171623256206512\n",
      "\n",
      "Epoch:    2/6     Loss: 4.099943322181701\n",
      "\n",
      "Epoch:    2/6     Loss: 4.18346963262558\n",
      "\n",
      "Epoch:    2/6     Loss: 4.159337522029877\n",
      "\n",
      "Epoch:    2/6     Loss: 4.1568827686309815\n",
      "\n",
      "Epoch:    2/6     Loss: 4.1603931984901426\n",
      "\n",
      "Epoch:    2/6     Loss: 4.1170470972061155\n",
      "\n",
      "Epoch:    2/6     Loss: 4.141941713809967\n",
      "\n",
      "Epoch:    2/6     Loss: 4.162903012275696\n",
      "\n",
      "Epoch:    2/6     Loss: 4.076460962772369\n",
      "\n",
      "Epoch:    2/6     Loss: 4.159330914974213\n",
      "\n",
      "Epoch:    2/6     Loss: 4.186647349834442\n",
      "\n",
      "Epoch:    2/6     Loss: 4.154785650253296\n",
      "\n",
      "Epoch:    2/6     Loss: 4.100212655544281\n",
      "\n",
      "Epoch:    2/6     Loss: 4.1359234623909\n",
      "\n",
      "Epoch:    2/6     Loss: 4.073352517604828\n",
      "\n",
      "Epoch:    2/6     Loss: 4.173784779548645\n",
      "\n",
      "Epoch:    2/6     Loss: 4.145089648246765\n",
      "\n",
      "Epoch:    2/6     Loss: 4.1079058847427365\n",
      "\n",
      "Epoch:    2/6     Loss: 4.119761858940125\n",
      "\n",
      "Epoch:    2/6     Loss: 4.104930490016938\n",
      "\n",
      "Epoch:    2/6     Loss: 4.0784977097511295\n",
      "\n",
      "Epoch:    2/6     Loss: 4.134409959316254\n",
      "\n",
      "Epoch:    2/6     Loss: 4.203175148963928\n",
      "\n",
      "Epoch:    2/6     Loss: 4.184431490421296\n",
      "\n",
      "Epoch:    2/6     Loss: 4.115237551212311\n",
      "\n",
      "Epoch:    2/6     Loss: 4.152225522518158\n",
      "\n",
      "Epoch:    2/6     Loss: 4.173791699409485\n",
      "\n",
      "Epoch:    2/6     Loss: 4.166709917545319\n",
      "\n",
      "Epoch:    2/6     Loss: 4.14659392786026\n",
      "\n",
      "Epoch:    2/6     Loss: 4.090330111026764\n",
      "\n",
      "Epoch:    2/6     Loss: 4.111056470394135\n",
      "\n",
      "Epoch:    2/6     Loss: 4.1320021824836735\n",
      "\n",
      "Epoch:    2/6     Loss: 4.099354892730713\n",
      "\n",
      "Epoch:    3/6     Loss: 4.069053848402215\n",
      "\n",
      "Epoch:    3/6     Loss: 3.9710193395614626\n",
      "\n",
      "Epoch:    3/6     Loss: 3.938680069446564\n",
      "\n",
      "Epoch:    3/6     Loss: 3.9763437371253967\n",
      "\n",
      "Epoch:    3/6     Loss: 3.9963629231452944\n",
      "\n",
      "Epoch:    3/6     Loss: 4.00918001794815\n",
      "\n",
      "Epoch:    3/6     Loss: 4.040466979503631\n",
      "\n",
      "Epoch:    3/6     Loss: 3.997437285423279\n",
      "\n",
      "Epoch:    3/6     Loss: 4.038129056930542\n",
      "\n",
      "Epoch:    3/6     Loss: 4.018391320705414\n",
      "\n",
      "Epoch:    3/6     Loss: 4.025431872367859\n",
      "\n",
      "Epoch:    3/6     Loss: 4.052704457759857\n",
      "\n",
      "Epoch:    3/6     Loss: 3.989874467372894\n",
      "\n",
      "Epoch:    3/6     Loss: 4.027693778991699\n",
      "\n",
      "Epoch:    3/6     Loss: 4.037446715354919\n",
      "\n",
      "Epoch:    3/6     Loss: 4.065974574565887\n",
      "\n",
      "Epoch:    3/6     Loss: 4.097498054504395\n",
      "\n",
      "Epoch:    3/6     Loss: 4.012687634944916\n",
      "\n",
      "Epoch:    3/6     Loss: 4.019154517173767\n",
      "\n",
      "Epoch:    3/6     Loss: 4.03164363861084\n",
      "\n",
      "Epoch:    3/6     Loss: 3.9988299980163573\n",
      "\n",
      "Epoch:    3/6     Loss: 4.030716121673584\n",
      "\n",
      "Epoch:    3/6     Loss: 4.052086616516113\n",
      "\n",
      "Epoch:    3/6     Loss: 4.09138941860199\n",
      "\n",
      "Epoch:    3/6     Loss: 4.060449463844299\n",
      "\n",
      "Epoch:    3/6     Loss: 4.0305998101234435\n",
      "\n",
      "Epoch:    3/6     Loss: 4.0303990840911865\n",
      "\n",
      "Epoch:    3/6     Loss: 4.010350572109222\n",
      "\n",
      "Epoch:    3/6     Loss: 4.008992352485657\n",
      "\n",
      "Epoch:    3/6     Loss: 4.087241001605988\n",
      "\n",
      "Epoch:    3/6     Loss: 4.063052725791931\n",
      "\n",
      "Epoch:    3/6     Loss: 4.042995770931244\n",
      "\n",
      "Epoch:    3/6     Loss: 3.9975667357444764\n",
      "\n",
      "Epoch:    3/6     Loss: 4.058455514907837\n",
      "\n",
      "Epoch:    3/6     Loss: 4.057075839996338\n",
      "\n",
      "Epoch:    3/6     Loss: 4.029744573116303\n",
      "\n",
      "Epoch:    3/6     Loss: 4.065016237735748\n",
      "\n",
      "Epoch:    3/6     Loss: 3.991884696006775\n",
      "\n",
      "Epoch:    3/6     Loss: 4.010676177978516\n",
      "\n",
      "Epoch:    3/6     Loss: 4.114911821365356\n",
      "\n",
      "Epoch:    3/6     Loss: 4.053815777778626\n",
      "\n",
      "Epoch:    3/6     Loss: 4.073090193271637\n",
      "\n",
      "Epoch:    3/6     Loss: 4.043966192722321\n",
      "\n",
      "Epoch:    3/6     Loss: 4.065809688091278\n",
      "\n",
      "Epoch:    3/6     Loss: 4.026040276527405\n",
      "\n",
      "Epoch:    3/6     Loss: 4.063980178833008\n",
      "\n",
      "Epoch:    3/6     Loss: 4.045703313827515\n",
      "\n",
      "Epoch:    3/6     Loss: 4.054837032794953\n",
      "\n",
      "Epoch:    3/6     Loss: 4.079871048927307\n",
      "\n",
      "Epoch:    3/6     Loss: 4.016629733085632\n",
      "\n",
      "Epoch:    3/6     Loss: 4.090097390651703\n",
      "\n",
      "Epoch:    3/6     Loss: 4.032795362949371\n",
      "\n",
      "Epoch:    3/6     Loss: 4.10973552942276\n",
      "\n",
      "Epoch:    3/6     Loss: 4.065458423614502\n",
      "\n",
      "Epoch:    3/6     Loss: 4.062570565223694\n",
      "\n",
      "Epoch:    4/6     Loss: 4.011098428603706\n",
      "\n",
      "Epoch:    4/6     Loss: 3.9262899050712585\n",
      "\n",
      "Epoch:    4/6     Loss: 3.9181591639518736\n",
      "\n",
      "Epoch:    4/6     Loss: 3.9518365693092345\n",
      "\n",
      "Epoch:    4/6     Loss: 3.9433692297935488\n",
      "\n",
      "Epoch:    4/6     Loss: 3.9339434146881103\n",
      "\n",
      "Epoch:    4/6     Loss: 3.914091079711914\n",
      "\n",
      "Epoch:    4/6     Loss: 3.9675949325561524\n",
      "\n",
      "Epoch:    4/6     Loss: 3.9798534684181215\n",
      "\n",
      "Epoch:    4/6     Loss: 3.9614515867233275\n",
      "\n",
      "Epoch:    4/6     Loss: 3.9666045384407043\n",
      "\n",
      "Epoch:    4/6     Loss: 3.9151292333602905\n",
      "\n",
      "Epoch:    4/6     Loss: 3.9685350589752195\n",
      "\n",
      "Epoch:    4/6     Loss: 3.9335706024169923\n",
      "\n",
      "Epoch:    4/6     Loss: 3.9790017499923707\n",
      "\n",
      "Epoch:    4/6     Loss: 3.9237945847511293\n",
      "\n",
      "Epoch:    4/6     Loss: 3.9907501487731936\n",
      "\n",
      "Epoch:    4/6     Loss: 3.9558706727027895\n",
      "\n",
      "Epoch:    4/6     Loss: 3.982647566795349\n",
      "\n",
      "Epoch:    4/6     Loss: 3.9888503456115725\n",
      "\n",
      "Epoch:    4/6     Loss: 3.9522872877120974\n",
      "\n",
      "Epoch:    4/6     Loss: 3.9638909316062927\n",
      "\n",
      "Epoch:    4/6     Loss: 3.9552328596115114\n",
      "\n",
      "Epoch:    4/6     Loss: 4.010176687717438\n",
      "\n",
      "Epoch:    4/6     Loss: 3.9833824877738953\n",
      "\n",
      "Epoch:    4/6     Loss: 3.953283417224884\n",
      "\n",
      "Epoch:    4/6     Loss: 4.009274097442627\n",
      "\n",
      "Epoch:    4/6     Loss: 3.9908357434272768\n",
      "\n",
      "Epoch:    4/6     Loss: 3.9841035671234133\n",
      "\n",
      "Epoch:    4/6     Loss: 4.032598921775818\n",
      "\n",
      "Epoch:    4/6     Loss: 3.964290300369263\n",
      "\n",
      "Epoch:    4/6     Loss: 4.04301137304306\n",
      "\n",
      "Epoch:    4/6     Loss: 3.98236088848114\n",
      "\n",
      "Epoch:    4/6     Loss: 4.005225257396698\n",
      "\n",
      "Epoch:    4/6     Loss: 4.048101193904877\n",
      "\n",
      "Epoch:    4/6     Loss: 4.015060677051544\n",
      "\n",
      "Epoch:    4/6     Loss: 4.017031503200531\n",
      "\n",
      "Epoch:    4/6     Loss: 4.030916588783264\n",
      "\n",
      "Epoch:    4/6     Loss: 4.016323417663574\n",
      "\n",
      "Epoch:    4/6     Loss: 4.003049736022949\n",
      "\n",
      "Epoch:    4/6     Loss: 4.036227147102356\n",
      "\n",
      "Epoch:    4/6     Loss: 4.01789319562912\n",
      "\n",
      "Epoch:    4/6     Loss: 4.007470315694809\n",
      "\n",
      "Epoch:    4/6     Loss: 3.999667243003845\n",
      "\n",
      "Epoch:    4/6     Loss: 3.9893905935287477\n",
      "\n",
      "Epoch:    4/6     Loss: 4.023261392593384\n",
      "\n",
      "Epoch:    4/6     Loss: 4.016965621471405\n",
      "\n",
      "Epoch:    4/6     Loss: 3.9826890625953673\n",
      "\n",
      "Epoch:    4/6     Loss: 3.9650040774345396\n",
      "\n",
      "Epoch:    4/6     Loss: 4.012656961202621\n",
      "\n",
      "Epoch:    4/6     Loss: 4.038943428516388\n",
      "\n",
      "Epoch:    4/6     Loss: 4.0392834391593935\n",
      "\n",
      "Epoch:    4/6     Loss: 4.040809788227081\n",
      "\n",
      "Epoch:    4/6     Loss: 4.003805331230163\n",
      "\n",
      "Epoch:    4/6     Loss: 4.007789224147797\n",
      "\n",
      "Epoch:    5/6     Loss: 3.939899031175386\n",
      "\n",
      "Epoch:    5/6     Loss: 3.8649765253067017\n",
      "\n",
      "Epoch:    5/6     Loss: 3.8272748284339904\n",
      "\n",
      "Epoch:    5/6     Loss: 3.8957048540115355\n",
      "\n",
      "Epoch:    5/6     Loss: 3.864227767467499\n",
      "\n",
      "Epoch:    5/6     Loss: 3.86326823759079\n",
      "\n",
      "Epoch:    5/6     Loss: 3.873670136451721\n",
      "\n",
      "Epoch:    5/6     Loss: 3.8894236421585084\n",
      "\n",
      "Epoch:    5/6     Loss: 3.881732319831848\n",
      "\n",
      "Epoch:    5/6     Loss: 3.8690817680358887\n",
      "\n",
      "Epoch:    5/6     Loss: 3.958586105823517\n",
      "\n",
      "Epoch:    5/6     Loss: 3.933482340335846\n",
      "\n",
      "Epoch:    5/6     Loss: 3.902973682880402\n",
      "\n",
      "Epoch:    5/6     Loss: 3.9540285286903383\n",
      "\n",
      "Epoch:    5/6     Loss: 3.9308201565742493\n",
      "\n",
      "Epoch:    5/6     Loss: 3.9440885500907896\n",
      "\n",
      "Epoch:    5/6     Loss: 3.942012936592102\n",
      "\n",
      "Epoch:    5/6     Loss: 3.966212111711502\n",
      "\n",
      "Epoch:    5/6     Loss: 3.952184730529785\n",
      "\n",
      "Epoch:    5/6     Loss: 3.9491336510181427\n",
      "\n",
      "Epoch:    5/6     Loss: 3.903294276237488\n",
      "\n",
      "Epoch:    5/6     Loss: 3.9634782886505127\n",
      "\n",
      "Epoch:    5/6     Loss: 3.9413594193458557\n",
      "\n",
      "Epoch:    5/6     Loss: 3.9108921566009522\n",
      "\n",
      "Epoch:    5/6     Loss: 3.956954596042633\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    5/6     Loss: 3.964633089065552\n",
      "\n",
      "Epoch:    5/6     Loss: 3.9608815474510193\n",
      "\n",
      "Epoch:    5/6     Loss: 3.906858479976654\n",
      "\n",
      "Epoch:    5/6     Loss: 3.941682635784149\n",
      "\n",
      "Epoch:    5/6     Loss: 3.9455833921432495\n",
      "\n",
      "Epoch:    5/6     Loss: 3.963539057254791\n",
      "\n",
      "Epoch:    5/6     Loss: 3.9656281604766845\n",
      "\n",
      "Epoch:    5/6     Loss: 3.9543934082984924\n",
      "\n",
      "Epoch:    5/6     Loss: 3.9516017498970033\n",
      "\n",
      "Epoch:    5/6     Loss: 3.9790806555747986\n",
      "\n",
      "Epoch:    5/6     Loss: 3.9919206614494325\n",
      "\n",
      "Epoch:    5/6     Loss: 4.0318493924140935\n",
      "\n",
      "Epoch:    5/6     Loss: 3.9607060680389403\n",
      "\n",
      "Epoch:    5/6     Loss: 3.984339373588562\n",
      "\n",
      "Epoch:    5/6     Loss: 3.9661324362754824\n",
      "\n",
      "Epoch:    5/6     Loss: 3.9560680980682372\n",
      "\n",
      "Epoch:    5/6     Loss: 3.967694787502289\n",
      "\n",
      "Epoch:    5/6     Loss: 4.01798328256607\n",
      "\n",
      "Epoch:    5/6     Loss: 3.951677890777588\n",
      "\n",
      "Epoch:    5/6     Loss: 3.9796190295219422\n",
      "\n",
      "Epoch:    5/6     Loss: 3.967918318271637\n",
      "\n",
      "Epoch:    5/6     Loss: 3.9741097240448\n",
      "\n",
      "Epoch:    5/6     Loss: 3.9593568115234374\n",
      "\n",
      "Epoch:    5/6     Loss: 3.94389018201828\n",
      "\n",
      "Epoch:    5/6     Loss: 3.9509979734420777\n",
      "\n",
      "Epoch:    5/6     Loss: 4.008028597354889\n",
      "\n",
      "Epoch:    5/6     Loss: 3.9888165001869202\n",
      "\n",
      "Epoch:    5/6     Loss: 3.9440324568748473\n",
      "\n",
      "Epoch:    5/6     Loss: 3.997589494228363\n",
      "\n",
      "Epoch:    5/6     Loss: 3.937285824775696\n",
      "\n",
      "Epoch:    6/6     Loss: 3.9053554441950737\n",
      "\n",
      "Epoch:    6/6     Loss: 3.829560544013977\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-68b09c756af5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mactive_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# training the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mtrained_rnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_every_n_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# saving the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-3d24a5366596>\u001b[0m in \u001b[0;36mtrain_rnn\u001b[0;34m(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches, path)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# forward, back prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_back_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;31m# record loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mbatch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-c3071b60fc82>\u001b[0m in \u001b[0;36mforward_back_prop\u001b[0;34m(rnn, optimizer, criterion, inp, target, hidden)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msftmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Data params\n",
    "# Sequence Length\n",
    "sequence_length = 200  # of words in a sequence\n",
    "# Batch Size\n",
    "batch_size = 32\n",
    "\n",
    "# data loader - do not change\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)\n",
    "\n",
    "# Training parameters\n",
    "# Number of Epochs\n",
    "num_epochs = 6\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model parameters\n",
    "# Vocab size\n",
    "vocab_size = len(vocab_to_int)\n",
    "# Output size\n",
    "output_size = len(vocab_to_int)\n",
    "# Embedding Dimension\n",
    "embedding_dim = 200\n",
    "# Hidden Dimension\n",
    "hidden_dim = 256\n",
    "# Number of RNN Layers\n",
    "n_layers = 2\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 500\n",
    "\n",
    "# create model and move to gpu if available\n",
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.2)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "# defining loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "path = './save/trained_rnn_256_02'\n",
    "\n",
    "# training the model\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches, path)\n",
    "\n",
    "# saving the trained model\n",
    "helper.save_model(path, trained_rnn)\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp 4. Reduce embedding size from 200 to 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5 epoch(s)...\n",
      "Epoch:    1/5     Loss: 5.721465928077698\n",
      "\n",
      "Epoch:    1/5     Loss: 5.162940397262573\n",
      "\n",
      "Epoch:    1/5     Loss: 5.0087122883796695\n",
      "\n",
      "Epoch:    1/5     Loss: 4.88570308637619\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0d46de789d10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mactive_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# training the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mtrained_rnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_every_n_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# saving the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-8bdcdd33eb1a>\u001b[0m in \u001b[0;36mtrain_rnn\u001b[0;34m(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches, path)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# forward, back prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_back_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;31m# record loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mbatch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-c3071b60fc82>\u001b[0m in \u001b[0;36mforward_back_prop\u001b[0;34m(rnn, optimizer, criterion, inp, target, hidden)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msftmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Data params\n",
    "# Sequence Length\n",
    "sequence_length = 50  # of words in a sequence\n",
    "# Batch Size\n",
    "batch_size = 32\n",
    "\n",
    "# data loader - do not change\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)\n",
    "\n",
    "# Training parameters\n",
    "# Number of Epochs\n",
    "num_epochs = 5\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model parameters\n",
    "# Vocab size\n",
    "vocab_size = len(vocab_to_int)\n",
    "# Output size\n",
    "output_size = len(vocab_to_int)\n",
    "# Embedding Dimension\n",
    "embedding_dim = 200\n",
    "# Hidden Dimension\n",
    "hidden_dim = 256\n",
    "# Number of RNN Layers\n",
    "n_layers = 2\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 500\n",
    "\n",
    "# create model and move to gpu if available\n",
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.2)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "# defining loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "path = './save/trained_rnn_256_02_100'\n",
    "\n",
    "# training the model\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches, path)\n",
    "\n",
    "# saving the trained model\n",
    "helper.save_model(path, trained_rnn)\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5 epoch(s)...\n",
      "Epoch:    1/5     Loss: 3.9293711585998534\n",
      "\n",
      "Epoch:    1/5     Loss: 3.8447829880714415\n",
      "\n",
      "Epoch:    1/5     Loss: 3.8469375219345094\n",
      "\n",
      "Epoch:    1/5     Loss: 3.8614208927154543\n",
      "\n",
      "Epoch:    1/5     Loss: 3.8364244422912597\n",
      "\n",
      "Epoch:    1/5     Loss: 3.849354079723358\n",
      "\n",
      "Epoch:    1/5     Loss: 3.9086210403442383\n",
      "\n",
      "Epoch:    1/5     Loss: 3.8789994926452636\n",
      "\n",
      "Epoch:    1/5     Loss: 3.8899957847595217\n",
      "\n",
      "Epoch:    1/5     Loss: 3.835445370674133\n",
      "\n",
      "Epoch:    1/5     Loss: 3.858889109134674\n",
      "\n",
      "Epoch:    1/5     Loss: 3.906621141910553\n",
      "\n",
      "Epoch:    1/5     Loss: 3.837168719768524\n",
      "\n",
      "Epoch:    1/5     Loss: 3.9131970920562744\n",
      "\n",
      "Epoch:    1/5     Loss: 3.914961143016815\n",
      "\n",
      "Epoch:    1/5     Loss: 3.895817253112793\n",
      "\n",
      "Epoch:    1/5     Loss: 3.9045004959106446\n",
      "\n",
      "Epoch:    1/5     Loss: 3.8762718186378478\n",
      "\n",
      "Epoch:    1/5     Loss: 3.8512273254394533\n",
      "\n",
      "Epoch:    1/5     Loss: 3.8808273396492003\n",
      "\n",
      "Epoch:    1/5     Loss: 3.9179111909866333\n",
      "\n",
      "Epoch:    1/5     Loss: 3.8664931020736693\n",
      "\n",
      "Epoch:    1/5     Loss: 3.8714547843933107\n",
      "\n",
      "Epoch:    1/5     Loss: 3.856818665981293\n",
      "\n",
      "Epoch:    1/5     Loss: 3.924957166671753\n",
      "\n",
      "Epoch:    1/5     Loss: 3.9106251034736634\n",
      "\n",
      "Epoch:    1/5     Loss: 3.9421687607765197\n",
      "\n",
      "Epoch:    1/5     Loss: 3.97222690820694\n",
      "\n",
      "Epoch:    1/5     Loss: 3.9149973998069765\n",
      "\n",
      "Epoch:    1/5     Loss: 3.9110811548233033\n",
      "\n",
      "Epoch:    1/5     Loss: 3.9679180302619934\n",
      "\n",
      "Epoch:    1/5     Loss: 3.8566539993286133\n",
      "\n",
      "Epoch:    1/5     Loss: 3.9704272618293763\n",
      "\n",
      "Epoch:    1/5     Loss: 3.913129883766174\n",
      "\n",
      "Epoch:    1/5     Loss: 3.9861270627975465\n",
      "\n",
      "Epoch:    1/5     Loss: 3.9163246021270752\n",
      "\n",
      "Epoch:    1/5     Loss: 3.9468586854934693\n",
      "\n",
      "Epoch:    1/5     Loss: 3.9479072518348692\n",
      "\n",
      "Epoch:    1/5     Loss: 3.9249535546302794\n",
      "\n",
      "Epoch:    1/5     Loss: 3.963323024749756\n",
      "\n",
      "Epoch:    1/5     Loss: 3.9347295203208925\n",
      "\n",
      "Epoch:    1/5     Loss: 3.957012840270996\n",
      "\n",
      "Epoch:    1/5     Loss: 3.9246811513900757\n",
      "\n",
      "Epoch:    1/5     Loss: 3.9862843933105467\n",
      "\n",
      "Epoch:    1/5     Loss: 3.9673847436904905\n",
      "\n",
      "Epoch:    1/5     Loss: 3.9287668533325197\n",
      "\n",
      "Epoch:    1/5     Loss: 3.9814036993980406\n",
      "\n",
      "Epoch:    1/5     Loss: 4.043101948738098\n",
      "\n",
      "Epoch:    1/5     Loss: 3.9400816402435304\n",
      "\n",
      "Epoch:    1/5     Loss: 3.952364734649658\n",
      "\n",
      "Epoch:    1/5     Loss: 3.978735070705414\n",
      "\n",
      "Epoch:    1/5     Loss: 4.022950917243958\n",
      "\n",
      "Epoch:    1/5     Loss: 3.974784828186035\n",
      "\n",
      "Epoch:    1/5     Loss: 3.931172183036804\n",
      "\n",
      "Epoch:    1/5     Loss: 4.004888807296753\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    2/5     Loss: 3.893249153818714\n",
      "\n",
      "Epoch:    2/5     Loss: 3.8289567742347717\n",
      "\n",
      "Epoch:    2/5     Loss: 3.8289521408081053\n",
      "\n",
      "Epoch:    2/5     Loss: 3.8300379774570463\n",
      "\n",
      "Epoch:    2/5     Loss: 3.848177722454071\n",
      "\n",
      "Epoch:    2/5     Loss: 3.8422219228744505\n",
      "\n",
      "Epoch:    2/5     Loss: 3.8487902970314027\n",
      "\n",
      "Epoch:    2/5     Loss: 3.8664010372161863\n",
      "\n",
      "Epoch:    2/5     Loss: 3.8133401770591737\n",
      "\n",
      "Epoch:    2/5     Loss: 3.845321710586548\n",
      "\n",
      "Epoch:    2/5     Loss: 3.8523562450408937\n",
      "\n",
      "Epoch:    2/5     Loss: 3.8518931338787077\n",
      "\n",
      "Epoch:    2/5     Loss: 3.9291054756641386\n",
      "\n",
      "Epoch:    2/5     Loss: 3.8247746891975405\n",
      "\n",
      "Epoch:    2/5     Loss: 3.876100085258484\n",
      "\n",
      "Epoch:    2/5     Loss: 3.8464357991218567\n",
      "\n",
      "Epoch:    2/5     Loss: 3.789661916255951\n",
      "\n",
      "Epoch:    2/5     Loss: 3.9045110926628115\n",
      "\n",
      "Epoch:    2/5     Loss: 3.884884579181671\n",
      "\n",
      "Epoch:    2/5     Loss: 3.8975182847976684\n",
      "\n",
      "Epoch:    2/5     Loss: 3.9083604226112367\n",
      "\n",
      "Epoch:    2/5     Loss: 3.9116014318466186\n",
      "\n",
      "Epoch:    2/5     Loss: 3.9197229857444764\n",
      "\n",
      "Epoch:    2/5     Loss: 3.8641728720664976\n",
      "\n",
      "Epoch:    2/5     Loss: 3.8680200533866884\n",
      "\n",
      "Epoch:    2/5     Loss: 3.9233213720321656\n",
      "\n",
      "Epoch:    2/5     Loss: 3.889379445552826\n",
      "\n",
      "Epoch:    2/5     Loss: 3.9397279467582704\n",
      "\n",
      "Epoch:    2/5     Loss: 3.8994318380355835\n",
      "\n",
      "Epoch:    2/5     Loss: 3.886223358154297\n",
      "\n",
      "Epoch:    2/5     Loss: 3.890931074142456\n",
      "\n",
      "Epoch:    2/5     Loss: 3.896623595237732\n",
      "\n",
      "Epoch:    2/5     Loss: 3.9156400036811827\n",
      "\n",
      "Epoch:    2/5     Loss: 3.944099663257599\n",
      "\n",
      "Epoch:    2/5     Loss: 3.9120775480270384\n",
      "\n",
      "Epoch:    2/5     Loss: 3.897767092227936\n",
      "\n",
      "Epoch:    2/5     Loss: 3.9539014682769777\n",
      "\n",
      "Epoch:    2/5     Loss: 3.889765612602234\n",
      "\n",
      "Epoch:    2/5     Loss: 3.928027838230133\n",
      "\n",
      "Epoch:    2/5     Loss: 3.950282874584198\n",
      "\n",
      "Epoch:    2/5     Loss: 3.9170335879325866\n",
      "\n",
      "Epoch:    2/5     Loss: 3.8680617594718933\n",
      "\n",
      "Epoch:    2/5     Loss: 3.947419429779053\n",
      "\n",
      "Epoch:    2/5     Loss: 3.927214336395264\n",
      "\n",
      "Epoch:    2/5     Loss: 3.9256308727264404\n",
      "\n",
      "Epoch:    2/5     Loss: 3.933312672138214\n",
      "\n",
      "Epoch:    2/5     Loss: 3.941391846179962\n",
      "\n",
      "Epoch:    2/5     Loss: 3.9714069576263427\n",
      "\n",
      "Epoch:    2/5     Loss: 3.9362509770393372\n",
      "\n",
      "Epoch:    2/5     Loss: 3.894002229690552\n",
      "\n",
      "Epoch:    2/5     Loss: 3.9517789311408995\n",
      "\n",
      "Epoch:    2/5     Loss: 3.9282391269207\n",
      "\n",
      "Epoch:    2/5     Loss: 3.9166533403396606\n",
      "\n",
      "Epoch:    2/5     Loss: 3.9545684571266175\n",
      "\n",
      "Epoch:    2/5     Loss: 3.9267708644866945\n",
      "\n",
      "Epoch:    3/5     Loss: 3.8611804291810077\n",
      "\n",
      "Epoch:    3/5     Loss: 3.7611305980682372\n",
      "\n",
      "Epoch:    3/5     Loss: 3.7427779750823973\n",
      "\n",
      "Epoch:    3/5     Loss: 3.830469810009003\n",
      "\n",
      "Epoch:    3/5     Loss: 3.78641543340683\n",
      "\n",
      "Epoch:    3/5     Loss: 3.7707492547035217\n",
      "\n",
      "Epoch:    3/5     Loss: 3.8125367078781127\n",
      "\n",
      "Epoch:    3/5     Loss: 3.813608290195465\n",
      "\n",
      "Epoch:    3/5     Loss: 3.8557933740615846\n",
      "\n",
      "Epoch:    3/5     Loss: 3.8566682300567625\n",
      "\n",
      "Epoch:    3/5     Loss: 3.814795744419098\n",
      "\n",
      "Epoch:    3/5     Loss: 3.892044676065445\n",
      "\n",
      "Epoch:    3/5     Loss: 3.868508530139923\n",
      "\n",
      "Epoch:    3/5     Loss: 3.8338334484100343\n",
      "\n",
      "Epoch:    3/5     Loss: 3.818845953941345\n",
      "\n",
      "Epoch:    3/5     Loss: 3.8964302778244018\n",
      "\n",
      "Epoch:    3/5     Loss: 3.8430754532814024\n",
      "\n",
      "Epoch:    3/5     Loss: 3.876396176815033\n",
      "\n",
      "Epoch:    3/5     Loss: 3.8422243909835814\n",
      "\n",
      "Epoch:    3/5     Loss: 3.843063115119934\n",
      "\n",
      "Epoch:    3/5     Loss: 3.8525262336730957\n",
      "\n",
      "Epoch:    3/5     Loss: 3.8704852652549744\n",
      "\n",
      "Epoch:    3/5     Loss: 3.864109180688858\n",
      "\n",
      "Epoch:    3/5     Loss: 3.910099524974823\n",
      "\n",
      "Epoch:    3/5     Loss: 3.9067847094535826\n",
      "\n",
      "Epoch:    3/5     Loss: 3.8725687868595124\n",
      "\n",
      "Epoch:    3/5     Loss: 3.8717904529571534\n",
      "\n",
      "Epoch:    3/5     Loss: 3.820407431125641\n",
      "\n",
      "Epoch:    3/5     Loss: 3.83090730714798\n",
      "\n",
      "Epoch:    3/5     Loss: 3.9092986297607424\n",
      "\n",
      "Epoch:    3/5     Loss: 3.90302858877182\n",
      "\n",
      "Epoch:    3/5     Loss: 3.866047486305237\n",
      "\n",
      "Epoch:    3/5     Loss: 3.8119906797409056\n",
      "\n",
      "Epoch:    3/5     Loss: 3.881064157962799\n",
      "\n",
      "Epoch:    3/5     Loss: 3.9461548953056336\n",
      "\n",
      "Epoch:    3/5     Loss: 3.865281713008881\n",
      "\n",
      "Epoch:    3/5     Loss: 3.897281136035919\n",
      "\n",
      "Epoch:    3/5     Loss: 3.8815948762893675\n",
      "\n",
      "Epoch:    3/5     Loss: 3.9213648052215575\n",
      "\n",
      "Epoch:    3/5     Loss: 3.8545336008071898\n",
      "\n",
      "Epoch:    3/5     Loss: 3.9980184655189515\n",
      "\n",
      "Epoch:    3/5     Loss: 3.9300157990455626\n",
      "\n",
      "Epoch:    3/5     Loss: 3.9162040457725524\n",
      "\n",
      "Epoch:    3/5     Loss: 3.911826069355011\n",
      "\n",
      "Epoch:    3/5     Loss: 3.9456055212020873\n",
      "\n",
      "Epoch:    3/5     Loss: 3.894301666736603\n",
      "\n",
      "Epoch:    3/5     Loss: 3.8825751113891602\n",
      "\n",
      "Epoch:    3/5     Loss: 3.8651834778785705\n",
      "\n",
      "Epoch:    3/5     Loss: 3.8903187766075136\n",
      "\n",
      "Epoch:    3/5     Loss: 3.90423837518692\n",
      "\n",
      "Epoch:    3/5     Loss: 3.9249811630249023\n",
      "\n",
      "Epoch:    3/5     Loss: 3.913261529445648\n",
      "\n",
      "Epoch:    3/5     Loss: 3.9659763317108156\n",
      "\n",
      "Epoch:    3/5     Loss: 3.8989501433372498\n",
      "\n",
      "Epoch:    3/5     Loss: 3.9530356583595276\n",
      "\n",
      "Epoch:    4/5     Loss: 3.8872994323299355\n",
      "\n",
      "Epoch:    4/5     Loss: 3.7702005252838133\n",
      "\n",
      "Epoch:    4/5     Loss: 3.770609585762024\n",
      "\n",
      "Epoch:    4/5     Loss: 3.742525137424469\n",
      "\n",
      "Epoch:    4/5     Loss: 3.773120331764221\n",
      "\n",
      "Epoch:    4/5     Loss: 3.753989077568054\n",
      "\n",
      "Epoch:    4/5     Loss: 3.7969410109519957\n",
      "\n",
      "Epoch:    4/5     Loss: 3.790436378955841\n",
      "\n",
      "Epoch:    4/5     Loss: 3.807674530506134\n",
      "\n",
      "Epoch:    4/5     Loss: 3.847514227390289\n",
      "\n",
      "Epoch:    4/5     Loss: 3.7279356915950776\n",
      "\n",
      "Epoch:    4/5     Loss: 3.7939478759765626\n",
      "\n",
      "Epoch:    4/5     Loss: 3.8131082148551942\n",
      "\n",
      "Epoch:    4/5     Loss: 3.8368409061431885\n",
      "\n",
      "Epoch:    4/5     Loss: 3.811899199485779\n",
      "\n",
      "Epoch:    4/5     Loss: 3.8994814314842223\n",
      "\n",
      "Epoch:    4/5     Loss: 3.8158766193389893\n",
      "\n",
      "Epoch:    4/5     Loss: 3.8675776162147524\n",
      "\n",
      "Epoch:    4/5     Loss: 3.849386258125305\n",
      "\n",
      "Epoch:    4/5     Loss: 3.8289927265644073\n",
      "\n",
      "Epoch:    4/5     Loss: 3.9023662123680114\n",
      "\n",
      "Epoch:    4/5     Loss: 3.8392357358932494\n",
      "\n",
      "Epoch:    4/5     Loss: 3.8749882485866545\n",
      "\n",
      "Epoch:    4/5     Loss: 3.863254322528839\n",
      "\n",
      "Epoch:    4/5     Loss: 3.8369122762680052\n",
      "\n",
      "Epoch:    4/5     Loss: 3.90355201292038\n",
      "\n",
      "Epoch:    4/5     Loss: 3.8415813002586363\n",
      "\n",
      "Epoch:    4/5     Loss: 3.864810586452484\n",
      "\n",
      "Epoch:    4/5     Loss: 3.8915501079559327\n",
      "\n",
      "Epoch:    4/5     Loss: 3.8955756554603576\n",
      "\n",
      "Epoch:    4/5     Loss: 3.814191466331482\n",
      "\n",
      "Epoch:    4/5     Loss: 3.8418839082717895\n",
      "\n",
      "Epoch:    4/5     Loss: 3.8406842708587647\n",
      "\n",
      "Epoch:    4/5     Loss: 3.9088286831378936\n",
      "\n",
      "Epoch:    4/5     Loss: 3.9042505474090574\n",
      "\n",
      "Epoch:    4/5     Loss: 3.839265770196915\n",
      "\n",
      "Epoch:    4/5     Loss: 3.8587137722969054\n",
      "\n",
      "Epoch:    4/5     Loss: 3.8394180190563203\n",
      "\n",
      "Epoch:    4/5     Loss: 3.9157542352676393\n",
      "\n",
      "Epoch:    4/5     Loss: 3.843595974445343\n",
      "\n",
      "Epoch:    4/5     Loss: 3.920462164402008\n",
      "\n",
      "Epoch:    4/5     Loss: 3.8679868001937865\n",
      "\n",
      "Epoch:    4/5     Loss: 3.9222261238098146\n",
      "\n",
      "Epoch:    4/5     Loss: 3.8842049226760866\n",
      "\n",
      "Epoch:    4/5     Loss: 3.878696183681488\n",
      "\n",
      "Epoch:    4/5     Loss: 3.883879364490509\n",
      "\n",
      "Epoch:    4/5     Loss: 3.8431197695732116\n",
      "\n",
      "Epoch:    4/5     Loss: 3.899136493682861\n",
      "\n",
      "Epoch:    4/5     Loss: 3.885763069629669\n",
      "\n",
      "Epoch:    4/5     Loss: 3.900002006530762\n",
      "\n",
      "Epoch:    4/5     Loss: 3.873050880908966\n",
      "\n",
      "Epoch:    4/5     Loss: 3.8972663540840147\n",
      "\n",
      "Epoch:    4/5     Loss: 3.910337579250336\n",
      "\n",
      "Epoch:    4/5     Loss: 3.9256237659454345\n",
      "\n",
      "Epoch:    4/5     Loss: 3.9381851620674135\n",
      "\n",
      "Epoch:    5/5     Loss: 3.8041765491167703\n",
      "\n",
      "Epoch:    5/5     Loss: 3.8115367038249968\n",
      "\n",
      "Epoch:    5/5     Loss: 3.763987209320068\n",
      "\n",
      "Epoch:    5/5     Loss: 3.7309260716438293\n",
      "\n",
      "Epoch:    5/5     Loss: 3.730459615468979\n",
      "\n",
      "Epoch:    5/5     Loss: 3.7727498865127562\n",
      "\n",
      "Epoch:    5/5     Loss: 3.7626324644088744\n",
      "\n",
      "Epoch:    5/5     Loss: 3.782371213436127\n",
      "\n",
      "Epoch:    5/5     Loss: 3.7649664025306704\n",
      "\n",
      "Epoch:    5/5     Loss: 3.816893813610077\n",
      "\n",
      "Epoch:    5/5     Loss: 3.806800485610962\n",
      "\n",
      "Epoch:    5/5     Loss: 3.7931372509002688\n",
      "\n",
      "Epoch:    5/5     Loss: 3.8105889401435853\n",
      "\n",
      "Epoch:    5/5     Loss: 3.781552614688873\n",
      "\n",
      "Epoch:    5/5     Loss: 3.8040521416664124\n",
      "\n",
      "Epoch:    5/5     Loss: 3.8167931127548216\n",
      "\n",
      "Epoch:    5/5     Loss: 3.771378267765045\n",
      "\n",
      "Epoch:    5/5     Loss: 3.810108871459961\n",
      "\n",
      "Epoch:    5/5     Loss: 3.805833303451538\n",
      "\n",
      "Epoch:    5/5     Loss: 3.8125988416671754\n",
      "\n",
      "Epoch:    5/5     Loss: 3.7708287785053254\n",
      "\n",
      "Epoch:    5/5     Loss: 3.7711476254463197\n",
      "\n",
      "Epoch:    5/5     Loss: 3.774852011203766\n",
      "\n",
      "Epoch:    5/5     Loss: 3.8486033239364623\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    5/5     Loss: 3.8501095325946806\n",
      "\n",
      "Epoch:    5/5     Loss: 3.8609207262992857\n",
      "\n",
      "Epoch:    5/5     Loss: 3.8121880140304567\n",
      "\n",
      "Epoch:    5/5     Loss: 3.825709506988525\n",
      "\n",
      "Epoch:    5/5     Loss: 3.8231336879730224\n",
      "\n",
      "Epoch:    5/5     Loss: 3.8174934535026552\n",
      "\n",
      "Epoch:    5/5     Loss: 3.822904124736786\n",
      "\n",
      "Epoch:    5/5     Loss: 3.8548113775253294\n",
      "\n",
      "Epoch:    5/5     Loss: 3.8990792741775513\n",
      "\n",
      "Epoch:    5/5     Loss: 3.889527039527893\n",
      "\n",
      "Epoch:    5/5     Loss: 3.8832168598175048\n",
      "\n",
      "Epoch:    5/5     Loss: 3.8670734872817993\n",
      "\n",
      "Epoch:    5/5     Loss: 3.886187370300293\n",
      "\n",
      "Epoch:    5/5     Loss: 3.8627657198905947\n",
      "\n",
      "Epoch:    5/5     Loss: 3.8214047574996948\n",
      "\n",
      "Epoch:    5/5     Loss: 3.890445140361786\n",
      "\n",
      "Epoch:    5/5     Loss: 3.8715633912086487\n",
      "\n",
      "Epoch:    5/5     Loss: 3.8525277614593505\n",
      "\n",
      "Epoch:    5/5     Loss: 3.8615055558681486\n",
      "\n",
      "Epoch:    5/5     Loss: 3.9045118484497072\n",
      "\n",
      "Epoch:    5/5     Loss: 3.884214285850525\n",
      "\n",
      "Epoch:    5/5     Loss: 3.877251081466675\n",
      "\n",
      "Epoch:    5/5     Loss: 3.857616949558258\n",
      "\n",
      "Epoch:    5/5     Loss: 3.935340832710266\n",
      "\n",
      "Epoch:    5/5     Loss: 3.9001657710075377\n",
      "\n",
      "Epoch:    5/5     Loss: 3.9094351534843446\n",
      "\n",
      "Epoch:    5/5     Loss: 3.9431135659217835\n",
      "\n",
      "Epoch:    5/5     Loss: 3.9064577593803405\n",
      "\n",
      "Epoch:    5/5     Loss: 3.8264343655109405\n",
      "\n",
      "Epoch:    5/5     Loss: 3.9198878269195556\n",
      "\n",
      "Epoch:    5/5     Loss: 3.878517833709717\n",
      "\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "# 5 more epochs\n",
    "\n",
    "# training the model\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches, path)\n",
    "\n",
    "# saving the trained model\n",
    "helper.save_model(path, trained_rnn)\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 30 epoch(s)...\n",
      "Epoch:    1/30    Loss: 3.780821882724762\n",
      "\n",
      "Epoch:    1/30    Loss: 3.67930615234375\n",
      "\n",
      "Epoch:    1/30    Loss: 3.7506258172988893\n",
      "\n",
      "Epoch:    1/30    Loss: 3.7372335381507873\n",
      "\n",
      "Epoch:    1/30    Loss: 3.792365481376648\n",
      "\n",
      "Epoch:    1/30    Loss: 3.7410316305160523\n",
      "\n",
      "Epoch:    1/30    Loss: 3.7957439074516297\n",
      "\n",
      "Epoch:    1/30    Loss: 3.7649880023002624\n",
      "\n",
      "Epoch:    1/30    Loss: 3.751134890079498\n",
      "\n",
      "Epoch:    1/30    Loss: 3.739771010875702\n",
      "\n",
      "Epoch:    1/30    Loss: 3.7966238260269165\n",
      "\n",
      "Epoch:    1/30    Loss: 3.782225522518158\n",
      "\n",
      "Epoch:    1/30    Loss: 3.7516883034706114\n",
      "\n",
      "Epoch:    1/30    Loss: 3.7998425731658934\n",
      "\n",
      "Epoch:    1/30    Loss: 3.720947208404541\n",
      "\n",
      "Epoch:    1/30    Loss: 3.826858285903931\n",
      "\n",
      "Epoch:    1/30    Loss: 3.8210852994918825\n",
      "\n",
      "Epoch:    1/30    Loss: 3.7976765203475953\n",
      "\n",
      "Epoch:    1/30    Loss: 3.809109055995941\n",
      "\n",
      "Epoch:    1/30    Loss: 3.803870913505554\n",
      "\n",
      "Epoch:    1/30    Loss: 3.804429000377655\n",
      "\n",
      "Epoch:    1/30    Loss: 3.846292145729065\n",
      "\n",
      "Epoch:    1/30    Loss: 3.8064199719429017\n",
      "\n",
      "Epoch:    1/30    Loss: 3.870468104362488\n",
      "\n",
      "Epoch:    1/30    Loss: 3.843453576564789\n",
      "\n",
      "Epoch:    1/30    Loss: 3.8718310174942014\n",
      "\n",
      "Epoch:    1/30    Loss: 3.8226883935928346\n",
      "\n",
      "Epoch:    1/30    Loss: 3.801508376121521\n",
      "\n",
      "Epoch:    1/30    Loss: 3.8644161744117738\n",
      "\n",
      "Epoch:    1/30    Loss: 3.8729945940971375\n",
      "\n",
      "Epoch:    1/30    Loss: 3.840623041629791\n",
      "\n",
      "Epoch:    1/30    Loss: 3.8593646297454836\n",
      "\n",
      "Epoch:    1/30    Loss: 3.8171615772247316\n",
      "\n",
      "Epoch:    1/30    Loss: 3.809512749671936\n",
      "\n",
      "Epoch:    1/30    Loss: 3.8393801698684693\n",
      "\n",
      "Epoch:    1/30    Loss: 3.825987612247467\n",
      "\n",
      "Epoch:    1/30    Loss: 3.823084897994995\n",
      "\n",
      "Epoch:    1/30    Loss: 3.8593653893470763\n",
      "\n",
      "Epoch:    1/30    Loss: 3.8539909834861756\n",
      "\n",
      "Epoch:    1/30    Loss: 3.7770513772964476\n",
      "\n",
      "Epoch:    1/30    Loss: 3.8812248377799987\n",
      "\n",
      "Epoch:    1/30    Loss: 3.85374778175354\n",
      "\n",
      "Epoch:    1/30    Loss: 3.8384140257835386\n",
      "\n",
      "Epoch:    1/30    Loss: 3.8787650866508483\n",
      "\n",
      "Epoch:    1/30    Loss: 3.862614179134369\n",
      "\n",
      "Epoch:    1/30    Loss: 3.9062705001831053\n",
      "\n",
      "Epoch:    1/30    Loss: 3.841116179943085\n",
      "\n",
      "Epoch:    1/30    Loss: 3.866412682056427\n",
      "\n",
      "Epoch:    1/30    Loss: 3.8741663587093353\n",
      "\n",
      "Epoch:    1/30    Loss: 3.8764740109443663\n",
      "\n",
      "Epoch:    1/30    Loss: 3.8187712364196775\n",
      "\n",
      "Epoch:    1/30    Loss: 3.8929553713798524\n",
      "\n",
      "Epoch:    1/30    Loss: 3.9134981780052187\n",
      "\n",
      "Epoch:    1/30    Loss: 3.84059770154953\n",
      "\n",
      "Epoch:    1/30    Loss: 3.7995334994792938\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    2/30    Loss: 3.7801699736346936\n",
      "\n",
      "Epoch:    2/30    Loss: 3.723932646751404\n",
      "\n",
      "Epoch:    2/30    Loss: 3.729721487998962\n",
      "\n",
      "Epoch:    2/30    Loss: 3.781629999637604\n",
      "\n",
      "Epoch:    2/30    Loss: 3.729441560745239\n",
      "\n",
      "Epoch:    2/30    Loss: 3.7192550463676453\n",
      "\n",
      "Epoch:    2/30    Loss: 3.731583860874176\n",
      "\n",
      "Epoch:    2/30    Loss: 3.7129325251579286\n",
      "\n",
      "Epoch:    2/30    Loss: 3.80433056306839\n",
      "\n",
      "Epoch:    2/30    Loss: 3.7644897422790526\n",
      "\n",
      "Epoch:    2/30    Loss: 3.7834016375541686\n",
      "\n",
      "Epoch:    2/30    Loss: 3.7828581829071046\n",
      "\n",
      "Epoch:    2/30    Loss: 3.7770398478507996\n",
      "\n",
      "Epoch:    2/30    Loss: 3.7234683592319486\n",
      "\n",
      "Epoch:    2/30    Loss: 3.7490962085723876\n",
      "\n",
      "Epoch:    2/30    Loss: 3.816656254053116\n",
      "\n",
      "Epoch:    2/30    Loss: 3.7889089951515196\n",
      "\n",
      "Epoch:    2/30    Loss: 3.751828172683716\n",
      "\n",
      "Epoch:    2/30    Loss: 3.8087905988693236\n",
      "\n",
      "Epoch:    2/30    Loss: 3.8695177016258238\n",
      "\n",
      "Epoch:    2/30    Loss: 3.8141164665222167\n",
      "\n",
      "Epoch:    2/30    Loss: 3.801946400642395\n",
      "\n",
      "Epoch:    2/30    Loss: 3.7905520091056824\n",
      "\n",
      "Epoch:    2/30    Loss: 3.8463141379356385\n",
      "\n",
      "Epoch:    2/30    Loss: 3.824480204343796\n",
      "\n",
      "Epoch:    2/30    Loss: 3.768589608192444\n",
      "\n",
      "Epoch:    2/30    Loss: 3.8220787062644956\n",
      "\n",
      "Epoch:    2/30    Loss: 3.7788062438964842\n",
      "\n",
      "Epoch:    2/30    Loss: 3.790052228927612\n",
      "\n",
      "Epoch:    2/30    Loss: 3.8436240382194518\n",
      "\n",
      "Epoch:    2/30    Loss: 3.813160982608795\n",
      "\n",
      "Epoch:    2/30    Loss: 3.7903464879989626\n",
      "\n",
      "Epoch:    2/30    Loss: 3.7905917954444885\n",
      "\n",
      "Epoch:    2/30    Loss: 3.8180239706039427\n",
      "\n",
      "Epoch:    2/30    Loss: 3.879410448551178\n",
      "\n",
      "Epoch:    2/30    Loss: 3.8166574931144712\n",
      "\n",
      "Epoch:    2/30    Loss: 3.804895268917084\n",
      "\n",
      "Epoch:    2/30    Loss: 3.8332210383415224\n",
      "\n",
      "Epoch:    2/30    Loss: 3.849809139251709\n",
      "\n",
      "Epoch:    2/30    Loss: 3.8470808639526366\n",
      "\n",
      "Epoch:    2/30    Loss: 3.857796377182007\n",
      "\n",
      "Epoch:    2/30    Loss: 3.7810369839668274\n",
      "\n",
      "Epoch:    2/30    Loss: 3.9089982194900514\n",
      "\n",
      "Epoch:    2/30    Loss: 3.853891987800598\n",
      "\n",
      "Epoch:    2/30    Loss: 3.8528405508995056\n",
      "\n",
      "Epoch:    2/30    Loss: 3.848303670883179\n",
      "\n",
      "Epoch:    2/30    Loss: 3.8581120285987853\n",
      "\n",
      "Epoch:    2/30    Loss: 3.8831425137519835\n",
      "\n",
      "Epoch:    2/30    Loss: 3.8390178565979003\n",
      "\n",
      "Epoch:    2/30    Loss: 3.8712326316833496\n",
      "\n",
      "Epoch:    2/30    Loss: 3.8416116037368773\n",
      "\n",
      "Epoch:    2/30    Loss: 3.8558316946029665\n",
      "\n",
      "Epoch:    2/30    Loss: 3.846032488822937\n",
      "\n",
      "Epoch:    2/30    Loss: 3.899204022407532\n",
      "\n",
      "Epoch:    2/30    Loss: 3.9041985092163087\n",
      "\n",
      "Epoch:    3/30    Loss: 3.798421984393847\n",
      "\n",
      "Epoch:    3/30    Loss: 3.6925341095924376\n",
      "\n",
      "Epoch:    3/30    Loss: 3.722891030550003\n",
      "\n",
      "Epoch:    3/30    Loss: 3.720309458732605\n",
      "\n",
      "Epoch:    3/30    Loss: 3.727145618438721\n",
      "\n",
      "Epoch:    3/30    Loss: 3.6820582623481752\n",
      "\n",
      "Epoch:    3/30    Loss: 3.7118524146080016\n",
      "\n",
      "Epoch:    3/30    Loss: 3.751140758514404\n",
      "\n",
      "Epoch:    3/30    Loss: 3.7469381330013274\n",
      "\n",
      "Epoch:    3/30    Loss: 3.771741418361664\n",
      "\n",
      "Epoch:    3/30    Loss: 3.796195286273956\n",
      "\n",
      "Epoch:    3/30    Loss: 3.7490847005844117\n",
      "\n",
      "Epoch:    3/30    Loss: 3.7604816393852234\n",
      "\n",
      "Epoch:    3/30    Loss: 3.7186185007095336\n",
      "\n",
      "Epoch:    3/30    Loss: 3.780995002746582\n",
      "\n",
      "Epoch:    3/30    Loss: 3.755673358440399\n",
      "\n",
      "Epoch:    3/30    Loss: 3.8044781079292296\n",
      "\n",
      "Epoch:    3/30    Loss: 3.8149128606319427\n",
      "\n",
      "Epoch:    3/30    Loss: 3.7804894433021548\n",
      "\n",
      "Epoch:    3/30    Loss: 3.7741385016441344\n",
      "\n",
      "Epoch:    3/30    Loss: 3.760318374633789\n",
      "\n",
      "Epoch:    3/30    Loss: 3.754129310131073\n",
      "\n",
      "Epoch:    3/30    Loss: 3.816777265071869\n",
      "\n",
      "Epoch:    3/30    Loss: 3.8064242100715635\n",
      "\n",
      "Epoch:    3/30    Loss: 3.7712502784729005\n",
      "\n",
      "Epoch:    3/30    Loss: 3.8102544455528258\n",
      "\n",
      "Epoch:    3/30    Loss: 3.813619680404663\n",
      "\n",
      "Epoch:    3/30    Loss: 3.8285626492500304\n",
      "\n",
      "Epoch:    3/30    Loss: 3.80775000333786\n",
      "\n",
      "Epoch:    3/30    Loss: 3.8044583735466\n",
      "\n",
      "Epoch:    3/30    Loss: 3.8011472005844116\n",
      "\n",
      "Epoch:    3/30    Loss: 3.8030724215507505\n",
      "\n",
      "Epoch:    3/30    Loss: 3.756925742149353\n",
      "\n",
      "Epoch:    3/30    Loss: 3.8224018030166627\n",
      "\n",
      "Epoch:    3/30    Loss: 3.7949067459106445\n",
      "\n",
      "Epoch:    3/30    Loss: 3.7854755487442016\n",
      "\n",
      "Epoch:    3/30    Loss: 3.7618004338741304\n",
      "\n",
      "Epoch:    3/30    Loss: 3.880565824985504\n",
      "\n",
      "Epoch:    3/30    Loss: 3.8682478260993958\n",
      "\n",
      "Epoch:    3/30    Loss: 3.802267804145813\n",
      "\n",
      "Epoch:    3/30    Loss: 3.8159807724952697\n",
      "\n",
      "Epoch:    3/30    Loss: 3.8005318775177\n",
      "\n",
      "Epoch:    3/30    Loss: 3.859824696302414\n",
      "\n",
      "Epoch:    3/30    Loss: 3.890535526275635\n",
      "\n",
      "Epoch:    3/30    Loss: 3.809056131601334\n",
      "\n",
      "Epoch:    3/30    Loss: 3.8070778994560244\n",
      "\n",
      "Epoch:    3/30    Loss: 3.8535169949531554\n",
      "\n",
      "Epoch:    3/30    Loss: 3.861694525718689\n",
      "\n",
      "Epoch:    3/30    Loss: 3.8784754021167753\n",
      "\n",
      "Epoch:    3/30    Loss: 3.892712687253952\n",
      "\n",
      "Epoch:    3/30    Loss: 3.8374869990348817\n",
      "\n",
      "Epoch:    3/30    Loss: 3.7962600750923157\n",
      "\n",
      "Epoch:    3/30    Loss: 3.8152824268341066\n",
      "\n",
      "Epoch:    3/30    Loss: 3.8598228340148926\n",
      "\n",
      "Epoch:    3/30    Loss: 3.886289268732071\n",
      "\n",
      "Epoch:    4/30    Loss: 3.7707188488685923\n",
      "\n",
      "Epoch:    4/30    Loss: 3.698491138458252\n",
      "\n",
      "Epoch:    4/30    Loss: 3.7369136934280394\n",
      "\n",
      "Epoch:    4/30    Loss: 3.689111387729645\n",
      "\n",
      "Epoch:    4/30    Loss: 3.6956963753700256\n",
      "\n",
      "Epoch:    4/30    Loss: 3.7434421734809877\n",
      "\n",
      "Epoch:    4/30    Loss: 3.736533877849579\n",
      "\n",
      "Epoch:    4/30    Loss: 3.754128248214722\n",
      "\n",
      "Epoch:    4/30    Loss: 3.754271427631378\n",
      "\n",
      "Epoch:    4/30    Loss: 3.7958219480514526\n",
      "\n",
      "Epoch:    4/30    Loss: 3.7650534439086916\n",
      "\n",
      "Epoch:    4/30    Loss: 3.7229715547561644\n",
      "\n",
      "Epoch:    4/30    Loss: 3.77821230173111\n",
      "\n",
      "Epoch:    4/30    Loss: 3.7074541194438932\n",
      "\n",
      "Epoch:    4/30    Loss: 3.770325764656067\n",
      "\n",
      "Epoch:    4/30    Loss: 3.7500262136459352\n",
      "\n",
      "Epoch:    4/30    Loss: 3.8015629518032075\n",
      "\n",
      "Epoch:    4/30    Loss: 3.7849144501686096\n",
      "\n",
      "Epoch:    4/30    Loss: 3.7556902084350585\n",
      "\n",
      "Epoch:    4/30    Loss: 3.730740744590759\n",
      "\n",
      "Epoch:    4/30    Loss: 3.754750414133072\n",
      "\n",
      "Epoch:    4/30    Loss: 3.755938467979431\n",
      "\n",
      "Epoch:    4/30    Loss: 3.7733100140094757\n",
      "\n",
      "Epoch:    4/30    Loss: 3.804218225955963\n",
      "\n",
      "Epoch:    4/30    Loss: 3.804652971744537\n",
      "\n",
      "Epoch:    4/30    Loss: 3.7886743683815003\n",
      "\n",
      "Epoch:    4/30    Loss: 3.782831782579422\n",
      "\n",
      "Epoch:    4/30    Loss: 3.754107014656067\n",
      "\n",
      "Epoch:    4/30    Loss: 3.7987101941108703\n",
      "\n",
      "Epoch:    4/30    Loss: 3.797175148010254\n",
      "\n",
      "Epoch:    4/30    Loss: 3.7485388236045836\n",
      "\n",
      "Epoch:    4/30    Loss: 3.748546428203583\n",
      "\n",
      "Epoch:    4/30    Loss: 3.788941866397858\n",
      "\n",
      "Epoch:    4/30    Loss: 3.794172389984131\n",
      "\n",
      "Epoch:    4/30    Loss: 3.7896725132465363\n",
      "\n",
      "Epoch:    4/30    Loss: 3.829041286945343\n",
      "\n",
      "Epoch:    4/30    Loss: 3.7943605492115022\n",
      "\n",
      "Epoch:    4/30    Loss: 3.839007325172424\n",
      "\n",
      "Epoch:    4/30    Loss: 3.8011015758514404\n",
      "\n",
      "Epoch:    4/30    Loss: 3.828229907989502\n",
      "\n",
      "Epoch:    4/30    Loss: 3.821243829250336\n",
      "\n",
      "Epoch:    4/30    Loss: 3.849946408510208\n",
      "\n",
      "Epoch:    4/30    Loss: 3.8607710542678833\n",
      "\n",
      "Epoch:    4/30    Loss: 3.753643165588379\n",
      "\n",
      "Epoch:    4/30    Loss: 3.7765085520744326\n",
      "\n",
      "Epoch:    4/30    Loss: 3.7856176352500914\n",
      "\n",
      "Epoch:    4/30    Loss: 3.8283627834320066\n",
      "\n",
      "Epoch:    4/30    Loss: 3.79808459353447\n",
      "\n",
      "Epoch:    4/30    Loss: 3.824217269897461\n",
      "\n",
      "Epoch:    4/30    Loss: 3.8428827962875367\n",
      "\n",
      "Epoch:    4/30    Loss: 3.8406857600212096\n",
      "\n",
      "Epoch:    4/30    Loss: 3.839630650520325\n",
      "\n",
      "Epoch:    4/30    Loss: 3.8682424612045287\n",
      "\n",
      "Epoch:    4/30    Loss: 3.8097774357795715\n",
      "\n",
      "Epoch:    4/30    Loss: 3.8176289632320404\n",
      "\n",
      "Epoch:    5/30    Loss: 3.755128771474917\n",
      "\n",
      "Epoch:    5/30    Loss: 3.6370241661071776\n",
      "\n",
      "Epoch:    5/30    Loss: 3.7436357140541077\n",
      "\n",
      "Epoch:    5/30    Loss: 3.7118896656036378\n",
      "\n",
      "Epoch:    5/30    Loss: 3.6866971755027773\n",
      "\n",
      "Epoch:    5/30    Loss: 3.671073169231415\n",
      "\n",
      "Epoch:    5/30    Loss: 3.733815598964691\n",
      "\n",
      "Epoch:    5/30    Loss: 3.702586984872818\n",
      "\n",
      "Epoch:    5/30    Loss: 3.7078314909934997\n",
      "\n",
      "Epoch:    5/30    Loss: 3.709357439041138\n",
      "\n",
      "Epoch:    5/30    Loss: 3.745529927253723\n",
      "\n",
      "Epoch:    5/30    Loss: 3.7255441222190857\n",
      "\n",
      "Epoch:    5/30    Loss: 3.6956794919967653\n",
      "\n",
      "Epoch:    5/30    Loss: 3.693556025505066\n",
      "\n",
      "Epoch:    5/30    Loss: 3.7844188380241395\n",
      "\n",
      "Epoch:    5/30    Loss: 3.7546567883491515\n",
      "\n",
      "Epoch:    5/30    Loss: 3.798580587387085\n",
      "\n",
      "Epoch:    5/30    Loss: 3.7214025774002075\n",
      "\n",
      "Epoch:    5/30    Loss: 3.7292360582351685\n",
      "\n",
      "Epoch:    5/30    Loss: 3.754496295452118\n",
      "\n",
      "Epoch:    5/30    Loss: 3.74479869222641\n",
      "\n",
      "Epoch:    5/30    Loss: 3.791560570716858\n",
      "\n",
      "Epoch:    5/30    Loss: 3.8090328550338746\n",
      "\n",
      "Epoch:    5/30    Loss: 3.789181488275528\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    5/30    Loss: 3.7906447553634646\n",
      "\n",
      "Epoch:    5/30    Loss: 3.7625469131469726\n",
      "\n",
      "Epoch:    5/30    Loss: 3.824274685382843\n",
      "\n",
      "Epoch:    5/30    Loss: 3.8081753005981445\n",
      "\n",
      "Epoch:    5/30    Loss: 3.821280647277832\n",
      "\n",
      "Epoch:    5/30    Loss: 3.7404413990974428\n",
      "\n",
      "Epoch:    5/30    Loss: 3.785963837623596\n",
      "\n",
      "Epoch:    5/30    Loss: 3.7507787642478942\n",
      "\n",
      "Epoch:    5/30    Loss: 3.830416308403015\n",
      "\n",
      "Epoch:    5/30    Loss: 3.8026523537635804\n",
      "\n",
      "Epoch:    5/30    Loss: 3.8374249744415283\n",
      "\n",
      "Epoch:    5/30    Loss: 3.7564953851699827\n",
      "\n",
      "Epoch:    5/30    Loss: 3.7827913181781767\n",
      "\n",
      "Epoch:    5/30    Loss: 3.7831589641571046\n",
      "\n",
      "Epoch:    5/30    Loss: 3.7839610147476197\n",
      "\n",
      "Epoch:    5/30    Loss: 3.770904727935791\n",
      "\n",
      "Epoch:    5/30    Loss: 3.8088799114227294\n",
      "\n",
      "Epoch:    5/30    Loss: 3.7861751885414123\n",
      "\n",
      "Epoch:    5/30    Loss: 3.8072671570777894\n",
      "\n",
      "Epoch:    5/30    Loss: 3.786687971830368\n",
      "\n",
      "Epoch:    5/30    Loss: 3.755433579921722\n",
      "\n",
      "Epoch:    5/30    Loss: 3.8482551608085633\n",
      "\n",
      "Epoch:    5/30    Loss: 3.7770838441848755\n",
      "\n",
      "Epoch:    5/30    Loss: 3.7934680781364443\n",
      "\n",
      "Epoch:    5/30    Loss: 3.7883463926315306\n",
      "\n",
      "Epoch:    5/30    Loss: 3.8345097093582154\n",
      "\n",
      "Epoch:    5/30    Loss: 3.805954436779022\n",
      "\n",
      "Epoch:    5/30    Loss: 3.849267966270447\n",
      "\n",
      "Epoch:    5/30    Loss: 3.8128550333976747\n",
      "\n",
      "Epoch:    5/30    Loss: 3.771587924003601\n",
      "\n",
      "Epoch:    5/30    Loss: 3.8176883869171143\n",
      "\n",
      "Epoch:    6/30    Loss: 3.745778356512932\n",
      "\n",
      "Epoch:    6/30    Loss: 3.6663081703186036\n",
      "\n",
      "Epoch:    6/30    Loss: 3.7049554233551025\n",
      "\n",
      "Epoch:    6/30    Loss: 3.688402355670929\n",
      "\n",
      "Epoch:    6/30    Loss: 3.6946272139549254\n",
      "\n",
      "Epoch:    6/30    Loss: 3.7495820517539977\n",
      "\n",
      "Epoch:    6/30    Loss: 3.6960297911167146\n",
      "\n",
      "Epoch:    6/30    Loss: 3.7466604857444765\n",
      "\n",
      "Epoch:    6/30    Loss: 3.694001578330994\n",
      "\n",
      "Epoch:    6/30    Loss: 3.7417356925010683\n",
      "\n",
      "Epoch:    6/30    Loss: 3.7008127913475035\n",
      "\n",
      "Epoch:    6/30    Loss: 3.7158754274845123\n",
      "\n",
      "Epoch:    6/30    Loss: 3.678774534225464\n",
      "\n",
      "Epoch:    6/30    Loss: 3.7446189546585082\n",
      "\n",
      "Epoch:    6/30    Loss: 3.7612598853111265\n",
      "\n",
      "Epoch:    6/30    Loss: 3.728732066869736\n",
      "\n",
      "Epoch:    6/30    Loss: 3.746993989467621\n",
      "\n",
      "Epoch:    6/30    Loss: 3.7182676701545714\n",
      "\n",
      "Epoch:    6/30    Loss: 3.6951489863395692\n",
      "\n",
      "Epoch:    6/30    Loss: 3.7619132740497587\n",
      "\n",
      "Epoch:    6/30    Loss: 3.8158106622695924\n",
      "\n",
      "Epoch:    6/30    Loss: 3.7663241147994997\n",
      "\n",
      "Epoch:    6/30    Loss: 3.7479469294548036\n",
      "\n",
      "Epoch:    6/30    Loss: 3.7699147350788116\n",
      "\n",
      "Epoch:    6/30    Loss: 3.7978569469451906\n",
      "\n",
      "Epoch:    6/30    Loss: 3.726693126678467\n",
      "\n",
      "Epoch:    6/30    Loss: 3.7235069417953492\n",
      "\n",
      "Epoch:    6/30    Loss: 3.750897164821625\n",
      "\n",
      "Epoch:    6/30    Loss: 3.7693759694099427\n",
      "\n",
      "Epoch:    6/30    Loss: 3.8034426736831666\n",
      "\n",
      "Epoch:    6/30    Loss: 3.7773215456008913\n",
      "\n",
      "Epoch:    6/30    Loss: 3.7620082459449766\n",
      "\n",
      "Epoch:    6/30    Loss: 3.7918698053359985\n",
      "\n",
      "Epoch:    6/30    Loss: 3.786947949409485\n",
      "\n",
      "Epoch:    6/30    Loss: 3.729727292537689\n",
      "\n",
      "Epoch:    6/30    Loss: 3.7838028173446654\n",
      "\n",
      "Epoch:    6/30    Loss: 3.810662125110626\n",
      "\n",
      "Epoch:    6/30    Loss: 3.7807438716888426\n",
      "\n",
      "Epoch:    6/30    Loss: 3.788737620353699\n",
      "\n",
      "Epoch:    6/30    Loss: 3.7856491379737855\n",
      "\n",
      "Epoch:    6/30    Loss: 3.77407542181015\n",
      "\n",
      "Epoch:    6/30    Loss: 3.8255868740081787\n",
      "\n",
      "Epoch:    6/30    Loss: 3.8225995059013367\n",
      "\n",
      "Epoch:    6/30    Loss: 3.7473649339675905\n",
      "\n",
      "Epoch:    6/30    Loss: 3.8278879384994506\n",
      "\n",
      "Epoch:    6/30    Loss: 3.766754650592804\n",
      "\n",
      "Epoch:    6/30    Loss: 3.8086469039916993\n",
      "\n",
      "Epoch:    6/30    Loss: 3.773433774471283\n",
      "\n",
      "Epoch:    6/30    Loss: 3.7814493203163146\n",
      "\n",
      "Epoch:    6/30    Loss: 3.7698180637359617\n",
      "\n",
      "Epoch:    6/30    Loss: 3.7809720964431763\n",
      "\n",
      "Epoch:    6/30    Loss: 3.804608204841614\n",
      "\n",
      "Epoch:    6/30    Loss: 3.8106668901443483\n",
      "\n",
      "Epoch:    6/30    Loss: 3.8103581326007845\n",
      "\n",
      "Epoch:    6/30    Loss: 3.849740575313568\n",
      "\n",
      "Epoch:    7/30    Loss: 3.7304675638947855\n",
      "\n",
      "Epoch:    7/30    Loss: 3.654130062818527\n",
      "\n",
      "Epoch:    7/30    Loss: 3.671308808803558\n",
      "\n",
      "Epoch:    7/30    Loss: 3.663717203140259\n",
      "\n",
      "Epoch:    7/30    Loss: 3.726071764945984\n",
      "\n",
      "Epoch:    7/30    Loss: 3.683344766378403\n",
      "\n",
      "Epoch:    7/30    Loss: 3.699360206604004\n",
      "\n",
      "Epoch:    7/30    Loss: 3.6616415054798126\n",
      "\n",
      "Epoch:    7/30    Loss: 3.6820798826217653\n",
      "\n",
      "Epoch:    7/30    Loss: 3.6968028864860534\n",
      "\n",
      "Epoch:    7/30    Loss: 3.683544065952301\n",
      "\n",
      "Epoch:    7/30    Loss: 3.7335454258918763\n",
      "\n",
      "Epoch:    7/30    Loss: 3.728577644824982\n",
      "\n",
      "Epoch:    7/30    Loss: 3.704898190975189\n",
      "\n",
      "Epoch:    7/30    Loss: 3.7125228748321533\n",
      "\n",
      "Epoch:    7/30    Loss: 3.745294106721878\n",
      "\n",
      "Epoch:    7/30    Loss: 3.687595280647278\n",
      "\n",
      "Epoch:    7/30    Loss: 3.700169327020645\n",
      "\n",
      "Epoch:    7/30    Loss: 3.7338830609321594\n",
      "\n",
      "Epoch:    7/30    Loss: 3.7216145968437195\n",
      "\n",
      "Epoch:    7/30    Loss: 3.7089721484184266\n",
      "\n",
      "Epoch:    7/30    Loss: 3.7373919620513916\n",
      "\n",
      "Epoch:    7/30    Loss: 3.782530293226242\n",
      "\n",
      "Epoch:    7/30    Loss: 3.7012603697776796\n",
      "\n",
      "Epoch:    7/30    Loss: 3.7498631370067597\n",
      "\n",
      "Epoch:    7/30    Loss: 3.7488985557556154\n",
      "\n",
      "Epoch:    7/30    Loss: 3.726895981788635\n",
      "\n",
      "Epoch:    7/30    Loss: 3.7096382904052736\n",
      "\n",
      "Epoch:    7/30    Loss: 3.7888606600761414\n",
      "\n",
      "Epoch:    7/30    Loss: 3.7497902204990385\n",
      "\n",
      "Epoch:    7/30    Loss: 3.788021746635437\n",
      "\n",
      "Epoch:    7/30    Loss: 3.771419240951538\n",
      "\n",
      "Epoch:    7/30    Loss: 3.807860770702362\n",
      "\n",
      "Epoch:    7/30    Loss: 3.751018371105194\n",
      "\n",
      "Epoch:    7/30    Loss: 3.768785307407379\n",
      "\n",
      "Epoch:    7/30    Loss: 3.7759136595726015\n",
      "\n",
      "Epoch:    7/30    Loss: 3.8007409954071045\n",
      "\n",
      "Epoch:    7/30    Loss: 3.6873870644569395\n",
      "\n",
      "Epoch:    7/30    Loss: 3.793929124355316\n",
      "\n",
      "Epoch:    7/30    Loss: 3.8296156706809996\n",
      "\n",
      "Epoch:    7/30    Loss: 3.763965169906616\n",
      "\n",
      "Epoch:    7/30    Loss: 3.797595847606659\n",
      "\n",
      "Epoch:    7/30    Loss: 3.8390601353645324\n",
      "\n",
      "Epoch:    7/30    Loss: 3.706511360406876\n",
      "\n",
      "Epoch:    7/30    Loss: 3.791508195400238\n",
      "\n",
      "Epoch:    7/30    Loss: 3.836972830295563\n",
      "\n",
      "Epoch:    7/30    Loss: 3.7845267481803893\n",
      "\n",
      "Epoch:    7/30    Loss: 3.716166449546814\n",
      "\n",
      "Epoch:    7/30    Loss: 3.7885939466953276\n",
      "\n",
      "Epoch:    7/30    Loss: 3.834950456857681\n",
      "\n",
      "Epoch:    7/30    Loss: 3.767914021015167\n",
      "\n",
      "Epoch:    7/30    Loss: 3.8261975636482237\n",
      "\n",
      "Epoch:    7/30    Loss: 3.8362151403427123\n",
      "\n",
      "Epoch:    7/30    Loss: 3.7929342787265776\n",
      "\n",
      "Epoch:    7/30    Loss: 3.8566606884002685\n",
      "\n",
      "Epoch:    8/30    Loss: 3.724829481344789\n",
      "\n",
      "Epoch:    8/30    Loss: 3.670189767360687\n",
      "\n",
      "Epoch:    8/30    Loss: 3.6348863463401795\n",
      "\n",
      "Epoch:    8/30    Loss: 3.692143629550934\n",
      "\n",
      "Epoch:    8/30    Loss: 3.7125020227432253\n",
      "\n",
      "Epoch:    8/30    Loss: 3.669883467197418\n",
      "\n",
      "Epoch:    8/30    Loss: 3.6809465074539185\n",
      "\n",
      "Epoch:    8/30    Loss: 3.6847962305545807\n",
      "\n",
      "Epoch:    8/30    Loss: 3.7029516129493714\n",
      "\n",
      "Epoch:    8/30    Loss: 3.6458967328071594\n",
      "\n",
      "Epoch:    8/30    Loss: 3.661770447969437\n",
      "\n",
      "Epoch:    8/30    Loss: 3.683020070314407\n",
      "\n",
      "Epoch:    8/30    Loss: 3.7403318939208985\n",
      "\n",
      "Epoch:    8/30    Loss: 3.6889272067546846\n",
      "\n",
      "Epoch:    8/30    Loss: 3.7198477001190184\n",
      "\n",
      "Epoch:    8/30    Loss: 3.6939887037277224\n",
      "\n",
      "Epoch:    8/30    Loss: 3.743752375125885\n",
      "\n",
      "Epoch:    8/30    Loss: 3.7267628388404845\n",
      "\n",
      "Epoch:    8/30    Loss: 3.71198708820343\n",
      "\n",
      "Epoch:    8/30    Loss: 3.693408654689789\n",
      "\n",
      "Epoch:    8/30    Loss: 3.7354744901657106\n",
      "\n",
      "Epoch:    8/30    Loss: 3.7175871624946595\n",
      "\n",
      "Epoch:    8/30    Loss: 3.737668116569519\n",
      "\n",
      "Epoch:    8/30    Loss: 3.7586336517333985\n",
      "\n",
      "Epoch:    8/30    Loss: 3.7585538229942324\n",
      "\n",
      "Epoch:    8/30    Loss: 3.7554601516723634\n",
      "\n",
      "Epoch:    8/30    Loss: 3.8164044411182405\n",
      "\n",
      "Epoch:    8/30    Loss: 3.727589436531067\n",
      "\n",
      "Epoch:    8/30    Loss: 3.7759634971618654\n",
      "\n",
      "Epoch:    8/30    Loss: 3.7072894570827484\n",
      "\n",
      "Epoch:    8/30    Loss: 3.7146696248054503\n",
      "\n",
      "Epoch:    8/30    Loss: 3.748111895084381\n",
      "\n",
      "Epoch:    8/30    Loss: 3.706479689121246\n",
      "\n",
      "Epoch:    8/30    Loss: 3.763306574344635\n",
      "\n",
      "Epoch:    8/30    Loss: 3.7455950598716736\n",
      "\n",
      "Epoch:    8/30    Loss: 3.7502697134017944\n",
      "\n",
      "Epoch:    8/30    Loss: 3.7052211799621584\n",
      "\n",
      "Epoch:    8/30    Loss: 3.798322741031647\n",
      "\n",
      "Epoch:    8/30    Loss: 3.7350903346538544\n",
      "\n",
      "Epoch:    8/30    Loss: 3.8131498675346376\n",
      "\n",
      "Epoch:    8/30    Loss: 3.8151374278068544\n",
      "\n",
      "Epoch:    8/30    Loss: 3.8116382403373716\n",
      "\n",
      "Epoch:    8/30    Loss: 3.7474367794990537\n",
      "\n",
      "Epoch:    8/30    Loss: 3.7894298787117005\n",
      "\n",
      "Epoch:    8/30    Loss: 3.790905632019043\n",
      "\n",
      "Epoch:    8/30    Loss: 3.8058667984008787\n",
      "\n",
      "Epoch:    8/30    Loss: 3.7775182824134825\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    8/30    Loss: 3.833738327026367\n",
      "\n",
      "Epoch:    8/30    Loss: 3.809627555370331\n",
      "\n",
      "Epoch:    8/30    Loss: 3.7492902765274048\n",
      "\n",
      "Epoch:    8/30    Loss: 3.7369990367889403\n",
      "\n",
      "Epoch:    8/30    Loss: 3.775944343090057\n",
      "\n",
      "Epoch:    8/30    Loss: 3.8530957469940184\n",
      "\n",
      "Epoch:    8/30    Loss: 3.8338695845603943\n",
      "\n",
      "Epoch:    8/30    Loss: 3.793320100784302\n",
      "\n",
      "Epoch:    9/30    Loss: 3.692833823156139\n",
      "\n",
      "Epoch:    9/30    Loss: 3.6469540677070618\n",
      "\n",
      "Epoch:    9/30    Loss: 3.6763525524139404\n",
      "\n",
      "Epoch:    9/30    Loss: 3.6582496547698975\n",
      "\n",
      "Epoch:    9/30    Loss: 3.685294219017029\n",
      "\n",
      "Epoch:    9/30    Loss: 3.619654175043106\n",
      "\n",
      "Epoch:    9/30    Loss: 3.6409227013587953\n",
      "\n",
      "Epoch:    9/30    Loss: 3.692809771537781\n",
      "\n",
      "Epoch:    9/30    Loss: 3.727855191230774\n",
      "\n",
      "Epoch:    9/30    Loss: 3.704535118341446\n",
      "\n",
      "Epoch:    9/30    Loss: 3.712372476577759\n",
      "\n",
      "Epoch:    9/30    Loss: 3.6610331053733827\n",
      "\n",
      "Epoch:    9/30    Loss: 3.697569134235382\n",
      "\n",
      "Epoch:    9/30    Loss: 3.6643227887153627\n",
      "\n",
      "Epoch:    9/30    Loss: 3.666763944149017\n",
      "\n",
      "Epoch:    9/30    Loss: 3.7462828559875487\n",
      "\n",
      "Epoch:    9/30    Loss: 3.739307740688324\n",
      "\n",
      "Epoch:    9/30    Loss: 3.7170307750701905\n",
      "\n",
      "Epoch:    9/30    Loss: 3.6867763948440553\n",
      "\n",
      "Epoch:    9/30    Loss: 3.7067070159912108\n",
      "\n",
      "Epoch:    9/30    Loss: 3.737266454935074\n",
      "\n",
      "Epoch:    9/30    Loss: 3.7688056206703187\n",
      "\n",
      "Epoch:    9/30    Loss: 3.739795244216919\n",
      "\n",
      "Epoch:    9/30    Loss: 3.729566065311432\n",
      "\n",
      "Epoch:    9/30    Loss: 3.734440104007721\n",
      "\n",
      "Epoch:    9/30    Loss: 3.77841805934906\n",
      "\n",
      "Epoch:    9/30    Loss: 3.740867609500885\n",
      "\n",
      "Epoch:    9/30    Loss: 3.6954062354564665\n",
      "\n",
      "Epoch:    9/30    Loss: 3.733383023738861\n",
      "\n",
      "Epoch:    9/30    Loss: 3.721699437379837\n",
      "\n",
      "Epoch:    9/30    Loss: 3.702641354560852\n",
      "\n",
      "Epoch:    9/30    Loss: 3.7348660407066343\n",
      "\n",
      "Epoch:    9/30    Loss: 3.7454812326431273\n",
      "\n",
      "Epoch:    9/30    Loss: 3.75942022228241\n",
      "\n",
      "Epoch:    9/30    Loss: 3.768512734413147\n",
      "\n",
      "Epoch:    9/30    Loss: 3.74673095703125\n",
      "\n",
      "Epoch:    9/30    Loss: 3.7291983852386474\n",
      "\n",
      "Epoch:    9/30    Loss: 3.77321298456192\n",
      "\n",
      "Epoch:    9/30    Loss: 3.7673218684196472\n",
      "\n",
      "Epoch:    9/30    Loss: 3.7542649931907652\n",
      "\n",
      "Epoch:    9/30    Loss: 3.735925771713257\n",
      "\n",
      "Epoch:    9/30    Loss: 3.800133602619171\n",
      "\n",
      "Epoch:    9/30    Loss: 3.7808982400894164\n",
      "\n",
      "Epoch:    9/30    Loss: 3.7363332605361936\n",
      "\n",
      "Epoch:    9/30    Loss: 3.780907907962799\n",
      "\n",
      "Epoch:    9/30    Loss: 3.7685285449028014\n",
      "\n",
      "Epoch:    9/30    Loss: 3.8164272074699404\n",
      "\n",
      "Epoch:    9/30    Loss: 3.7830910930633546\n",
      "\n",
      "Epoch:    9/30    Loss: 3.832420732975006\n",
      "\n",
      "Epoch:    9/30    Loss: 3.7944199006557464\n",
      "\n",
      "Epoch:    9/30    Loss: 3.758053739309311\n",
      "\n",
      "Epoch:    9/30    Loss: 3.7900322756767273\n",
      "\n",
      "Epoch:    9/30    Loss: 3.7704648442268374\n",
      "\n",
      "Epoch:    9/30    Loss: 3.7884544517993928\n",
      "\n",
      "Epoch:    9/30    Loss: 3.746942714691162\n",
      "\n",
      "Epoch:   10/30    Loss: 3.748700765986421\n",
      "\n",
      "Epoch:   10/30    Loss: 3.643047283887863\n",
      "\n",
      "Epoch:   10/30    Loss: 3.6238568670749665\n",
      "\n",
      "Epoch:   10/30    Loss: 3.683283949613571\n",
      "\n",
      "Epoch:   10/30    Loss: 3.602491307258606\n",
      "\n",
      "Epoch:   10/30    Loss: 3.730472867965698\n",
      "\n",
      "Epoch:   10/30    Loss: 3.679895485877991\n",
      "\n",
      "Epoch:   10/30    Loss: 3.6709647426605225\n",
      "\n",
      "Epoch:   10/30    Loss: 3.6316197385787965\n",
      "\n",
      "Epoch:   10/30    Loss: 3.6648452644348146\n",
      "\n",
      "Epoch:   10/30    Loss: 3.6777854800224303\n",
      "\n",
      "Epoch:   10/30    Loss: 3.6948599729537963\n",
      "\n",
      "Epoch:   10/30    Loss: 3.6854570417404173\n",
      "\n",
      "Epoch:   10/30    Loss: 3.6946578669548034\n",
      "\n",
      "Epoch:   10/30    Loss: 3.6609256420135496\n",
      "\n",
      "Epoch:   10/30    Loss: 3.676174288749695\n",
      "\n",
      "Epoch:   10/30    Loss: 3.680769481420517\n",
      "\n",
      "Epoch:   10/30    Loss: 3.746006623983383\n",
      "\n",
      "Epoch:   10/30    Loss: 3.6895907192230224\n",
      "\n",
      "Epoch:   10/30    Loss: 3.664555736064911\n",
      "\n",
      "Epoch:   10/30    Loss: 3.737478974342346\n",
      "\n",
      "Epoch:   10/30    Loss: 3.7503440608978273\n",
      "\n",
      "Epoch:   10/30    Loss: 3.7433280386924745\n",
      "\n",
      "Epoch:   10/30    Loss: 3.684520610332489\n",
      "\n",
      "Epoch:   10/30    Loss: 3.7222420949935913\n",
      "\n",
      "Epoch:   10/30    Loss: 3.735958790063858\n",
      "\n",
      "Epoch:   10/30    Loss: 3.7018302297592163\n",
      "\n",
      "Epoch:   10/30    Loss: 3.7484501132965087\n",
      "\n",
      "Epoch:   10/30    Loss: 3.757501444339752\n",
      "\n",
      "Epoch:   10/30    Loss: 3.7238353691101076\n",
      "\n",
      "Epoch:   10/30    Loss: 3.676808337688446\n",
      "\n",
      "Epoch:   10/30    Loss: 3.695533830165863\n",
      "\n",
      "Epoch:   10/30    Loss: 3.774964614391327\n",
      "\n",
      "Epoch:   10/30    Loss: 3.7626621079444886\n",
      "\n",
      "Epoch:   10/30    Loss: 3.7721838326454162\n",
      "\n",
      "Epoch:   10/30    Loss: 3.7550184705257417\n",
      "\n",
      "Epoch:   10/30    Loss: 3.7709627518653868\n",
      "\n",
      "Epoch:   10/30    Loss: 3.737417505264282\n",
      "\n",
      "Epoch:   10/30    Loss: 3.7464768924713137\n",
      "\n",
      "Epoch:   10/30    Loss: 3.746489745616913\n",
      "\n",
      "Epoch:   10/30    Loss: 3.8176673636436464\n",
      "\n",
      "Epoch:   10/30    Loss: 3.7627080078125\n",
      "\n",
      "Epoch:   10/30    Loss: 3.8105638456344604\n",
      "\n",
      "Epoch:   10/30    Loss: 3.8266865973472597\n",
      "\n",
      "Epoch:   10/30    Loss: 3.777654641151428\n",
      "\n",
      "Epoch:   10/30    Loss: 3.736707136631012\n",
      "\n",
      "Epoch:   10/30    Loss: 3.7521952414512634\n",
      "\n",
      "Epoch:   10/30    Loss: 3.765812854528427\n",
      "\n",
      "Epoch:   10/30    Loss: 3.7449931025505068\n",
      "\n",
      "Epoch:   10/30    Loss: 3.8081131789684295\n",
      "\n",
      "Epoch:   10/30    Loss: 3.7659328508377077\n",
      "\n",
      "Epoch:   10/30    Loss: 3.7820574259757995\n",
      "\n",
      "Epoch:   10/30    Loss: 3.7943608980178833\n",
      "\n",
      "Epoch:   10/30    Loss: 3.8174756741523743\n",
      "\n",
      "Epoch:   10/30    Loss: 3.766339096069336\n",
      "\n",
      "Epoch:   11/30    Loss: 3.6936843128509174\n",
      "\n",
      "Epoch:   11/30    Loss: 3.623847960948944\n",
      "\n",
      "Epoch:   11/30    Loss: 3.650421420574188\n",
      "\n",
      "Epoch:   11/30    Loss: 3.64357008934021\n",
      "\n",
      "Epoch:   11/30    Loss: 3.655533122062683\n",
      "\n",
      "Epoch:   11/30    Loss: 3.6050060925483702\n",
      "\n",
      "Epoch:   11/30    Loss: 3.646452108383179\n",
      "\n",
      "Epoch:   11/30    Loss: 3.6562858567237853\n",
      "\n",
      "Epoch:   11/30    Loss: 3.7170289504528045\n",
      "\n",
      "Epoch:   11/30    Loss: 3.6650359563827513\n",
      "\n",
      "Epoch:   11/30    Loss: 3.657875930786133\n",
      "\n",
      "Epoch:   11/30    Loss: 3.686896312713623\n",
      "\n",
      "Epoch:   11/30    Loss: 3.7046324424743653\n",
      "\n",
      "Epoch:   11/30    Loss: 3.692281072616577\n",
      "\n",
      "Epoch:   11/30    Loss: 3.6916297962665556\n",
      "\n",
      "Epoch:   11/30    Loss: 3.672311217784882\n",
      "\n",
      "Epoch:   11/30    Loss: 3.7017690925598146\n",
      "\n",
      "Epoch:   11/30    Loss: 3.6675121555328367\n",
      "\n",
      "Epoch:   11/30    Loss: 3.6934797134399413\n",
      "\n",
      "Epoch:   11/30    Loss: 3.6961633276939394\n",
      "\n",
      "Epoch:   11/30    Loss: 3.675484251499176\n",
      "\n",
      "Epoch:   11/30    Loss: 3.6677708520889283\n",
      "\n",
      "Epoch:   11/30    Loss: 3.681956868171692\n",
      "\n",
      "Epoch:   11/30    Loss: 3.713243456840515\n",
      "\n",
      "Epoch:   11/30    Loss: 3.723920753002167\n",
      "\n",
      "Epoch:   11/30    Loss: 3.7171991214752196\n",
      "\n",
      "Epoch:   11/30    Loss: 3.7078605563640594\n",
      "\n",
      "Epoch:   11/30    Loss: 3.6740423703193663\n",
      "\n",
      "Epoch:   11/30    Loss: 3.6686022572517394\n",
      "\n",
      "Epoch:   11/30    Loss: 3.7287143359184265\n",
      "\n",
      "Epoch:   11/30    Loss: 3.772675150871277\n",
      "\n",
      "Epoch:   11/30    Loss: 3.691535749435425\n",
      "\n",
      "Epoch:   11/30    Loss: 3.732116765975952\n",
      "\n",
      "Epoch:   11/30    Loss: 3.7655225772857666\n",
      "\n",
      "Epoch:   11/30    Loss: 3.7600005168914796\n",
      "\n",
      "Epoch:   11/30    Loss: 3.750712148666382\n",
      "\n",
      "Epoch:   11/30    Loss: 3.719177925825119\n",
      "\n",
      "Epoch:   11/30    Loss: 3.7283773601055143\n",
      "\n",
      "Epoch:   11/30    Loss: 3.7544406509399413\n",
      "\n",
      "Epoch:   11/30    Loss: 3.7546785163879393\n",
      "\n",
      "Epoch:   11/30    Loss: 3.7891496815681456\n",
      "\n",
      "Epoch:   11/30    Loss: 3.7689158143997195\n",
      "\n",
      "Epoch:   11/30    Loss: 3.7509423480033877\n",
      "\n",
      "Epoch:   11/30    Loss: 3.7454743366241456\n",
      "\n",
      "Epoch:   11/30    Loss: 3.725861217498779\n",
      "\n",
      "Epoch:   11/30    Loss: 3.733919694900513\n",
      "\n",
      "Epoch:   11/30    Loss: 3.7639176502227785\n",
      "\n",
      "Epoch:   11/30    Loss: 3.823072559118271\n",
      "\n",
      "Epoch:   11/30    Loss: 3.709207808494568\n",
      "\n",
      "Epoch:   11/30    Loss: 3.763757593870163\n",
      "\n",
      "Epoch:   11/30    Loss: 3.768814299106598\n",
      "\n",
      "Epoch:   11/30    Loss: 3.815746032238007\n",
      "\n",
      "Epoch:   11/30    Loss: 3.7998243451118467\n",
      "\n",
      "Epoch:   11/30    Loss: 3.8316574721336365\n",
      "\n",
      "Epoch:   11/30    Loss: 3.746732272148132\n",
      "\n",
      "Epoch:   12/30    Loss: 3.6824051958240873\n",
      "\n",
      "Epoch:   12/30    Loss: 3.6367193479537963\n",
      "\n",
      "Epoch:   12/30    Loss: 3.629098073005676\n",
      "\n",
      "Epoch:   12/30    Loss: 3.5808028802871705\n",
      "\n",
      "Epoch:   12/30    Loss: 3.560210198879242\n",
      "\n",
      "Epoch:   12/30    Loss: 3.6586552805900574\n",
      "\n",
      "Epoch:   12/30    Loss: 3.7000402855873107\n",
      "\n",
      "Epoch:   12/30    Loss: 3.6102086918354033\n",
      "\n",
      "Epoch:   12/30    Loss: 3.6914860548973083\n",
      "\n",
      "Epoch:   12/30    Loss: 3.6759599003791807\n",
      "\n",
      "Epoch:   12/30    Loss: 3.6648106648921965\n",
      "\n",
      "Epoch:   12/30    Loss: 3.6470461421012876\n",
      "\n",
      "Epoch:   12/30    Loss: 3.650985350608826\n",
      "\n",
      "Epoch:   12/30    Loss: 3.6330427384376525\n",
      "\n",
      "Epoch:   12/30    Loss: 3.73792909860611\n",
      "\n",
      "Epoch:   12/30    Loss: 3.710762845516205\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   12/30    Loss: 3.7375953764915466\n",
      "\n",
      "Epoch:   12/30    Loss: 3.6753658995628355\n",
      "\n",
      "Epoch:   12/30    Loss: 3.6519146614074707\n",
      "\n",
      "Epoch:   12/30    Loss: 3.7013957991600037\n",
      "\n",
      "Epoch:   12/30    Loss: 3.6464655060768125\n",
      "\n",
      "Epoch:   12/30    Loss: 3.6704834294319153\n",
      "\n",
      "Epoch:   12/30    Loss: 3.683557197093964\n",
      "\n",
      "Epoch:   12/30    Loss: 3.745357353210449\n",
      "\n",
      "Epoch:   12/30    Loss: 3.6925543365478517\n",
      "\n",
      "Epoch:   12/30    Loss: 3.7193840327262877\n",
      "\n",
      "Epoch:   12/30    Loss: 3.700639096260071\n",
      "\n",
      "Epoch:   12/30    Loss: 3.7534811148643494\n",
      "\n",
      "Epoch:   12/30    Loss: 3.6754116134643553\n",
      "\n",
      "Epoch:   12/30    Loss: 3.6973078227043152\n",
      "\n",
      "Epoch:   12/30    Loss: 3.729731701374054\n",
      "\n",
      "Epoch:   12/30    Loss: 3.711660390853882\n",
      "\n",
      "Epoch:   12/30    Loss: 3.7476407632827757\n",
      "\n",
      "Epoch:   12/30    Loss: 3.7176458792686464\n",
      "\n",
      "Epoch:   12/30    Loss: 3.697091348648071\n",
      "\n",
      "Epoch:   12/30    Loss: 3.7255109939575197\n",
      "\n",
      "Epoch:   12/30    Loss: 3.720441635131836\n",
      "\n",
      "Epoch:   12/30    Loss: 3.6811618342399597\n",
      "\n",
      "Epoch:   12/30    Loss: 3.76044119644165\n",
      "\n",
      "Epoch:   12/30    Loss: 3.7313628945350645\n",
      "\n",
      "Epoch:   12/30    Loss: 3.784727086544037\n",
      "\n",
      "Epoch:   12/30    Loss: 3.7343484282493593\n",
      "\n",
      "Epoch:   12/30    Loss: 3.746787495136261\n",
      "\n",
      "Epoch:   12/30    Loss: 3.738113431453705\n",
      "\n",
      "Epoch:   12/30    Loss: 3.7597002036571503\n",
      "\n",
      "Epoch:   12/30    Loss: 3.7520653867721556\n",
      "\n",
      "Epoch:   12/30    Loss: 3.75052702999115\n",
      "\n",
      "Epoch:   12/30    Loss: 3.7720541296005248\n",
      "\n",
      "Epoch:   12/30    Loss: 3.7660275630950926\n",
      "\n",
      "Epoch:   12/30    Loss: 3.7848360991477965\n",
      "\n",
      "Epoch:   12/30    Loss: 3.784640267133713\n",
      "\n",
      "Epoch:   12/30    Loss: 3.771111310958862\n",
      "\n",
      "Epoch:   12/30    Loss: 3.7743639302253724\n",
      "\n",
      "Epoch:   12/30    Loss: 3.821278765678406\n",
      "\n",
      "Epoch:   12/30    Loss: 3.800209474563599\n",
      "\n",
      "Epoch:   13/30    Loss: 3.6980083258184666\n",
      "\n",
      "Epoch:   13/30    Loss: 3.6502426691055296\n",
      "\n",
      "Epoch:   13/30    Loss: 3.6142437977790833\n",
      "\n",
      "Epoch:   13/30    Loss: 3.6248253145217895\n",
      "\n",
      "Epoch:   13/30    Loss: 3.668510115861893\n",
      "\n",
      "Epoch:   13/30    Loss: 3.6075840659141543\n",
      "\n",
      "Epoch:   13/30    Loss: 3.6095115327835083\n",
      "\n",
      "Epoch:   13/30    Loss: 3.6386279120445253\n",
      "\n",
      "Epoch:   13/30    Loss: 3.639197971343994\n",
      "\n",
      "Epoch:   13/30    Loss: 3.634574897289276\n",
      "\n",
      "Epoch:   13/30    Loss: 3.701177361011505\n",
      "\n",
      "Epoch:   13/30    Loss: 3.685034894943237\n",
      "\n",
      "Epoch:   13/30    Loss: 3.664485737323761\n",
      "\n",
      "Epoch:   13/30    Loss: 3.6293020877838136\n",
      "\n",
      "Epoch:   13/30    Loss: 3.6335147733688355\n",
      "\n",
      "Epoch:   13/30    Loss: 3.6392100520133974\n",
      "\n",
      "Epoch:   13/30    Loss: 3.6841966099739074\n",
      "\n",
      "Epoch:   13/30    Loss: 3.695401430130005\n",
      "\n",
      "Epoch:   13/30    Loss: 3.741195704936981\n",
      "\n",
      "Epoch:   13/30    Loss: 3.629121533155441\n",
      "\n",
      "Epoch:   13/30    Loss: 3.743380585193634\n",
      "\n",
      "Epoch:   13/30    Loss: 3.6992341256141663\n",
      "\n",
      "Epoch:   13/30    Loss: 3.6572504105567933\n",
      "\n",
      "Epoch:   13/30    Loss: 3.692677457332611\n",
      "\n",
      "Epoch:   13/30    Loss: 3.7179071669578554\n",
      "\n",
      "Epoch:   13/30    Loss: 3.6950315458774567\n",
      "\n",
      "Epoch:   13/30    Loss: 3.741266107559204\n",
      "\n",
      "Epoch:   13/30    Loss: 3.7011158912181856\n",
      "\n",
      "Epoch:   13/30    Loss: 3.6791277446746826\n",
      "\n",
      "Epoch:   13/30    Loss: 3.7020307450294494\n",
      "\n",
      "Epoch:   13/30    Loss: 3.6674459891319273\n",
      "\n",
      "Epoch:   13/30    Loss: 3.7828493375778196\n",
      "\n",
      "Epoch:   13/30    Loss: 3.7422804989814757\n",
      "\n",
      "Epoch:   13/30    Loss: 3.693328592300415\n",
      "\n",
      "Epoch:   13/30    Loss: 3.6678687176704408\n",
      "\n",
      "Epoch:   13/30    Loss: 3.7386015300750732\n",
      "\n",
      "Epoch:   13/30    Loss: 3.7397798714637758\n",
      "\n",
      "Epoch:   13/30    Loss: 3.7082194771766663\n",
      "\n",
      "Epoch:   13/30    Loss: 3.7171543765068056\n",
      "\n",
      "Epoch:   13/30    Loss: 3.7843377709388735\n",
      "\n",
      "Epoch:   13/30    Loss: 3.738024820327759\n",
      "\n",
      "Epoch:   13/30    Loss: 3.765097653388977\n",
      "\n",
      "Epoch:   13/30    Loss: 3.775838572025299\n",
      "\n",
      "Epoch:   13/30    Loss: 3.7449159207344054\n",
      "\n",
      "Epoch:   13/30    Loss: 3.7731447682380677\n",
      "\n",
      "Epoch:   13/30    Loss: 3.729698682308197\n",
      "\n",
      "Epoch:   13/30    Loss: 3.7395323848724367\n",
      "\n",
      "Epoch:   13/30    Loss: 3.7325804691314697\n",
      "\n",
      "Epoch:   13/30    Loss: 3.759058346271515\n",
      "\n",
      "Epoch:   13/30    Loss: 3.694303906440735\n",
      "\n",
      "Epoch:   13/30    Loss: 3.751484741687775\n",
      "\n",
      "Epoch:   13/30    Loss: 3.7985082440376283\n",
      "\n",
      "Epoch:   13/30    Loss: 3.776582459926605\n",
      "\n",
      "Epoch:   13/30    Loss: 3.709821716785431\n",
      "\n",
      "Epoch:   13/30    Loss: 3.7456414585113524\n",
      "\n",
      "Epoch:   14/30    Loss: 3.7025585337860942\n",
      "\n",
      "Epoch:   14/30    Loss: 3.602259861469269\n",
      "\n",
      "Epoch:   14/30    Loss: 3.633988373994827\n",
      "\n",
      "Epoch:   14/30    Loss: 3.630395037651062\n",
      "\n",
      "Epoch:   14/30    Loss: 3.618204692840576\n",
      "\n",
      "Epoch:   14/30    Loss: 3.6159804215431213\n",
      "\n",
      "Epoch:   14/30    Loss: 3.6301316084861757\n",
      "\n",
      "Epoch:   14/30    Loss: 3.669094621181488\n",
      "\n",
      "Epoch:   14/30    Loss: 3.6653813610076904\n",
      "\n",
      "Epoch:   14/30    Loss: 3.6397581927776335\n",
      "\n",
      "Epoch:   14/30    Loss: 3.597666996479034\n",
      "\n",
      "Epoch:   14/30    Loss: 3.629511827468872\n",
      "\n",
      "Epoch:   14/30    Loss: 3.6542867333889006\n",
      "\n",
      "Epoch:   14/30    Loss: 3.668342022418976\n",
      "\n",
      "Epoch:   14/30    Loss: 3.660148499965668\n",
      "\n",
      "Epoch:   14/30    Loss: 3.6501738147735594\n",
      "\n",
      "Epoch:   14/30    Loss: 3.6901273045539855\n",
      "\n",
      "Epoch:   14/30    Loss: 3.666771099567413\n",
      "\n",
      "Epoch:   14/30    Loss: 3.679632886886597\n",
      "\n",
      "Epoch:   14/30    Loss: 3.7012426924705504\n",
      "\n",
      "Epoch:   14/30    Loss: 3.657415297985077\n",
      "\n",
      "Epoch:   14/30    Loss: 3.6693802509307862\n",
      "\n",
      "Epoch:   14/30    Loss: 3.6795445609092714\n",
      "\n",
      "Epoch:   14/30    Loss: 3.6943668737411497\n",
      "\n",
      "Epoch:   14/30    Loss: 3.6834056067466734\n",
      "\n",
      "Epoch:   14/30    Loss: 3.712310959339142\n",
      "\n",
      "Epoch:   14/30    Loss: 3.718969741344452\n",
      "\n",
      "Epoch:   14/30    Loss: 3.708619800567627\n",
      "\n",
      "Epoch:   14/30    Loss: 3.672009287595749\n",
      "\n",
      "Epoch:   14/30    Loss: 3.6914332857131957\n",
      "\n",
      "Epoch:   14/30    Loss: 3.7304950981140137\n",
      "\n",
      "Epoch:   14/30    Loss: 3.7004825365543366\n",
      "\n",
      "Epoch:   14/30    Loss: 3.6731523370742796\n",
      "\n",
      "Epoch:   14/30    Loss: 3.7251203219890594\n",
      "\n",
      "Epoch:   14/30    Loss: 3.7002680447101595\n",
      "\n",
      "Epoch:   14/30    Loss: 3.7072892198562624\n",
      "\n",
      "Epoch:   14/30    Loss: 3.75169621181488\n",
      "\n",
      "Epoch:   14/30    Loss: 3.7162140369415284\n",
      "\n",
      "Epoch:   14/30    Loss: 3.8073478002548216\n",
      "\n",
      "Epoch:   14/30    Loss: 3.734648541688919\n",
      "\n",
      "Epoch:   14/30    Loss: 3.6956396095752715\n",
      "\n",
      "Epoch:   14/30    Loss: 3.7480767703056337\n",
      "\n",
      "Epoch:   14/30    Loss: 3.7429270877838134\n",
      "\n",
      "Epoch:   14/30    Loss: 3.7327572751045226\n",
      "\n",
      "Epoch:   14/30    Loss: 3.6953547167778016\n",
      "\n",
      "Epoch:   14/30    Loss: 3.6811015167236327\n",
      "\n",
      "Epoch:   14/30    Loss: 3.702303837776184\n",
      "\n",
      "Epoch:   14/30    Loss: 3.7826434092521666\n",
      "\n",
      "Epoch:   14/30    Loss: 3.777364938735962\n",
      "\n",
      "Epoch:   14/30    Loss: 3.7295126481056213\n",
      "\n",
      "Epoch:   14/30    Loss: 3.774613895177841\n",
      "\n",
      "Epoch:   14/30    Loss: 3.7674717044830324\n",
      "\n",
      "Epoch:   14/30    Loss: 3.7238253726959227\n",
      "\n",
      "Epoch:   14/30    Loss: 3.771603218793869\n",
      "\n",
      "Epoch:   14/30    Loss: 3.775143202781677\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 30 more epochs\n",
    "num_epochs = 30\n",
    "\n",
    "# training the model\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches, path)\n",
    "\n",
    "# saving the trained model\n",
    "helper.save_model(path, trained_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data params\n",
    "# Sequence Length\n",
    "sequence_length = 50  # of words in a sequence\n",
    "# Batch Size\n",
    "batch_size = 32\n",
    "\n",
    "# data loader - do not change\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)\n",
    "\n",
    "# Training parameters\n",
    "# Number of Epochs\n",
    "num_epochs = 30\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model parameters\n",
    "# Vocab size\n",
    "vocab_size = len(vocab_to_int)\n",
    "# Output size\n",
    "output_size = len(vocab_to_int)\n",
    "# Embedding Dimension\n",
    "embedding_dim = 200\n",
    "# Hidden Dimension\n",
    "hidden_dim = 256\n",
    "# Number of RNN Layers\n",
    "n_layers = 2\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model and move to gpu if available\n",
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.2)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "# defining loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "path = './save/trained_rnn_256_02_50.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50 epoch(s)...\n",
      "Epoch:    1/50    Loss: 5.685048009395599\n",
      "\n",
      "Epoch:    1/50    Loss: 5.1329756364822385\n",
      "\n",
      "Epoch:    1/50    Loss: 4.994591465950013\n",
      "\n",
      "Epoch:    1/50    Loss: 4.8797411217689515\n",
      "\n",
      "Epoch:    1/50    Loss: 4.879319806098938\n",
      "\n",
      "Epoch:    1/50    Loss: 4.738553686141968\n",
      "\n",
      "Epoch:    1/50    Loss: 4.6876709861755375\n",
      "\n",
      "Epoch:    1/50    Loss: 4.624606618404388\n",
      "\n",
      "Epoch:    1/50    Loss: 4.604458067893982\n",
      "\n",
      "Epoch:    1/50    Loss: 4.622936310291291\n",
      "\n",
      "Epoch:    1/50    Loss: 4.529778764247895\n",
      "\n",
      "Epoch:    1/50    Loss: 4.549249366283417\n",
      "\n",
      "Epoch:    1/50    Loss: 4.501344722270965\n",
      "\n",
      "Epoch:    1/50    Loss: 4.488463315486908\n",
      "\n",
      "Epoch:    1/50    Loss: 4.457786512851715\n",
      "\n",
      "Epoch:    1/50    Loss: 4.476368714809418\n",
      "\n",
      "Epoch:    1/50    Loss: 4.433107254028321\n",
      "\n",
      "Epoch:    1/50    Loss: 4.435282142162323\n",
      "\n",
      "Epoch:    1/50    Loss: 4.409528411865234\n",
      "\n",
      "Epoch:    1/50    Loss: 4.466201627254486\n",
      "\n",
      "Epoch:    1/50    Loss: 4.422207685947418\n",
      "\n",
      "Epoch:    1/50    Loss: 4.438485827445984\n",
      "\n",
      "Epoch:    1/50    Loss: 4.341257464408875\n",
      "\n",
      "Epoch:    1/50    Loss: 4.3053562245368955\n",
      "\n",
      "Epoch:    1/50    Loss: 4.3459654216766355\n",
      "\n",
      "Epoch:    1/50    Loss: 4.35303009557724\n",
      "\n",
      "Epoch:    1/50    Loss: 4.335308005332947\n",
      "\n",
      "Epoch:    1/50    Loss: 4.3295113091468815\n",
      "\n",
      "Epoch:    1/50    Loss: 4.335898066043854\n",
      "\n",
      "Epoch:    1/50    Loss: 4.345632545948028\n",
      "\n",
      "Epoch:    1/50    Loss: 4.306132273674011\n",
      "\n",
      "Epoch:    1/50    Loss: 4.287344679355622\n",
      "\n",
      "Epoch:    1/50    Loss: 4.2722327256202695\n",
      "\n",
      "Epoch:    1/50    Loss: 4.283062111854553\n",
      "\n",
      "Epoch:    1/50    Loss: 4.322088387489319\n",
      "\n",
      "Epoch:    1/50    Loss: 4.330937077045441\n",
      "\n",
      "Epoch:    1/50    Loss: 4.320365389347076\n",
      "\n",
      "Epoch:    1/50    Loss: 4.304198430061341\n",
      "\n",
      "Epoch:    1/50    Loss: 4.254876298427582\n",
      "\n",
      "Epoch:    1/50    Loss: 4.293918390274047\n",
      "\n",
      "Epoch:    1/50    Loss: 4.318804376125335\n",
      "\n",
      "Epoch:    1/50    Loss: 4.280623865127564\n",
      "\n",
      "Epoch:    1/50    Loss: 4.217215072154999\n",
      "\n",
      "Epoch:    1/50    Loss: 4.232402142047882\n",
      "\n",
      "Epoch:    1/50    Loss: 4.275009516716003\n",
      "\n",
      "Epoch:    1/50    Loss: 4.207522782802582\n",
      "\n",
      "Epoch:    1/50    Loss: 4.269136159896851\n",
      "\n",
      "Epoch:    1/50    Loss: 4.221053208351135\n",
      "\n",
      "Epoch:    1/50    Loss: 4.2490044736862185\n",
      "\n",
      "Epoch:    1/50    Loss: 4.2501928601264956\n",
      "\n",
      "Epoch:    1/50    Loss: 4.282561422348023\n",
      "\n",
      "Epoch:    1/50    Loss: 4.245893078327179\n",
      "\n",
      "Epoch:    1/50    Loss: 4.222499364376068\n",
      "\n",
      "Epoch:    1/50    Loss: 4.226141643047333\n",
      "\n",
      "Epoch:    1/50    Loss: 4.219094651222229\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    2/50    Loss: 4.151971351882638\n",
      "\n",
      "Epoch:    2/50    Loss: 4.0496391592025756\n",
      "\n",
      "Epoch:    2/50    Loss: 4.060782099246979\n",
      "\n",
      "Epoch:    2/50    Loss: 4.057024963855744\n",
      "\n",
      "Epoch:    2/50    Loss: 4.118402906417847\n",
      "\n",
      "Epoch:    2/50    Loss: 4.066749235153198\n",
      "\n",
      "Epoch:    2/50    Loss: 4.104742618083954\n",
      "\n",
      "Epoch:    2/50    Loss: 4.1109090433120725\n",
      "\n",
      "Epoch:    2/50    Loss: 4.108331465721131\n",
      "\n",
      "Epoch:    2/50    Loss: 4.068841254472733\n",
      "\n",
      "Epoch:    2/50    Loss: 4.1599115042686465\n",
      "\n",
      "Epoch:    2/50    Loss: 4.084322585582733\n",
      "\n",
      "Epoch:    2/50    Loss: 4.121622440338135\n",
      "\n",
      "Epoch:    2/50    Loss: 4.147054700851441\n",
      "\n",
      "Epoch:    2/50    Loss: 4.147810961723327\n",
      "\n",
      "Epoch:    2/50    Loss: 4.122154385566711\n",
      "\n",
      "Epoch:    2/50    Loss: 4.160003968238831\n",
      "\n",
      "Epoch:    2/50    Loss: 4.173973856925964\n",
      "\n",
      "Epoch:    2/50    Loss: 4.135274160385132\n",
      "\n",
      "Epoch:    2/50    Loss: 4.092129889965057\n",
      "\n",
      "Epoch:    2/50    Loss: 4.115961433410645\n",
      "\n",
      "Epoch:    2/50    Loss: 4.133073672771454\n",
      "\n",
      "Epoch:    2/50    Loss: 4.096144061565399\n",
      "\n",
      "Epoch:    2/50    Loss: 4.096924269199372\n",
      "\n",
      "Epoch:    2/50    Loss: 4.114502998828888\n",
      "\n",
      "Epoch:    2/50    Loss: 4.141309792995453\n",
      "\n",
      "Epoch:    2/50    Loss: 4.152046485424042\n",
      "\n",
      "Epoch:    2/50    Loss: 4.136328838348389\n",
      "\n",
      "Epoch:    2/50    Loss: 4.17349443769455\n",
      "\n",
      "Epoch:    2/50    Loss: 4.199922616958618\n",
      "\n",
      "Epoch:    2/50    Loss: 4.14822325181961\n",
      "\n",
      "Epoch:    2/50    Loss: 4.1198089022636415\n",
      "\n",
      "Epoch:    2/50    Loss: 4.063295217990875\n",
      "\n",
      "Epoch:    2/50    Loss: 4.129193519115448\n",
      "\n",
      "Epoch:    2/50    Loss: 4.1792303814888\n",
      "\n",
      "Epoch:    2/50    Loss: 4.1108463969230655\n",
      "\n",
      "Epoch:    2/50    Loss: 4.135543274879455\n",
      "\n",
      "Epoch:    2/50    Loss: 4.125399582862854\n",
      "\n",
      "Epoch:    2/50    Loss: 4.165905167579651\n",
      "\n",
      "Epoch:    2/50    Loss: 4.062586151123047\n",
      "\n",
      "Epoch:    2/50    Loss: 4.087446662902832\n",
      "\n",
      "Epoch:    2/50    Loss: 4.114668424129486\n",
      "\n",
      "Epoch:    2/50    Loss: 4.066007351398468\n",
      "\n",
      "Epoch:    2/50    Loss: 4.148806417942047\n",
      "\n",
      "Epoch:    2/50    Loss: 4.120964694499969\n",
      "\n",
      "Epoch:    2/50    Loss: 4.1165189456939695\n",
      "\n",
      "Epoch:    2/50    Loss: 4.187835682868958\n",
      "\n",
      "Epoch:    2/50    Loss: 4.126693629264832\n",
      "\n",
      "Epoch:    2/50    Loss: 4.164907735824585\n",
      "\n",
      "Epoch:    2/50    Loss: 4.151688796520233\n",
      "\n",
      "Epoch:    2/50    Loss: 4.16391264629364\n",
      "\n",
      "Epoch:    2/50    Loss: 4.1334132113456725\n",
      "\n",
      "Epoch:    2/50    Loss: 4.136273201227188\n",
      "\n",
      "Epoch:    2/50    Loss: 4.091074924468995\n",
      "\n",
      "Epoch:    2/50    Loss: 4.092358067035675\n",
      "\n",
      "Epoch:    3/50    Loss: 4.028265682257474\n",
      "\n",
      "Epoch:    3/50    Loss: 3.9929762196540834\n",
      "\n",
      "Epoch:    3/50    Loss: 3.949726313114166\n",
      "\n",
      "Epoch:    3/50    Loss: 3.9951522088050844\n",
      "\n",
      "Epoch:    3/50    Loss: 3.98779195690155\n",
      "\n",
      "Epoch:    3/50    Loss: 4.009870512008667\n",
      "\n",
      "Epoch:    3/50    Loss: 4.002447407245636\n",
      "\n",
      "Epoch:    3/50    Loss: 4.027440876483917\n",
      "\n",
      "Epoch:    3/50    Loss: 4.004753342151642\n",
      "\n",
      "Epoch:    3/50    Loss: 3.989502324104309\n",
      "\n",
      "Epoch:    3/50    Loss: 4.0284571890831\n",
      "\n",
      "Epoch:    3/50    Loss: 4.015265898227692\n",
      "\n",
      "Epoch:    3/50    Loss: 4.05682644367218\n",
      "\n",
      "Epoch:    3/50    Loss: 4.0298200383186344\n",
      "\n",
      "Epoch:    3/50    Loss: 4.011434971809387\n",
      "\n",
      "Epoch:    3/50    Loss: 4.057143946647644\n",
      "\n",
      "Epoch:    3/50    Loss: 4.019872124195099\n",
      "\n",
      "Epoch:    3/50    Loss: 4.034874350070953\n",
      "\n",
      "Epoch:    3/50    Loss: 4.051180665493011\n",
      "\n",
      "Epoch:    3/50    Loss: 4.016830743789673\n",
      "\n",
      "Epoch:    3/50    Loss: 4.023281661987305\n",
      "\n",
      "Epoch:    3/50    Loss: 4.027473686695099\n",
      "\n",
      "Epoch:    3/50    Loss: 4.044107924938202\n",
      "\n",
      "Epoch:    3/50    Loss: 4.003603323936463\n",
      "\n",
      "Epoch:    3/50    Loss: 4.048120232582092\n",
      "\n",
      "Epoch:    3/50    Loss: 4.007785218715668\n",
      "\n",
      "Epoch:    3/50    Loss: 4.033333487033844\n",
      "\n",
      "Epoch:    3/50    Loss: 4.071577681541443\n",
      "\n",
      "Epoch:    3/50    Loss: 4.067813439846039\n",
      "\n",
      "Epoch:    3/50    Loss: 4.068173684597015\n",
      "\n",
      "Epoch:    3/50    Loss: 4.0578545351028446\n",
      "\n",
      "Epoch:    3/50    Loss: 4.096163235187531\n",
      "\n",
      "Epoch:    3/50    Loss: 4.079602712154388\n",
      "\n",
      "Epoch:    3/50    Loss: 4.085672975540161\n",
      "\n",
      "Epoch:    3/50    Loss: 4.040180401802063\n",
      "\n",
      "Epoch:    3/50    Loss: 4.082153419971466\n",
      "\n",
      "Epoch:    3/50    Loss: 4.094188144683838\n",
      "\n",
      "Epoch:    3/50    Loss: 4.044191102981568\n",
      "\n",
      "Epoch:    3/50    Loss: 4.0452523021698\n",
      "\n",
      "Epoch:    3/50    Loss: 3.99752814245224\n",
      "\n",
      "Epoch:    3/50    Loss: 4.031062400341034\n",
      "\n",
      "Epoch:    3/50    Loss: 4.095609966754913\n",
      "\n",
      "Epoch:    3/50    Loss: 4.06834372329712\n",
      "\n",
      "Epoch:    3/50    Loss: 4.1243957018852235\n",
      "\n",
      "Epoch:    3/50    Loss: 4.019148559093475\n",
      "\n",
      "Epoch:    3/50    Loss: 4.062911458492279\n",
      "\n",
      "Epoch:    3/50    Loss: 3.9966269726753234\n",
      "\n",
      "Epoch:    3/50    Loss: 4.037288097858429\n",
      "\n",
      "Epoch:    3/50    Loss: 4.033727947711944\n",
      "\n",
      "Epoch:    3/50    Loss: 4.07358871459961\n",
      "\n",
      "Epoch:    3/50    Loss: 4.125852843284607\n",
      "\n",
      "Epoch:    3/50    Loss: 4.101249570846558\n",
      "\n",
      "Epoch:    3/50    Loss: 4.0219313654899596\n",
      "\n",
      "Epoch:    3/50    Loss: 4.082438663959503\n",
      "\n",
      "Epoch:    3/50    Loss: 4.038572130203247\n",
      "\n",
      "Epoch:    4/50    Loss: 3.9743287899450626\n",
      "\n",
      "Epoch:    4/50    Loss: 3.875429002761841\n",
      "\n",
      "Epoch:    4/50    Loss: 3.924068035125732\n",
      "\n",
      "Epoch:    4/50    Loss: 3.9380896482467653\n",
      "\n",
      "Epoch:    4/50    Loss: 3.9380440921783446\n",
      "\n",
      "Epoch:    4/50    Loss: 3.925919470310211\n",
      "\n",
      "Epoch:    4/50    Loss: 3.95513445854187\n",
      "\n",
      "Epoch:    4/50    Loss: 3.973057705402374\n",
      "\n",
      "Epoch:    4/50    Loss: 3.9400678272247314\n",
      "\n",
      "Epoch:    4/50    Loss: 3.959557380199432\n",
      "\n",
      "Epoch:    4/50    Loss: 3.907839421749115\n",
      "\n",
      "Epoch:    4/50    Loss: 3.9752906522750853\n",
      "\n",
      "Epoch:    4/50    Loss: 3.9492488474845886\n",
      "\n",
      "Epoch:    4/50    Loss: 3.959997089385986\n",
      "\n",
      "Epoch:    4/50    Loss: 4.000778465270996\n",
      "\n",
      "Epoch:    4/50    Loss: 3.9450328521728517\n",
      "\n",
      "Epoch:    4/50    Loss: 3.958462956905365\n",
      "\n",
      "Epoch:    4/50    Loss: 3.9289774494171144\n",
      "\n",
      "Epoch:    4/50    Loss: 4.005276278972626\n",
      "\n",
      "Epoch:    4/50    Loss: 4.018716547966004\n",
      "\n",
      "Epoch:    4/50    Loss: 4.035465517997742\n",
      "\n",
      "Epoch:    4/50    Loss: 3.9695174050331117\n",
      "\n",
      "Epoch:    4/50    Loss: 3.978791721343994\n",
      "\n",
      "Epoch:    4/50    Loss: 3.9791518964767456\n",
      "\n",
      "Epoch:    4/50    Loss: 4.009143200874329\n",
      "\n",
      "Epoch:    4/50    Loss: 4.02148960351944\n",
      "\n",
      "Epoch:    4/50    Loss: 3.9840310177803038\n",
      "\n",
      "Epoch:    4/50    Loss: 4.020355056762695\n",
      "\n",
      "Epoch:    4/50    Loss: 3.9985571789741514\n",
      "\n",
      "Epoch:    4/50    Loss: 3.9266690382957457\n",
      "\n",
      "Epoch:    4/50    Loss: 3.988805763244629\n",
      "\n",
      "Epoch:    4/50    Loss: 4.019857738494873\n",
      "\n",
      "Epoch:    4/50    Loss: 4.0201555705070495\n",
      "\n",
      "Epoch:    4/50    Loss: 4.036265418052674\n",
      "\n",
      "Epoch:    4/50    Loss: 4.018684745788574\n",
      "\n",
      "Epoch:    4/50    Loss: 3.9739542655944824\n",
      "\n",
      "Epoch:    4/50    Loss: 3.9603335938453674\n",
      "\n",
      "Epoch:    4/50    Loss: 3.975317301750183\n",
      "\n",
      "Epoch:    4/50    Loss: 3.977572901248932\n",
      "\n",
      "Epoch:    4/50    Loss: 4.0287847261428835\n",
      "\n",
      "Epoch:    4/50    Loss: 3.9927136220932007\n",
      "\n",
      "Epoch:    4/50    Loss: 4.047961896419525\n",
      "\n",
      "Epoch:    4/50    Loss: 3.99469869184494\n",
      "\n",
      "Epoch:    4/50    Loss: 4.022287598609925\n",
      "\n",
      "Epoch:    4/50    Loss: 4.02352931690216\n",
      "\n",
      "Epoch:    4/50    Loss: 4.0011302862167355\n",
      "\n",
      "Epoch:    4/50    Loss: 4.0961893310546875\n",
      "\n",
      "Epoch:    4/50    Loss: 4.050855748176574\n",
      "\n",
      "Epoch:    4/50    Loss: 3.967140483379364\n",
      "\n",
      "Epoch:    4/50    Loss: 4.056999904155731\n",
      "\n",
      "Epoch:    4/50    Loss: 3.9923979320526124\n",
      "\n",
      "Epoch:    4/50    Loss: 3.984301695823669\n",
      "\n",
      "Epoch:    4/50    Loss: 4.022336521625519\n",
      "\n",
      "Epoch:    4/50    Loss: 4.051270463466644\n",
      "\n",
      "Epoch:    4/50    Loss: 4.036283653736114\n",
      "\n",
      "Epoch:    5/50    Loss: 3.9066649840030494\n",
      "\n",
      "Epoch:    5/50    Loss: 3.8662576255798338\n",
      "\n",
      "Epoch:    5/50    Loss: 3.877395781517029\n",
      "\n",
      "Epoch:    5/50    Loss: 3.9173671765327454\n",
      "\n",
      "Epoch:    5/50    Loss: 3.8745916895866395\n",
      "\n",
      "Epoch:    5/50    Loss: 3.92428675365448\n",
      "\n",
      "Epoch:    5/50    Loss: 3.882497619152069\n",
      "\n",
      "Epoch:    5/50    Loss: 3.9249680457115175\n",
      "\n",
      "Epoch:    5/50    Loss: 3.877353003978729\n",
      "\n",
      "Epoch:    5/50    Loss: 3.8968655047416685\n",
      "\n",
      "Epoch:    5/50    Loss: 3.92571683883667\n",
      "\n",
      "Epoch:    5/50    Loss: 3.9325492687225343\n",
      "\n",
      "Epoch:    5/50    Loss: 3.8989757466316224\n",
      "\n",
      "Epoch:    5/50    Loss: 3.9221433806419372\n",
      "\n",
      "Epoch:    5/50    Loss: 3.898358899593353\n",
      "\n",
      "Epoch:    5/50    Loss: 3.901142221927643\n",
      "\n",
      "Epoch:    5/50    Loss: 3.9391865978240967\n",
      "\n",
      "Epoch:    5/50    Loss: 3.877298502445221\n",
      "\n",
      "Epoch:    5/50    Loss: 3.9759465942382812\n",
      "\n",
      "Epoch:    5/50    Loss: 3.9115162911415102\n",
      "\n",
      "Epoch:    5/50    Loss: 3.9093574118614196\n",
      "\n",
      "Epoch:    5/50    Loss: 3.9182687220573427\n",
      "\n",
      "Epoch:    5/50    Loss: 3.9318768582344057\n",
      "\n",
      "Epoch:    5/50    Loss: 3.957667486190796\n",
      "\n",
      "Epoch:    5/50    Loss: 3.8952803654670713\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    5/50    Loss: 3.91636496591568\n",
      "\n",
      "Epoch:    5/50    Loss: 3.9201814498901366\n",
      "\n",
      "Epoch:    5/50    Loss: 3.998312590837479\n",
      "\n",
      "Epoch:    5/50    Loss: 3.9385947732925417\n",
      "\n",
      "Epoch:    5/50    Loss: 3.935790941238403\n",
      "\n",
      "Epoch:    5/50    Loss: 3.9802274551391603\n",
      "\n",
      "Epoch:    5/50    Loss: 3.929594298839569\n",
      "\n",
      "Epoch:    5/50    Loss: 3.974986482143402\n",
      "\n",
      "Epoch:    5/50    Loss: 3.9732351784706115\n",
      "\n",
      "Epoch:    5/50    Loss: 3.9431003451347353\n",
      "\n",
      "Epoch:    5/50    Loss: 4.00488867521286\n",
      "\n",
      "Epoch:    5/50    Loss: 4.016292507171631\n",
      "\n",
      "Epoch:    5/50    Loss: 3.937231632232666\n",
      "\n",
      "Epoch:    5/50    Loss: 3.981154451370239\n",
      "\n",
      "Epoch:    5/50    Loss: 3.952433512687683\n",
      "\n",
      "Epoch:    5/50    Loss: 4.01593558883667\n",
      "\n",
      "Epoch:    5/50    Loss: 3.992716524600983\n",
      "\n",
      "Epoch:    5/50    Loss: 3.9460494599342346\n",
      "\n",
      "Epoch:    5/50    Loss: 3.9484254622459414\n",
      "\n",
      "Epoch:    5/50    Loss: 3.937362286567688\n",
      "\n",
      "Epoch:    5/50    Loss: 4.029122782707215\n",
      "\n",
      "Epoch:    5/50    Loss: 3.9952198519706728\n",
      "\n",
      "Epoch:    5/50    Loss: 3.9955338225364687\n",
      "\n",
      "Epoch:    5/50    Loss: 3.9817421572208405\n",
      "\n",
      "Epoch:    5/50    Loss: 3.936818920135498\n",
      "\n",
      "Epoch:    5/50    Loss: 3.9867468237876893\n",
      "\n",
      "Epoch:    5/50    Loss: 3.9903808455467225\n",
      "\n",
      "Epoch:    5/50    Loss: 4.060189168453217\n",
      "\n",
      "Epoch:    5/50    Loss: 3.9885107078552244\n",
      "\n",
      "Epoch:    5/50    Loss: 3.946423926830292\n",
      "\n",
      "Epoch:    6/50    Loss: 3.8839674905010555\n",
      "\n",
      "Epoch:    6/50    Loss: 3.8366626863479616\n",
      "\n",
      "Epoch:    6/50    Loss: 3.8667593789100647\n",
      "\n",
      "Epoch:    6/50    Loss: 3.8524732971191407\n",
      "\n",
      "Epoch:    6/50    Loss: 3.846280966281891\n",
      "\n",
      "Epoch:    6/50    Loss: 3.8506519632339478\n",
      "\n",
      "Epoch:    6/50    Loss: 3.8582960352897646\n",
      "\n",
      "Epoch:    6/50    Loss: 3.900390534877777\n",
      "\n",
      "Epoch:    6/50    Loss: 3.909676083087921\n",
      "\n",
      "Epoch:    6/50    Loss: 3.899766970396042\n",
      "\n",
      "Epoch:    6/50    Loss: 3.844661253452301\n",
      "\n",
      "Epoch:    6/50    Loss: 3.887787196159363\n",
      "\n",
      "Epoch:    6/50    Loss: 3.839520107984543\n",
      "\n",
      "Epoch:    6/50    Loss: 3.928454780578613\n",
      "\n",
      "Epoch:    6/50    Loss: 3.921516552448273\n",
      "\n",
      "Epoch:    6/50    Loss: 3.884833640575409\n",
      "\n",
      "Epoch:    6/50    Loss: 3.8850118694305418\n",
      "\n",
      "Epoch:    6/50    Loss: 3.933588583469391\n",
      "\n",
      "Epoch:    6/50    Loss: 3.895422646045685\n",
      "\n",
      "Epoch:    6/50    Loss: 3.871016965866089\n",
      "\n",
      "Epoch:    6/50    Loss: 3.9097711968421938\n",
      "\n",
      "Epoch:    6/50    Loss: 3.9310649461746214\n",
      "\n",
      "Epoch:    6/50    Loss: 3.8681336839199068\n",
      "\n",
      "Epoch:    6/50    Loss: 3.9376183252334593\n",
      "\n",
      "Epoch:    6/50    Loss: 3.915304048061371\n",
      "\n",
      "Epoch:    6/50    Loss: 3.9014670066833497\n",
      "\n",
      "Epoch:    6/50    Loss: 3.8158485488891603\n",
      "\n",
      "Epoch:    6/50    Loss: 3.8959741826057432\n",
      "\n",
      "Epoch:    6/50    Loss: 3.9429489917755127\n",
      "\n",
      "Epoch:    6/50    Loss: 3.904821095466614\n",
      "\n",
      "Epoch:    6/50    Loss: 3.9308335280418394\n",
      "\n",
      "Epoch:    6/50    Loss: 3.8786418871879578\n",
      "\n",
      "Epoch:    6/50    Loss: 3.9585287466049195\n",
      "\n",
      "Epoch:    6/50    Loss: 3.891883060455322\n",
      "\n",
      "Epoch:    6/50    Loss: 3.9676803283691404\n",
      "\n",
      "Epoch:    6/50    Loss: 3.925051025390625\n",
      "\n",
      "Epoch:    6/50    Loss: 3.8787657141685488\n",
      "\n",
      "Epoch:    6/50    Loss: 3.9464194679260256\n",
      "\n",
      "Epoch:    6/50    Loss: 3.967624288082123\n",
      "\n",
      "Epoch:    6/50    Loss: 3.9284767093658446\n",
      "\n",
      "Epoch:    6/50    Loss: 3.9090957970619202\n",
      "\n",
      "Epoch:    6/50    Loss: 3.926970970630646\n",
      "\n",
      "Epoch:    6/50    Loss: 3.9519478611946104\n",
      "\n",
      "Epoch:    6/50    Loss: 3.961843852519989\n",
      "\n",
      "Epoch:    6/50    Loss: 3.977443771839142\n",
      "\n",
      "Epoch:    6/50    Loss: 3.9642465224266052\n",
      "\n",
      "Epoch:    6/50    Loss: 3.9153035154342652\n",
      "\n",
      "Epoch:    6/50    Loss: 3.9940912246704103\n",
      "\n",
      "Epoch:    6/50    Loss: 3.93159436416626\n",
      "\n",
      "Epoch:    6/50    Loss: 3.92790877532959\n",
      "\n",
      "Epoch:    6/50    Loss: 3.9797411077022553\n",
      "\n",
      "Epoch:    6/50    Loss: 3.9757020759582518\n",
      "\n",
      "Epoch:    6/50    Loss: 3.9665702228546142\n",
      "\n",
      "Epoch:    6/50    Loss: 3.9454944705963135\n",
      "\n",
      "Epoch:    6/50    Loss: 3.9593262600898744\n",
      "\n",
      "Epoch:    7/50    Loss: 3.8571155014103407\n",
      "\n",
      "Epoch:    7/50    Loss: 3.8300516312122346\n",
      "\n",
      "Epoch:    7/50    Loss: 3.782397569179535\n",
      "\n",
      "Epoch:    7/50    Loss: 3.872949987888336\n",
      "\n",
      "Epoch:    7/50    Loss: 3.8577808635234834\n",
      "\n",
      "Epoch:    7/50    Loss: 3.813651307106018\n",
      "\n",
      "Epoch:    7/50    Loss: 3.8533320550918577\n",
      "\n",
      "Epoch:    7/50    Loss: 3.7819964251518248\n",
      "\n",
      "Epoch:    7/50    Loss: 3.8459972410202026\n",
      "\n",
      "Epoch:    7/50    Loss: 3.8040849437713624\n",
      "\n",
      "Epoch:    7/50    Loss: 3.8606775608062742\n",
      "\n",
      "Epoch:    7/50    Loss: 3.845290780067444\n",
      "\n",
      "Epoch:    7/50    Loss: 3.8128377895355223\n",
      "\n",
      "Epoch:    7/50    Loss: 3.8675519523620605\n",
      "\n",
      "Epoch:    7/50    Loss: 3.8521549129486083\n",
      "\n",
      "Epoch:    7/50    Loss: 3.868437834739685\n",
      "\n",
      "Epoch:    7/50    Loss: 3.8789988722801207\n",
      "\n",
      "Epoch:    7/50    Loss: 3.8219437589645384\n",
      "\n",
      "Epoch:    7/50    Loss: 3.8733252120018005\n",
      "\n",
      "Epoch:    7/50    Loss: 3.8389461624622343\n",
      "\n",
      "Epoch:    7/50    Loss: 3.8836796684265136\n",
      "\n",
      "Epoch:    7/50    Loss: 3.895865566730499\n",
      "\n",
      "Epoch:    7/50    Loss: 3.9068257684707643\n",
      "\n",
      "Epoch:    7/50    Loss: 3.8800342359542848\n",
      "\n",
      "Epoch:    7/50    Loss: 3.8724151940345766\n",
      "\n",
      "Epoch:    7/50    Loss: 3.880751064300537\n",
      "\n",
      "Epoch:    7/50    Loss: 3.9423527212142946\n",
      "\n",
      "Epoch:    7/50    Loss: 3.832540672302246\n",
      "\n",
      "Epoch:    7/50    Loss: 3.8806977090835573\n",
      "\n",
      "Epoch:    7/50    Loss: 3.9529941787719727\n",
      "\n",
      "Epoch:    7/50    Loss: 3.8705293660163878\n",
      "\n",
      "Epoch:    7/50    Loss: 3.903194399833679\n",
      "\n",
      "Epoch:    7/50    Loss: 3.9340760564804076\n",
      "\n",
      "Epoch:    7/50    Loss: 3.8869183840751647\n",
      "\n",
      "Epoch:    7/50    Loss: 3.9006630487442018\n",
      "\n",
      "Epoch:    7/50    Loss: 3.9386240158081054\n",
      "\n",
      "Epoch:    7/50    Loss: 3.878761387825012\n",
      "\n",
      "Epoch:    7/50    Loss: 3.9159085969924927\n",
      "\n",
      "Epoch:    7/50    Loss: 3.8907131605148315\n",
      "\n",
      "Epoch:    7/50    Loss: 3.948006820201874\n",
      "\n",
      "Epoch:    7/50    Loss: 3.9005776906013487\n",
      "\n",
      "Epoch:    7/50    Loss: 3.937498876094818\n",
      "\n",
      "Epoch:    7/50    Loss: 3.889160831451416\n",
      "\n",
      "Epoch:    7/50    Loss: 3.8630682725906373\n",
      "\n",
      "Epoch:    7/50    Loss: 3.8916489996910095\n",
      "\n",
      "Epoch:    7/50    Loss: 3.94109765958786\n",
      "\n",
      "Epoch:    7/50    Loss: 3.9509037013053896\n",
      "\n",
      "Epoch:    7/50    Loss: 3.973099085330963\n",
      "\n",
      "Epoch:    7/50    Loss: 3.9530017013549803\n",
      "\n",
      "Epoch:    7/50    Loss: 3.9643595905303957\n",
      "\n",
      "Epoch:    7/50    Loss: 3.9646948981285095\n",
      "\n",
      "Epoch:    7/50    Loss: 3.9463688440322877\n",
      "\n",
      "Epoch:    7/50    Loss: 3.9035770745277403\n",
      "\n",
      "Epoch:    7/50    Loss: 3.890948510169983\n",
      "\n",
      "Epoch:    7/50    Loss: 3.921826302051544\n",
      "\n",
      "Epoch:    8/50    Loss: 3.8766756305411527\n",
      "\n",
      "Epoch:    8/50    Loss: 3.780636642456055\n",
      "\n",
      "Epoch:    8/50    Loss: 3.793537457942963\n",
      "\n",
      "Epoch:    8/50    Loss: 3.7858131318092347\n",
      "\n",
      "Epoch:    8/50    Loss: 3.8289085817337036\n",
      "\n",
      "Epoch:    8/50    Loss: 3.855728316783905\n",
      "\n",
      "Epoch:    8/50    Loss: 3.7736763038635255\n",
      "\n",
      "Epoch:    8/50    Loss: 3.8548600990772246\n",
      "\n",
      "Epoch:    8/50    Loss: 3.79545294713974\n",
      "\n",
      "Epoch:    8/50    Loss: 3.7780129642486573\n",
      "\n",
      "Epoch:    8/50    Loss: 3.7615070123672485\n",
      "\n",
      "Epoch:    8/50    Loss: 3.8329362125396726\n",
      "\n",
      "Epoch:    8/50    Loss: 3.822795682430267\n",
      "\n",
      "Epoch:    8/50    Loss: 3.8082135300636293\n",
      "\n",
      "Epoch:    8/50    Loss: 3.8687147092819214\n",
      "\n",
      "Epoch:    8/50    Loss: 3.84617164516449\n",
      "\n",
      "Epoch:    8/50    Loss: 3.8645497522354124\n",
      "\n",
      "Epoch:    8/50    Loss: 3.7836224250793458\n",
      "\n",
      "Epoch:    8/50    Loss: 3.8360125098228455\n",
      "\n",
      "Epoch:    8/50    Loss: 3.816914005756378\n",
      "\n",
      "Epoch:    8/50    Loss: 3.897944562911987\n",
      "\n",
      "Epoch:    8/50    Loss: 3.8611731157302858\n",
      "\n",
      "Epoch:    8/50    Loss: 3.854099996089935\n",
      "\n",
      "Epoch:    8/50    Loss: 3.8136418948173523\n",
      "\n",
      "Epoch:    8/50    Loss: 3.8736067056655883\n",
      "\n",
      "Epoch:    8/50    Loss: 3.8461823592185973\n",
      "\n",
      "Epoch:    8/50    Loss: 3.869915786743164\n",
      "\n",
      "Epoch:    8/50    Loss: 3.9101866846084596\n",
      "\n",
      "Epoch:    8/50    Loss: 3.9102525424957277\n",
      "\n",
      "Epoch:    8/50    Loss: 3.840132058143616\n",
      "\n",
      "Epoch:    8/50    Loss: 3.8802171869277955\n",
      "\n",
      "Epoch:    8/50    Loss: 3.8673176255226136\n",
      "\n",
      "Epoch:    8/50    Loss: 3.847337408065796\n",
      "\n",
      "Epoch:    8/50    Loss: 3.8484956364631655\n",
      "\n",
      "Epoch:    8/50    Loss: 3.8827643661499023\n",
      "\n",
      "Epoch:    8/50    Loss: 3.9312905807495118\n",
      "\n",
      "Epoch:    8/50    Loss: 3.8780599455833435\n",
      "\n",
      "Epoch:    8/50    Loss: 3.886399277687073\n",
      "\n",
      "Epoch:    8/50    Loss: 3.8990415744781495\n",
      "\n",
      "Epoch:    8/50    Loss: 3.8582833132743835\n",
      "\n",
      "Epoch:    8/50    Loss: 3.8720747954845427\n",
      "\n",
      "Epoch:    8/50    Loss: 3.9181816787719725\n",
      "\n",
      "Epoch:    8/50    Loss: 3.930283309459686\n",
      "\n",
      "Epoch:    8/50    Loss: 3.8709535162448883\n",
      "\n",
      "Epoch:    8/50    Loss: 3.908662501335144\n",
      "\n",
      "Epoch:    8/50    Loss: 3.921328890800476\n",
      "\n",
      "Epoch:    8/50    Loss: 3.9709386324882505\n",
      "\n",
      "Epoch:    8/50    Loss: 3.9494469623565673\n",
      "\n",
      "Epoch:    8/50    Loss: 3.922590834140778\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    8/50    Loss: 3.9254247436523437\n",
      "\n",
      "Epoch:    8/50    Loss: 3.907474632740021\n",
      "\n",
      "Epoch:    8/50    Loss: 3.866246123790741\n",
      "\n",
      "Epoch:    8/50    Loss: 3.8731051626205444\n",
      "\n",
      "Epoch:    8/50    Loss: 3.919538911819458\n",
      "\n",
      "Epoch:    8/50    Loss: 3.9484634146690367\n",
      "\n",
      "Epoch:    9/50    Loss: 3.836232755690405\n",
      "\n",
      "Epoch:    9/50    Loss: 3.744816421508789\n",
      "\n",
      "Epoch:    9/50    Loss: 3.728318878173828\n",
      "\n",
      "Epoch:    9/50    Loss: 3.818788923740387\n",
      "\n",
      "Epoch:    9/50    Loss: 3.737968522071838\n",
      "\n",
      "Epoch:    9/50    Loss: 3.7978273563385008\n",
      "\n",
      "Epoch:    9/50    Loss: 3.777088213443756\n",
      "\n",
      "Epoch:    9/50    Loss: 3.747180685997009\n",
      "\n",
      "Epoch:    9/50    Loss: 3.7775460605621336\n",
      "\n",
      "Epoch:    9/50    Loss: 3.7728996839523314\n",
      "\n",
      "Epoch:    9/50    Loss: 3.7905140357017517\n",
      "\n",
      "Epoch:    9/50    Loss: 3.785618449687958\n",
      "\n",
      "Epoch:    9/50    Loss: 3.8062315735816954\n",
      "\n",
      "Epoch:    9/50    Loss: 3.8129915337562563\n",
      "\n",
      "Epoch:    9/50    Loss: 3.795929003238678\n",
      "\n",
      "Epoch:    9/50    Loss: 3.8037237496376037\n",
      "\n",
      "Epoch:    9/50    Loss: 3.7998047499656677\n",
      "\n",
      "Epoch:    9/50    Loss: 3.8057963962554933\n",
      "\n",
      "Epoch:    9/50    Loss: 3.8130289783477784\n",
      "\n",
      "Epoch:    9/50    Loss: 3.842584891319275\n",
      "\n",
      "Epoch:    9/50    Loss: 3.8119787406921386\n",
      "\n",
      "Epoch:    9/50    Loss: 3.8436934146881105\n",
      "\n",
      "Epoch:    9/50    Loss: 3.787684051513672\n",
      "\n",
      "Epoch:    9/50    Loss: 3.8526102871894836\n",
      "\n",
      "Epoch:    9/50    Loss: 3.8211222705841066\n",
      "\n",
      "Epoch:    9/50    Loss: 3.84108216714859\n",
      "\n",
      "Epoch:    9/50    Loss: 3.8155759377479552\n",
      "\n",
      "Epoch:    9/50    Loss: 3.881145179271698\n",
      "\n",
      "Epoch:    9/50    Loss: 3.8505912246704104\n",
      "\n",
      "Epoch:    9/50    Loss: 3.847024075984955\n",
      "\n",
      "Epoch:    9/50    Loss: 3.8856004815101626\n",
      "\n",
      "Epoch:    9/50    Loss: 3.8911501231193544\n",
      "\n",
      "Epoch:    9/50    Loss: 3.849334844827652\n",
      "\n",
      "Epoch:    9/50    Loss: 3.9431792545318602\n",
      "\n",
      "Epoch:    9/50    Loss: 3.8428530516624453\n",
      "\n",
      "Epoch:    9/50    Loss: 3.8632857990264893\n",
      "\n",
      "Epoch:    9/50    Loss: 3.894140859603882\n",
      "\n",
      "Epoch:    9/50    Loss: 3.8521303052902223\n",
      "\n",
      "Epoch:    9/50    Loss: 3.8586601099967957\n",
      "\n",
      "Epoch:    9/50    Loss: 3.887768159866333\n",
      "\n",
      "Epoch:    9/50    Loss: 3.8640243310928346\n",
      "\n",
      "Epoch:    9/50    Loss: 3.8377910089492797\n",
      "\n",
      "Epoch:    9/50    Loss: 3.8818733630180358\n",
      "\n",
      "Epoch:    9/50    Loss: 3.903629460811615\n",
      "\n",
      "Epoch:    9/50    Loss: 3.948372465133667\n",
      "\n",
      "Epoch:    9/50    Loss: 3.892216239929199\n",
      "\n",
      "Epoch:    9/50    Loss: 3.893476598262787\n",
      "\n",
      "Epoch:    9/50    Loss: 3.905189351081848\n",
      "\n",
      "Epoch:    9/50    Loss: 3.9156674213409426\n",
      "\n",
      "Epoch:    9/50    Loss: 3.886239511489868\n",
      "\n",
      "Epoch:    9/50    Loss: 3.8334880957603454\n",
      "\n",
      "Epoch:    9/50    Loss: 3.89947310590744\n",
      "\n",
      "Epoch:    9/50    Loss: 3.8885143265724182\n",
      "\n",
      "Epoch:    9/50    Loss: 3.89599413728714\n",
      "\n",
      "Epoch:    9/50    Loss: 3.8743563442230227\n",
      "\n",
      "Epoch:   10/50    Loss: 3.8069389918351284\n",
      "\n",
      "Epoch:   10/50    Loss: 3.763038177013397\n",
      "\n",
      "Epoch:   10/50    Loss: 3.7158889532089234\n",
      "\n",
      "Epoch:   10/50    Loss: 3.695960412979126\n",
      "\n",
      "Epoch:   10/50    Loss: 3.7522970163822174\n",
      "\n",
      "Epoch:   10/50    Loss: 3.787948715209961\n",
      "\n",
      "Epoch:   10/50    Loss: 3.7456943526268005\n",
      "\n",
      "Epoch:   10/50    Loss: 3.7490111618041992\n",
      "\n",
      "Epoch:   10/50    Loss: 3.7077200841903686\n",
      "\n",
      "Epoch:   10/50    Loss: 3.7399567992687226\n",
      "\n",
      "Epoch:   10/50    Loss: 3.759074768066406\n",
      "\n",
      "Epoch:   10/50    Loss: 3.736024909734726\n",
      "\n",
      "Epoch:   10/50    Loss: 3.8527463603019716\n",
      "\n",
      "Epoch:   10/50    Loss: 3.813627130031586\n",
      "\n",
      "Epoch:   10/50    Loss: 3.849855152130127\n",
      "\n",
      "Epoch:   10/50    Loss: 3.8450037689208982\n",
      "\n",
      "Epoch:   10/50    Loss: 3.838103401184082\n",
      "\n",
      "Epoch:   10/50    Loss: 3.770872028827667\n",
      "\n",
      "Epoch:   10/50    Loss: 3.8473249378204346\n",
      "\n",
      "Epoch:   10/50    Loss: 3.8248939628601075\n",
      "\n",
      "Epoch:   10/50    Loss: 3.801792450428009\n",
      "\n",
      "Epoch:   10/50    Loss: 3.8553491816520693\n",
      "\n",
      "Epoch:   10/50    Loss: 3.769097562789917\n",
      "\n",
      "Epoch:   10/50    Loss: 3.8271759905815124\n",
      "\n",
      "Epoch:   10/50    Loss: 3.827592092514038\n",
      "\n",
      "Epoch:   10/50    Loss: 3.7892992124557496\n",
      "\n",
      "Epoch:   10/50    Loss: 3.8032768270969393\n",
      "\n",
      "Epoch:   10/50    Loss: 3.778453511714935\n",
      "\n",
      "Epoch:   10/50    Loss: 3.8320931735038757\n",
      "\n",
      "Epoch:   10/50    Loss: 3.8812627725601194\n",
      "\n",
      "Epoch:   10/50    Loss: 3.754089383363724\n",
      "\n",
      "Epoch:   10/50    Loss: 3.8478367528915407\n",
      "\n",
      "Epoch:   10/50    Loss: 3.863070024013519\n",
      "\n",
      "Epoch:   10/50    Loss: 3.853789915084839\n",
      "\n",
      "Epoch:   10/50    Loss: 3.829266956806183\n",
      "\n",
      "Epoch:   10/50    Loss: 3.83525710105896\n",
      "\n",
      "Epoch:   10/50    Loss: 3.866515974521637\n",
      "\n",
      "Epoch:   10/50    Loss: 3.8194483389854432\n",
      "\n",
      "Epoch:   10/50    Loss: 3.8486988878250123\n",
      "\n",
      "Epoch:   10/50    Loss: 3.907272479534149\n",
      "\n",
      "Epoch:   10/50    Loss: 3.855313735485077\n",
      "\n",
      "Epoch:   10/50    Loss: 3.8669648690223695\n",
      "\n",
      "Epoch:   10/50    Loss: 3.8615177669525145\n",
      "\n",
      "Epoch:   10/50    Loss: 3.9043233642578126\n",
      "\n",
      "Epoch:   10/50    Loss: 3.8472857999801637\n",
      "\n",
      "Epoch:   10/50    Loss: 3.8148629875183104\n",
      "\n",
      "Epoch:   10/50    Loss: 3.8359077219963074\n",
      "\n",
      "Epoch:   10/50    Loss: 3.8727366614341734\n",
      "\n",
      "Epoch:   10/50    Loss: 3.8663521485328673\n",
      "\n",
      "Epoch:   10/50    Loss: 3.878087520122528\n",
      "\n",
      "Epoch:   10/50    Loss: 3.838806969165802\n",
      "\n",
      "Epoch:   10/50    Loss: 3.940944667816162\n",
      "\n",
      "Epoch:   10/50    Loss: 3.8802659602165224\n",
      "\n",
      "Epoch:   10/50    Loss: 3.8660719654560087\n",
      "\n",
      "Epoch:   10/50    Loss: 3.908710287094116\n",
      "\n",
      "Epoch:   11/50    Loss: 3.8023195982523705\n",
      "\n",
      "Epoch:   11/50    Loss: 3.701143325805664\n",
      "\n",
      "Epoch:   11/50    Loss: 3.689992220878601\n",
      "\n",
      "Epoch:   11/50    Loss: 3.695530123710632\n",
      "\n",
      "Epoch:   11/50    Loss: 3.712446445465088\n",
      "\n",
      "Epoch:   11/50    Loss: 3.7831936836242677\n",
      "\n",
      "Epoch:   11/50    Loss: 3.680327496290207\n",
      "\n",
      "Epoch:   11/50    Loss: 3.731705915927887\n",
      "\n",
      "Epoch:   11/50    Loss: 3.7734532833099363\n",
      "\n",
      "Epoch:   11/50    Loss: 3.8278282356262205\n",
      "\n",
      "Epoch:   11/50    Loss: 3.7856450514793396\n",
      "\n",
      "Epoch:   11/50    Loss: 3.719926121711731\n",
      "\n",
      "Epoch:   11/50    Loss: 3.7675157403945922\n",
      "\n",
      "Epoch:   11/50    Loss: 3.7387983927726745\n",
      "\n",
      "Epoch:   11/50    Loss: 3.7675951189994814\n",
      "\n",
      "Epoch:   11/50    Loss: 3.8290403933525083\n",
      "\n",
      "Epoch:   11/50    Loss: 3.8013447151184083\n",
      "\n",
      "Epoch:   11/50    Loss: 3.7641609380245207\n",
      "\n",
      "Epoch:   11/50    Loss: 3.7695833311080933\n",
      "\n",
      "Epoch:   11/50    Loss: 3.7536088767051696\n",
      "\n",
      "Epoch:   11/50    Loss: 3.8073495030403137\n",
      "\n",
      "Epoch:   11/50    Loss: 3.8497943091392517\n",
      "\n",
      "Epoch:   11/50    Loss: 3.8089535837173463\n",
      "\n",
      "Epoch:   11/50    Loss: 3.7300387387275697\n",
      "\n",
      "Epoch:   11/50    Loss: 3.7749219284057616\n",
      "\n",
      "Epoch:   11/50    Loss: 3.8180604162216185\n",
      "\n",
      "Epoch:   11/50    Loss: 3.8491484704017638\n",
      "\n",
      "Epoch:   11/50    Loss: 3.846570679664612\n",
      "\n",
      "Epoch:   11/50    Loss: 3.840793613433838\n",
      "\n",
      "Epoch:   11/50    Loss: 3.8600402204990387\n",
      "\n",
      "Epoch:   11/50    Loss: 3.880864879608154\n",
      "\n",
      "Epoch:   11/50    Loss: 3.865053699016571\n",
      "\n",
      "Epoch:   11/50    Loss: 3.867129675626755\n",
      "\n",
      "Epoch:   11/50    Loss: 3.910389717102051\n",
      "\n",
      "Epoch:   11/50    Loss: 3.759338140964508\n",
      "\n",
      "Epoch:   11/50    Loss: 3.791974533557892\n",
      "\n",
      "Epoch:   11/50    Loss: 3.82520890378952\n",
      "\n",
      "Epoch:   11/50    Loss: 3.838581118106842\n",
      "\n",
      "Epoch:   11/50    Loss: 3.8291501641273498\n",
      "\n",
      "Epoch:   11/50    Loss: 3.8562724208831787\n",
      "\n",
      "Epoch:   11/50    Loss: 3.8777278232574464\n",
      "\n",
      "Epoch:   11/50    Loss: 3.8840373854637145\n",
      "\n",
      "Epoch:   11/50    Loss: 3.8688985300064087\n",
      "\n",
      "Epoch:   11/50    Loss: 3.801881817817688\n",
      "\n",
      "Epoch:   11/50    Loss: 3.8211265621185304\n",
      "\n",
      "Epoch:   11/50    Loss: 3.8343842287063596\n",
      "\n",
      "Epoch:   11/50    Loss: 3.818020473957062\n",
      "\n",
      "Epoch:   11/50    Loss: 3.8323145763874056\n",
      "\n",
      "Epoch:   11/50    Loss: 3.858722961902618\n",
      "\n",
      "Epoch:   11/50    Loss: 3.8256692507267\n",
      "\n",
      "Epoch:   11/50    Loss: 3.8377104210853576\n",
      "\n",
      "Epoch:   11/50    Loss: 3.836238188266754\n",
      "\n",
      "Epoch:   11/50    Loss: 3.8449273262023924\n",
      "\n",
      "Epoch:   11/50    Loss: 3.842348883152008\n",
      "\n",
      "Epoch:   11/50    Loss: 3.9240153975486756\n",
      "\n",
      "Epoch:   12/50    Loss: 3.7802877638437975\n",
      "\n",
      "Epoch:   12/50    Loss: 3.723679886341095\n",
      "\n",
      "Epoch:   12/50    Loss: 3.7311968154907227\n",
      "\n",
      "Epoch:   12/50    Loss: 3.6566617863178252\n",
      "\n",
      "Epoch:   12/50    Loss: 3.746131709575653\n",
      "\n",
      "Epoch:   12/50    Loss: 3.7832236733436586\n",
      "\n",
      "Epoch:   12/50    Loss: 3.7103730363845826\n",
      "\n",
      "Epoch:   12/50    Loss: 3.7283757395744326\n",
      "\n",
      "Epoch:   12/50    Loss: 3.7687816104888916\n",
      "\n",
      "Epoch:   12/50    Loss: 3.764242904186249\n",
      "\n",
      "Epoch:   12/50    Loss: 3.7423725123405456\n",
      "\n",
      "Epoch:   12/50    Loss: 3.7493441290855407\n",
      "\n",
      "Epoch:   12/50    Loss: 3.715946069717407\n",
      "\n",
      "Epoch:   12/50    Loss: 3.7375737462043763\n",
      "\n",
      "Epoch:   12/50    Loss: 3.7856849570274353\n",
      "\n",
      "Epoch:   12/50    Loss: 3.715234864473343\n",
      "\n",
      "Epoch:   12/50    Loss: 3.736017351388931\n",
      "\n",
      "Epoch:   12/50    Loss: 3.74936740064621\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   12/50    Loss: 3.814783143043518\n",
      "\n",
      "Epoch:   12/50    Loss: 3.8080726432800294\n",
      "\n",
      "Epoch:   12/50    Loss: 3.837598303318024\n",
      "\n",
      "Epoch:   12/50    Loss: 3.810673879861832\n",
      "\n",
      "Epoch:   12/50    Loss: 3.7653287925720216\n",
      "\n",
      "Epoch:   12/50    Loss: 3.7858401494026186\n",
      "\n",
      "Epoch:   12/50    Loss: 3.7806829240322113\n",
      "\n",
      "Epoch:   12/50    Loss: 3.7457386293411257\n",
      "\n",
      "Epoch:   12/50    Loss: 3.7695328578948977\n",
      "\n",
      "Epoch:   12/50    Loss: 3.831898524045944\n",
      "\n",
      "Epoch:   12/50    Loss: 3.794652039051056\n",
      "\n",
      "Epoch:   12/50    Loss: 3.776790024280548\n",
      "\n",
      "Epoch:   12/50    Loss: 3.7249666576385496\n",
      "\n",
      "Epoch:   12/50    Loss: 3.7905641651153563\n",
      "\n",
      "Epoch:   12/50    Loss: 3.7869106554985046\n",
      "\n",
      "Epoch:   12/50    Loss: 3.7979332304000852\n",
      "\n",
      "Epoch:   12/50    Loss: 3.8723476190567014\n",
      "\n",
      "Epoch:   12/50    Loss: 3.7547103571891784\n",
      "\n",
      "Epoch:   12/50    Loss: 3.809424132823944\n",
      "\n",
      "Epoch:   12/50    Loss: 3.8462761182785035\n",
      "\n",
      "Epoch:   12/50    Loss: 3.8223204097747803\n",
      "\n",
      "Epoch:   12/50    Loss: 3.7457438220977783\n",
      "\n",
      "Epoch:   12/50    Loss: 3.8481059927940366\n",
      "\n",
      "Epoch:   12/50    Loss: 3.801568515777588\n",
      "\n",
      "Epoch:   12/50    Loss: 3.876044332027435\n",
      "\n",
      "Epoch:   12/50    Loss: 3.860679986476898\n",
      "\n",
      "Epoch:   12/50    Loss: 3.8141100823879244\n",
      "\n",
      "Epoch:   12/50    Loss: 3.8158201031684875\n",
      "\n",
      "Epoch:   12/50    Loss: 3.8707888932228087\n",
      "\n",
      "Epoch:   12/50    Loss: 3.879358169078827\n",
      "\n",
      "Epoch:   12/50    Loss: 3.8179426288604734\n",
      "\n",
      "Epoch:   12/50    Loss: 3.845846671104431\n",
      "\n",
      "Epoch:   12/50    Loss: 3.8180760154724123\n",
      "\n",
      "Epoch:   12/50    Loss: 3.8886458320617674\n",
      "\n",
      "Epoch:   12/50    Loss: 3.788012899637222\n",
      "\n",
      "Epoch:   12/50    Loss: 3.926920299053192\n",
      "\n",
      "Epoch:   12/50    Loss: 3.8766198511123657\n",
      "\n",
      "Epoch:   13/50    Loss: 3.750532277642864\n",
      "\n",
      "Epoch:   13/50    Loss: 3.6451173479557037\n",
      "\n",
      "Epoch:   13/50    Loss: 3.6791082930564882\n",
      "\n",
      "Epoch:   13/50    Loss: 3.746315260410309\n",
      "\n",
      "Epoch:   13/50    Loss: 3.6802765300273896\n",
      "\n",
      "Epoch:   13/50    Loss: 3.7083770446777344\n",
      "\n",
      "Epoch:   13/50    Loss: 3.7371371545791625\n",
      "\n",
      "Epoch:   13/50    Loss: 3.692749668598175\n",
      "\n",
      "Epoch:   13/50    Loss: 3.7410147013664248\n",
      "\n",
      "Epoch:   13/50    Loss: 3.6668916244506837\n",
      "\n",
      "Epoch:   13/50    Loss: 3.739258605957031\n",
      "\n",
      "Epoch:   13/50    Loss: 3.7494277987480165\n",
      "\n",
      "Epoch:   13/50    Loss: 3.702701564788818\n",
      "\n",
      "Epoch:   13/50    Loss: 3.680077688217163\n",
      "\n",
      "Epoch:   13/50    Loss: 3.765257878780365\n",
      "\n",
      "Epoch:   13/50    Loss: 3.750110565662384\n",
      "\n",
      "Epoch:   13/50    Loss: 3.742616870164871\n",
      "\n",
      "Epoch:   13/50    Loss: 3.743918702125549\n",
      "\n",
      "Epoch:   13/50    Loss: 3.7501503398418428\n",
      "\n",
      "Epoch:   13/50    Loss: 3.8137060742378237\n",
      "\n",
      "Epoch:   13/50    Loss: 3.7573226261138917\n",
      "\n",
      "Epoch:   13/50    Loss: 3.774525897026062\n",
      "\n",
      "Epoch:   13/50    Loss: 3.7891117606163025\n",
      "\n",
      "Epoch:   13/50    Loss: 3.8010486407279966\n",
      "\n",
      "Epoch:   13/50    Loss: 3.7530162930488586\n",
      "\n",
      "Epoch:   13/50    Loss: 3.7857487955093383\n",
      "\n",
      "Epoch:   13/50    Loss: 3.760384739637375\n",
      "\n",
      "Epoch:   13/50    Loss: 3.801633801460266\n",
      "\n",
      "Epoch:   13/50    Loss: 3.813210368156433\n",
      "\n",
      "Epoch:   13/50    Loss: 3.797058307170868\n",
      "\n",
      "Epoch:   13/50    Loss: 3.8485238938331605\n",
      "\n",
      "Epoch:   13/50    Loss: 3.752528341293335\n",
      "\n",
      "Epoch:   13/50    Loss: 3.7600969014167784\n",
      "\n",
      "Epoch:   13/50    Loss: 3.806409752368927\n",
      "\n",
      "Epoch:   13/50    Loss: 3.852965067386627\n",
      "\n",
      "Epoch:   13/50    Loss: 3.840600666999817\n",
      "\n",
      "Epoch:   13/50    Loss: 3.809458866119385\n",
      "\n",
      "Epoch:   13/50    Loss: 3.7877461338043212\n",
      "\n",
      "Epoch:   13/50    Loss: 3.821640921115875\n",
      "\n",
      "Epoch:   13/50    Loss: 3.8182757472991944\n",
      "\n",
      "Epoch:   13/50    Loss: 3.7680846824645995\n",
      "\n",
      "Epoch:   13/50    Loss: 3.8472583651542664\n",
      "\n",
      "Epoch:   13/50    Loss: 3.792660106420517\n",
      "\n",
      "Epoch:   13/50    Loss: 3.8288929738998414\n",
      "\n",
      "Epoch:   13/50    Loss: 3.799029025554657\n",
      "\n",
      "Epoch:   13/50    Loss: 3.829295819759369\n",
      "\n",
      "Epoch:   13/50    Loss: 3.8147692565917968\n",
      "\n",
      "Epoch:   13/50    Loss: 3.8569528017044066\n",
      "\n",
      "Epoch:   13/50    Loss: 3.8419149849414826\n",
      "\n",
      "Epoch:   13/50    Loss: 3.8495496063232424\n",
      "\n",
      "Epoch:   13/50    Loss: 3.861506552219391\n",
      "\n",
      "Epoch:   13/50    Loss: 3.828385223865509\n",
      "\n",
      "Epoch:   13/50    Loss: 3.842016439437866\n",
      "\n",
      "Epoch:   13/50    Loss: 3.8879045901298523\n",
      "\n",
      "Epoch:   13/50    Loss: 3.838100434303284\n",
      "\n",
      "Epoch:   14/50    Loss: 3.735875003414067\n",
      "\n",
      "Epoch:   14/50    Loss: 3.6460236411094664\n",
      "\n",
      "Epoch:   14/50    Loss: 3.63890265083313\n",
      "\n",
      "Epoch:   14/50    Loss: 3.7154160470962525\n",
      "\n",
      "Epoch:   14/50    Loss: 3.6530505270957945\n",
      "\n",
      "Epoch:   14/50    Loss: 3.6698445262908934\n",
      "\n",
      "Epoch:   14/50    Loss: 3.7180733289718626\n",
      "\n",
      "Epoch:   14/50    Loss: 3.7315134124755858\n",
      "\n",
      "Epoch:   14/50    Loss: 3.7138264136314394\n",
      "\n",
      "Epoch:   14/50    Loss: 3.722693202495575\n",
      "\n",
      "Epoch:   14/50    Loss: 3.7009198174476623\n",
      "\n",
      "Epoch:   14/50    Loss: 3.6973877935409547\n",
      "\n",
      "Epoch:   14/50    Loss: 3.7224166977405546\n",
      "\n",
      "Epoch:   14/50    Loss: 3.752049844264984\n",
      "\n",
      "Epoch:   14/50    Loss: 3.717792264699936\n",
      "\n",
      "Epoch:   14/50    Loss: 3.760465728759766\n",
      "\n",
      "Epoch:   14/50    Loss: 3.7598190903663635\n",
      "\n",
      "Epoch:   14/50    Loss: 3.7187590103149413\n",
      "\n",
      "Epoch:   14/50    Loss: 3.7438199441432953\n",
      "\n",
      "Epoch:   14/50    Loss: 3.73835525894165\n",
      "\n",
      "Epoch:   14/50    Loss: 3.7689162378311156\n",
      "\n",
      "Epoch:   14/50    Loss: 3.7645474705696107\n",
      "\n",
      "Epoch:   14/50    Loss: 3.813329288482666\n",
      "\n",
      "Epoch:   14/50    Loss: 3.7951168065071106\n",
      "\n",
      "Epoch:   14/50    Loss: 3.7499512486457824\n",
      "\n",
      "Epoch:   14/50    Loss: 3.7497845945358277\n",
      "\n",
      "Epoch:   14/50    Loss: 3.776300859451294\n",
      "\n",
      "Epoch:   14/50    Loss: 3.813822189331055\n",
      "\n",
      "Epoch:   14/50    Loss: 3.8132064232826233\n",
      "\n",
      "Epoch:   14/50    Loss: 3.8213587460517884\n",
      "\n",
      "Epoch:   14/50    Loss: 3.80905832862854\n",
      "\n",
      "Epoch:   14/50    Loss: 3.789675382375717\n",
      "\n",
      "Epoch:   14/50    Loss: 3.7805618133544923\n",
      "\n",
      "Epoch:   14/50    Loss: 3.8400472936630248\n",
      "\n",
      "Epoch:   14/50    Loss: 3.7394547204971316\n",
      "\n",
      "Epoch:   14/50    Loss: 3.809875711917877\n",
      "\n",
      "Epoch:   14/50    Loss: 3.8302642483711242\n",
      "\n",
      "Epoch:   14/50    Loss: 3.785455060482025\n",
      "\n",
      "Epoch:   14/50    Loss: 3.783892889022827\n",
      "\n",
      "Epoch:   14/50    Loss: 3.7414140276908876\n",
      "\n",
      "Epoch:   14/50    Loss: 3.8448580181598664\n",
      "\n",
      "Epoch:   14/50    Loss: 3.8056885795593263\n",
      "\n",
      "Epoch:   14/50    Loss: 3.8158186616897583\n",
      "\n",
      "Epoch:   14/50    Loss: 3.8081646642684936\n",
      "\n",
      "Epoch:   14/50    Loss: 3.8348738837242125\n",
      "\n",
      "Epoch:   14/50    Loss: 3.826937207221985\n",
      "\n",
      "Epoch:   14/50    Loss: 3.776022340297699\n",
      "\n",
      "Epoch:   14/50    Loss: 3.8601610741615295\n",
      "\n",
      "Epoch:   14/50    Loss: 3.792957321166992\n",
      "\n",
      "Epoch:   14/50    Loss: 3.7779922046661376\n",
      "\n",
      "Epoch:   14/50    Loss: 3.8340825209617613\n",
      "\n",
      "Epoch:   14/50    Loss: 3.8030439319610596\n",
      "\n",
      "Epoch:   14/50    Loss: 3.775484872817993\n",
      "\n",
      "Epoch:   14/50    Loss: 3.8710734615325926\n",
      "\n",
      "Epoch:   14/50    Loss: 3.827314003944397\n",
      "\n",
      "Epoch:   15/50    Loss: 3.741863411311145\n",
      "\n",
      "Epoch:   15/50    Loss: 3.65844909620285\n",
      "\n",
      "Epoch:   15/50    Loss: 3.6799949622154235\n",
      "\n",
      "Epoch:   15/50    Loss: 3.6414608178138734\n",
      "\n",
      "Epoch:   15/50    Loss: 3.723711864709854\n",
      "\n",
      "Epoch:   15/50    Loss: 3.7551554050445555\n",
      "\n",
      "Epoch:   15/50    Loss: 3.6909240312576292\n",
      "\n",
      "Epoch:   15/50    Loss: 3.6690400738716127\n",
      "\n",
      "Epoch:   15/50    Loss: 3.7505314559936522\n",
      "\n",
      "Epoch:   15/50    Loss: 3.7000297269821165\n",
      "\n",
      "Epoch:   15/50    Loss: 3.6856174795627594\n",
      "\n",
      "Epoch:   15/50    Loss: 3.7259836175441743\n",
      "\n",
      "Epoch:   15/50    Loss: 3.7334083552360533\n",
      "\n",
      "Epoch:   15/50    Loss: 3.7238104858398438\n",
      "\n",
      "Epoch:   15/50    Loss: 3.675983638763428\n",
      "\n",
      "Epoch:   15/50    Loss: 3.724691877365112\n",
      "\n",
      "Epoch:   15/50    Loss: 3.7876406874656676\n",
      "\n",
      "Epoch:   15/50    Loss: 3.7754472966194155\n",
      "\n",
      "Epoch:   15/50    Loss: 3.7622530398368834\n",
      "\n",
      "Epoch:   15/50    Loss: 3.7232196006774902\n",
      "\n",
      "Epoch:   15/50    Loss: 3.778601619243622\n",
      "\n",
      "Epoch:   15/50    Loss: 3.6775119013786317\n",
      "\n",
      "Epoch:   15/50    Loss: 3.7507893671989443\n",
      "\n",
      "Epoch:   15/50    Loss: 3.7547414541244506\n",
      "\n",
      "Epoch:   15/50    Loss: 3.729914406776428\n",
      "\n",
      "Epoch:   15/50    Loss: 3.798032483577728\n",
      "\n",
      "Epoch:   15/50    Loss: 3.7343609170913696\n",
      "\n",
      "Epoch:   15/50    Loss: 3.8437848057746886\n",
      "\n",
      "Epoch:   15/50    Loss: 3.7463205037117002\n",
      "\n",
      "Epoch:   15/50    Loss: 3.7428653979301454\n",
      "\n",
      "Epoch:   15/50    Loss: 3.7925634410381317\n",
      "\n",
      "Epoch:   15/50    Loss: 3.7843356146812437\n",
      "\n",
      "Epoch:   15/50    Loss: 3.791919008731842\n",
      "\n",
      "Epoch:   15/50    Loss: 3.73991135263443\n",
      "\n",
      "Epoch:   15/50    Loss: 3.8004104914665224\n",
      "\n",
      "Epoch:   15/50    Loss: 3.73225884437561\n",
      "\n",
      "Epoch:   15/50    Loss: 3.7520017280578615\n",
      "\n",
      "Epoch:   15/50    Loss: 3.7214020442962648\n",
      "\n",
      "Epoch:   15/50    Loss: 3.78645357632637\n",
      "\n",
      "Epoch:   15/50    Loss: 3.793211701154709\n",
      "\n",
      "Epoch:   15/50    Loss: 3.7890307829380037\n",
      "\n",
      "Epoch:   15/50    Loss: 3.7778976740837096\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   15/50    Loss: 3.7986195859909055\n",
      "\n",
      "Epoch:   15/50    Loss: 3.760214962720871\n",
      "\n",
      "Epoch:   15/50    Loss: 3.83793346619606\n",
      "\n",
      "Epoch:   15/50    Loss: 3.808772473335266\n",
      "\n",
      "Epoch:   15/50    Loss: 3.8355291838645935\n",
      "\n",
      "Epoch:   15/50    Loss: 3.8198231654167176\n",
      "\n",
      "Epoch:   15/50    Loss: 3.8233414912223815\n",
      "\n",
      "Epoch:   15/50    Loss: 3.7971067407131196\n",
      "\n",
      "Epoch:   15/50    Loss: 3.8118988370895384\n",
      "\n",
      "Epoch:   15/50    Loss: 3.800294083595276\n",
      "\n",
      "Epoch:   15/50    Loss: 3.8272801542282107\n",
      "\n",
      "Epoch:   15/50    Loss: 3.7992992432117463\n",
      "\n",
      "Epoch:   15/50    Loss: 3.8432971494197847\n",
      "\n",
      "Epoch:   16/50    Loss: 3.734625144091915\n",
      "\n",
      "Epoch:   16/50    Loss: 3.6752483532428744\n",
      "\n",
      "Epoch:   16/50    Loss: 3.656185678958893\n",
      "\n",
      "Epoch:   16/50    Loss: 3.6443500452041624\n",
      "\n",
      "Epoch:   16/50    Loss: 3.6917846496105193\n",
      "\n",
      "Epoch:   16/50    Loss: 3.648266029596329\n",
      "\n",
      "Epoch:   16/50    Loss: 3.7008004505634307\n",
      "\n",
      "Epoch:   16/50    Loss: 3.7402029604911804\n",
      "\n",
      "Epoch:   16/50    Loss: 3.680185530662537\n",
      "\n",
      "Epoch:   16/50    Loss: 3.722255536556244\n",
      "\n",
      "Epoch:   16/50    Loss: 3.720042459011078\n",
      "\n",
      "Epoch:   16/50    Loss: 3.709480583667755\n",
      "\n",
      "Epoch:   16/50    Loss: 3.689775728702545\n",
      "\n",
      "Epoch:   16/50    Loss: 3.677689858675003\n",
      "\n",
      "Epoch:   16/50    Loss: 3.7719334738254546\n",
      "\n",
      "Epoch:   16/50    Loss: 3.772509235858917\n",
      "\n",
      "Epoch:   16/50    Loss: 3.754264173269272\n",
      "\n",
      "Epoch:   16/50    Loss: 3.661254144668579\n",
      "\n",
      "Epoch:   16/50    Loss: 3.7322890095710752\n",
      "\n",
      "Epoch:   16/50    Loss: 3.7868094058036803\n",
      "\n",
      "Epoch:   16/50    Loss: 3.7452509870529176\n",
      "\n",
      "Epoch:   16/50    Loss: 3.798064922809601\n",
      "\n",
      "Epoch:   16/50    Loss: 3.722793538093567\n",
      "\n",
      "Epoch:   16/50    Loss: 3.7143326053619385\n",
      "\n",
      "Epoch:   16/50    Loss: 3.7550767431259153\n",
      "\n",
      "Epoch:   16/50    Loss: 3.769092761039734\n",
      "\n",
      "Epoch:   16/50    Loss: 3.76574674654007\n",
      "\n",
      "Epoch:   16/50    Loss: 3.7364023389816285\n",
      "\n",
      "Epoch:   16/50    Loss: 3.758667489528656\n",
      "\n",
      "Epoch:   16/50    Loss: 3.7776397824287415\n",
      "\n",
      "Epoch:   16/50    Loss: 3.775999177455902\n",
      "\n",
      "Epoch:   16/50    Loss: 3.7249379448890685\n",
      "\n",
      "Epoch:   16/50    Loss: 3.7648087677955626\n",
      "\n",
      "Epoch:   16/50    Loss: 3.79132772064209\n",
      "\n",
      "Epoch:   16/50    Loss: 3.672877823829651\n",
      "\n",
      "Epoch:   16/50    Loss: 3.8394569597244264\n",
      "\n",
      "Epoch:   16/50    Loss: 3.8278985114097597\n",
      "\n",
      "Epoch:   16/50    Loss: 3.7123453617095947\n",
      "\n",
      "Epoch:   16/50    Loss: 3.7821820464134217\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 50 epochs\n",
    "num_epochs = 50\n",
    "    \n",
    "# training the model\n",
    "trained_rnnrnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches, path=path)\n",
    "\n",
    "# saving the trained model\n",
    "helper.save_model(path, trained_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20 epoch(s)...\n",
      "Epoch:    1/20    Loss: 20.230086227416994\n",
      "\n",
      "Epoch:    1/20    Loss: 20.15226756286621\n",
      "\n",
      "Epoch:    1/20    Loss: 20.11580291366577\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4db9e2ec501b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# training the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrained_rnnrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_every_n_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# saving the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-8bdcdd33eb1a>\u001b[0m in \u001b[0;36mtrain_rnn\u001b[0;34m(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches, path)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# forward, back prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_back_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;31m# record loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mbatch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-c3071b60fc82>\u001b[0m in \u001b[0;36mforward_back_prop\u001b[0;34m(rnn, optimizer, criterion, inp, target, hidden)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msftmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 50 epochs\n",
    "num_epochs = 20\n",
    "    \n",
    "rnn = helper.load_model(path)\n",
    "\n",
    "# training the model\n",
    "trained_rnnrnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches, path=path)\n",
    "\n",
    "# saving the trained model\n",
    "helper.save_model(path, trained_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: How did you decide on your model hyperparameters? \n",
    "For example, did you try different sequence_lengths and find that one size made the model converge faster? What about your hidden_dim and n_layers; how did you decide on those?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** I ran a few experiments. Exp 1 had similar hyperparemter values as the toy codelab we had for classifying the sentiment of a given text. I still had loss greater than 3.5. Exp 2 had a setting of the hidden layer size being increased. The final loss was a tiny bit smaller, but it was still greater than 3.5. Exp 3 had the dropout probability decreased from 0.5 in Exp 1 and 2 to 0.2 with the rest set the same as Exp 2. I saw a much quicker performance improvement. I think given the small dataset size, the dropout prob of 0.5 must have been too big. Exp 4 had everything the same as Exp 3 except the hidden layer size reduced by half. This was inspired by Exp 3 in which we saw the model performance improvement when we provided more data points to be available. It probably means the network is too big for the small amount of training examples we have. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Checkpoint\n",
    "\n",
    "After running the above training cell, your model will be saved by name, `trained_rnn`, and if you save your notebook progress, **you can pause here and come back to this code at another time**. You can resume your progress by running the next cell, which will load in our word:id dictionaries _and_ load in your saved model by name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import torch\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "trained_rnn = helper.load_model(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate TV Script\n",
    "With the network trained and saved, you'll use it to generate a new, \"fake\" Seinfeld TV script in this section.\n",
    "\n",
    "### Generate Text\n",
    "To generate the text, the network needs to start with a single word and repeat its predictions until it reaches a set length. You'll be using the `generate` function to do this. It takes a word id to start with, `prime_id`, and generates a set length of text, `predict_len`. Also note that it uses topk sampling to introduce some randomness in choosing the most likely next word, given an output set of word scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
    "    \"\"\"\n",
    "    Generate text using the neural network\n",
    "    :param decoder: The PyTorch Module that holds the trained neural network\n",
    "    :param prime_id: The word id to start the first prediction\n",
    "    :param int_to_vocab: Dict of word id keys to word values\n",
    "    :param token_dict: Dict of puncuation tokens keys to puncuation values\n",
    "    :param pad_value: The value used to pad a sequence\n",
    "    :param predict_len: The length of text to generate\n",
    "    :return: The generated text\n",
    "    \"\"\"\n",
    "    rnn.eval()\n",
    "    \n",
    "    # create a sequence (batch_size=1) with the prime_id\n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "    current_seq[-1][-1] = prime_id\n",
    "    predicted = [int_to_vocab[prime_id]]\n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        if train_on_gpu:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "            current_seq = torch.LongTensor(current_seq)\n",
    "        \n",
    "        # initialize the hidden state\n",
    "        hidden = rnn.init_hidden(current_seq.size(0))\n",
    "        \n",
    "        # get the output of the rnn\n",
    "        output, _ = rnn(current_seq, hidden)\n",
    "        \n",
    "        # get the next word probabilities\n",
    "        p = F.softmax(output, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "         \n",
    "        # use top_k sampling to get the index of the next word\n",
    "        top_k = 5\n",
    "        p, top_i = p.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next word index with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
    "        \n",
    "        # retrieve that word from the dictionary\n",
    "        word = int_to_vocab[word_i]\n",
    "        predicted.append(word)     \n",
    "        \n",
    "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
    "        current_seq = np.roll(current_seq, -1, 1)\n",
    "        current_seq[-1][-1] = word_i\n",
    "    \n",
    "    gen_sentences = ' '.join(predicted)\n",
    "    \n",
    "    # Replace punctuation tokens\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
    "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
    "    gen_sentences = gen_sentences.replace('( ', '(')\n",
    "    \n",
    "    # return all the sentences\n",
    "    return gen_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a New Script\n",
    "It's time to generate the text. Set `gen_length` to the length of TV script you want to generate and set `prime_word` to one of the following to start the prediction:\n",
    "- \"jerry\"\n",
    "- \"elaine\"\n",
    "- \"george\"\n",
    "- \"kramer\"\n",
    "\n",
    "You can set the prime word to _any word_ in our dictionary, but it's best to start with a name for generating a TV script. (You can also start with any other names you find in the original text file!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell multiple times to get different results!\n",
    "gen_length = 400 # modify the length to your preference\n",
    "prime_word = 'jerry' # name for starting the script\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "pad_word = helper.SPECIAL_WORDS['PADDING']\n",
    "generated_script = generate(trained_rnn, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "print(generated_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save your favorite scripts\n",
    "\n",
    "Once you have a script that you like (or find interesting), save it to a text file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save script to a text file\n",
    "f =  open(\"generated_script_2.txt\",\"w\")\n",
    "f.write(generated_script)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The TV Script is Not Perfect\n",
    "It's ok if the TV script doesn't make perfect sense. It should look like alternating lines of dialogue, here is one such example of a few generated lines.\n",
    "\n",
    "### Example generated script\n",
    "\n",
    ">jerry: what about me?\n",
    ">\n",
    ">jerry: i don't have to wait.\n",
    ">\n",
    ">kramer:(to the sales table)\n",
    ">\n",
    ">elaine:(to jerry) hey, look at this, i'm a good doctor.\n",
    ">\n",
    ">newman:(to elaine) you think i have no idea of this...\n",
    ">\n",
    ">elaine: oh, you better take the phone, and he was a little nervous.\n",
    ">\n",
    ">kramer:(to the phone) hey, hey, jerry, i don't want to be a little bit.(to kramer and jerry) you can't.\n",
    ">\n",
    ">jerry: oh, yeah. i don't even know, i know.\n",
    ">\n",
    ">jerry:(to the phone) oh, i know.\n",
    ">\n",
    ">kramer:(laughing) you know...(to jerry) you don't know.\n",
    "\n",
    "You can see that there are multiple characters that say (somewhat) complete sentences, but it doesn't have to be perfect! It takes quite a while to get good results, and often, you'll have to use a smaller vocabulary (and discard uncommon words), or get more data.  The Seinfeld dataset is about 3.4 MB, which is big enough for our purposes; for script generation you'll want more than 1 MB of text, generally. \n",
    "\n",
    "# Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"dlnd_tv_script_generation.ipynb\" and save another copy as an HTML file by clicking \"File\" -> \"Download as..\"->\"html\". Include the \"helper.py\" and \"problem_unittests.py\" files in your submission. Once you download these files, compress them into one zip file for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
